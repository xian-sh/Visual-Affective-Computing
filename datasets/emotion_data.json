{
    "zhao_affective_2022": {
        "title": "Affective Image Content Analysis: Two Decades Review and New Perspectives",
        "year": "2022",
        "type": "article",
        "venue": "TPAMI",
        "url": "https://doi.org/10.1109/TPAMI.2021.3094362",
        "doi": "10.1109/TPAMI.2021.3094362"
    },
    "zuo_affecti_2020": {
        "title": "AffectI: A Game for Diverse, Reliable, and Efficient Affective Image Annotation",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3413744",
        "doi": "10.1145/3394171.3413744"
    },
    "yao_attention-aware_2019": {
        "title": "Attention-Aware Polarity Sensitive Embedding for Affective Image Retrieval",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ICCV",
        "url": "https://doi.org/10.1109/ICCV.2019.00123",
        "doi": "10.1109/ICCV.2019.00123"
    },
    "wang_image_2018": {
        "title": "Image Captioning with Affective Guiding and Selective Attention",
        "year": "2018",
        "type": "article",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3226037",
        "doi": "10.1145/3226037"
    },
    "berlincioni_neuromorphic_2023": {
        "title": "Neuromorphic Event-based Facial Expression Recognition",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW59228.2023.00432",
        "doi": "10.1109/CVPRW59228.2023.00432"
    },
    "filntisis_spectre_2023": {
        "title": "SPECTRE: Visual Speech-Informed Perceptual 3D Facial Expression Reconstruction from Videos",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW59228.2023.00609",
        "doi": "10.1109/CVPRW59228.2023.00609"
    },
    "hong_dynamic_2023": {
        "title": "Dynamic Noise Injection for Facial Expression Recognition In-the-Wild",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW59228.2023.00605",
        "doi": "10.1109/CVPRW59228.2023.00605"
    },
    "lee_frame_2023": {
        "title": "Frame Level Emotion Guided Dynamic Facial Expression Recognition with Emotion Grouping",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW59228.2023.00602",
        "doi": "10.1109/CVPRW59228.2023.00602"
    },
    "liu_facial_2023": {
        "title": "Facial Expression Recognition Based on Multi-modal Features for Videos in the Wild",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW59228.2023.00624",
        "doi": "10.1109/CVPRW59228.2023.00624"
    },
    "neo_large-scale_2023": {
        "title": "Large-Scale Facial Expression Recognition Using Dual-Domain Affect Fusion for Noisy Labels",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW59228.2023.00603",
        "doi": "10.1109/CVPRW59228.2023.00603"
    },
    "nguyen_micron-bert_2023": {
        "title": "Micron-BERT: BERT-Based Facial Micro-Expression Recognition",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52729.2023.00149",
        "doi": "10.1109/CVPR52729.2023.00149"
    },
    "savchenko_emotieffnets_2023": {
        "title": "EmotiEffNets for Facial Processing in Video-based Valence-Arousal Prediction, Expression Classification and Action Unit Detection",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW59228.2023.00606",
        "doi": "10.1109/CVPRW59228.2023.00606"
    },
    "wang_rethinking_2023": {
        "title": "Rethinking the Learning Paradigm for Dynamic Facial Expression Recognition",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52729.2023.01722",
        "doi": "10.1109/CVPR52729.2023.01722"
    },
    "yu_exploring_2023": {
        "title": "Exploring Large-scale Unlabeled Faces to Enhance Facial Expression Recognition",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW59228.2023.00616",
        "doi": "10.1109/CVPRW59228.2023.00616"
    },
    "bryant_multi-dimensional_2022": {
        "title": "Multi-Dimensional, Nuanced and Subjective - Measuring the Perception of Facial Expressions",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52688.2022.02026",
        "doi": "10.1109/CVPR52688.2022.02026"
    },
    "jeong_classification_2022": {
        "title": "Classification of Facial Expression In-the-Wild based on Ensemble of Multi-head Cross Attention Networks",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW56347.2022.00262",
        "doi": "10.1109/CVPRW56347.2022.00262"
    },
    "jourabloo_robust_2022": {
        "title": "Robust Egocentric Photo-realistic Facial Expression Transfer for Virtual Reality",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52688.2022.01968",
        "doi": "10.1109/CVPR52688.2022.01968"
    },
    "li_towards_2022": {
        "title": "Towards Semi-Supervised Deep Facial Expression Recognition with An Adaptive Confidence Margin",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52688.2022.00413",
        "doi": "10.1109/CVPR52688.2022.00413"
    },
    "otberdout_sparse_2022": {
        "title": "Sparse to Dense Dynamic 3D Facial Expression Generation",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52688.2022.01974",
        "doi": "10.1109/CVPR52688.2022.01974"
    },
    "ouzar_video-based_2022": {
        "title": "Video-based multimodal spontaneous emotion recognition using facial expressions and physiological signals",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW56347.2022.00275",
        "doi": "10.1109/CVPRW56347.2022.00275"
    },
    "papantoniou_neural_2022": {
        "title": "Neural Emotion Director: Speech-preserving semantic control of facial expressions in \"in-the-wild\" videos",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52688.2022.01822",
        "doi": "10.1109/CVPR52688.2022.01822"
    },
    "phan_facial_2022": {
        "title": "Facial Expression Classification using Fusion of Deep Neural Network in Video",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW56347.2022.00280",
        "doi": "10.1109/CVPRW56347.2022.00280"
    },
    "psaroudakis_mixaugment_2022": {
        "title": "MixAugment \\& Mixup: Augmentation Methods for Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW56347.2022.00264",
        "doi": "10.1109/CVPRW56347.2022.00264"
    },
    "wang_ferv39k_2022": {
        "title": "FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52688.2022.02025",
        "doi": "10.1109/CVPR52688.2022.02025"
    },
    "xue_coarse--fine_2022": {
        "title": "Coarse-to-Fine Cascaded Networks with Smooth Predicting for Video Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW56347.2022.00269",
        "doi": "10.1109/CVPRW56347.2022.00269"
    },
    "zeng_face2exp_2022": {
        "title": "Face2Exp: Combating Data Biases for Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52688.2022.01965",
        "doi": "10.1109/CVPR52688.2022.01965"
    },
    "zhang_transformer-based_2022": {
        "title": "Transformer-based Multimodal Information Fusion for Facial Expression Analysis",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW56347.2022.00271",
        "doi": "10.1109/CVPRW56347.2022.00271"
    },
    "banerjee_legan_2021": {
        "title": "LEGAN: Disentangled Manipulation of Directional Lighting and Facial Expressions Whilst Leveraging Human Perceptual Judgements",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://openaccess.thecvf.com/content/CVPR2021W/AMFG/html/Banerjee\\_LEGAN\\_Disentangled\\_Manipulation\\_of\\_Directional\\_Lighting\\_and\\_Facial\\_Expressions\\_Whilst\\_CVPRW\\_2021\\_paper.html",
        "doi": "10.1109/CVPRW53098.2021.00169"
    },
    "barros_i_2021": {
        "title": "I Only Have Eyes for You: The Impact of Masks on Convolutional-Based Facial Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://openaccess.thecvf.com/content/CVPR2021W/LXCV/html/Barros\\_I\\_Only\\_Have\\_Eyes\\_for\\_You\\_The\\_Impact\\_of\\_Masks\\_CVPRW\\_2021\\_paper.html",
        "doi": "10.1109/CVPRW53098.2021.00134"
    },
    "lei_micro-expression_2021": {
        "title": "Micro-Expression Recognition Based on Facial Graph Representation Learning and Facial Action Unit Fusion",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://openaccess.thecvf.com/content/CVPR2021W/AUVi/html/Lei\\_Micro-Expression\\_Recognition\\_Based\\_on\\_Facial\\_Graph\\_Representation\\_Learning\\_and\\_Facial\\_CVPRW\\_2021\\_paper.html",
        "doi": "10.1109/CVPRW53098.2021.00173"
    },
    "ruan_feature_2021": {
        "title": "Feature Decomposition and Reconstruction Learning for Effective Facial Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content/CVPR2021/html/Ruan\\_Feature\\_Decomposition\\_and\\_Reconstruction\\_Learning\\_for\\_Effective\\_Facial\\_Expression\\_Recognition\\_CVPR\\_2021\\_paper.html",
        "doi": "10.1109/CVPR46437.2021.00757"
    },
    "sanchez_affective_2021": {
        "title": "Affective Processes: Stochastic Modelling of Temporal Context for Emotion and Facial Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content/CVPR2021/html/Sanchez\\_Affective\\_Processes\\_Stochastic\\_Modelling\\_of\\_Temporal\\_Context\\_for\\_Emotion\\_and\\_CVPR\\_2021\\_paper.html",
        "doi": "10.1109/CVPR46437.2021.00896"
    },
    "she_dive_2021": {
        "title": "Dive Into Ambiguity: Latent Distribution Mining and Pairwise Uncertainty Estimation for Facial Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content/CVPR2021/html/She\\_Dive\\_Into\\_Ambiguity\\_Latent\\_Distribution\\_Mining\\_and\\_Pairwise\\_Uncertainty\\_Estimation\\_CVPR\\_2021\\_paper.html",
        "doi": "10.1109/CVPR46437.2021.00618"
    },
    "zhang_learning_2021": {
        "title": "Learning a Facial Expression Embedding Disentangled From Identity",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content/CVPR2021/html/Zhang\\_Learning\\_a\\_Facial\\_Expression\\_Embedding\\_Disentangled\\_From\\_Identity\\_CVPR\\_2021\\_paper.html",
        "doi": "10.1109/CVPR46437.2021.00669"
    },
    "chen_label_2020": {
        "title": "Label Distribution Learning on Auxiliary Label Space Graphs for Facial Expression Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content\\_CVPR\\_2020/html/Chen\\_Label\\_Distribution\\_Learning\\_on\\_Auxiliary\\_Label\\_Space\\_Graphs\\_for\\_Facial\\_CVPR\\_2020\\_paper.html",
        "doi": "10.1109/CVPR42600.2020.01400"
    },
    "farzaneh_discriminant_2020": {
        "title": "Discriminant Distribution-Agnostic Loss for Facial Expression Recognition in the Wild",
        "year": "2020",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://openaccess.thecvf.com/content\\_CVPRW\\_2020/html/w29/Farzaneh\\_Discriminant\\_Distribution-Agnostic\\_Loss\\_for\\_Facial\\_Expression\\_Recognition\\_in\\_the\\_Wild\\_CVPRW\\_2020\\_paper.html",
        "doi": "10.1109/CVPRW50498.2020.00211"
    },
    "he_image2audio_2020": {
        "title": "Image2Audio: Facilitating Semi-supervised Audio Emotion Recognition with Facial Expression Image",
        "year": "2020",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://openaccess.thecvf.com/content\\_CVPRW\\_2020/html/w54/He\\_Image2Audio\\_Facilitating\\_Semi-Supervised\\_Audio\\_Emotion\\_Recognition\\_With\\_Facial\\_Expression\\_Image\\_CVPRW\\_2020\\_paper.html",
        "doi": "10.1109/CVPRW50498.2020.00464"
    },
    "sariyanidi_can_2020": {
        "title": "Can Facial Pose and Expression Be Separated With Weak Perspective Camera?",
        "year": "2020",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR42600.2020.00720",
        "doi": "10.1109/CVPR42600.2020.00720"
    },
    "wang_suppressing_2020": {
        "title": "Suppressing Uncertainties for Large-Scale Facial Expression Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content\\_CVPR\\_2020/html/Wang\\_Suppressing\\_Uncertainties\\_for\\_Large-Scale\\_Facial\\_Expression\\_Recognition\\_CVPR\\_2020\\_paper.html",
        "doi": "10.1109/CVPR42600.2020.00693"
    },
    "wu_cascade_2020": {
        "title": "Cascade EF-GAN: Progressive Facial Expression Editing With Local Focuses",
        "year": "2020",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content\\_CVPR\\_2020/html/Wu\\_Cascade\\_EF-GAN\\_Progressive\\_Facial\\_Expression\\_Editing\\_With\\_Local\\_Focuses\\_CVPR\\_2020\\_paper.html",
        "doi": "10.1109/CVPR42600.2020.00507"
    },
    "bera_modelling_2019": {
        "title": "Modelling Multi-Channel Emotions Using Facial Expression and Trajectory Cues for Improving Socially-Aware Robot Navigation",
        "year": "2019",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "http://openaccess.thecvf.com/content\\_CVPRW\\_2019/html/AMFG/Bera\\_Modelling\\_Multi-Channel\\_Emotions\\_Using\\_Facial\\_Expression\\_and\\_Trajectory\\_Cues\\_for\\_CVPRW\\_2019\\_paper.html",
        "doi": "10.1109/CVPRW.2019.00035"
    },
    "kumar_classification_2019": {
        "title": "CLASSIFICATION OF FACIAL MICRO-EXPRESSIONS USING MOTION MAGNIFIED EMOTION AVATAR IMAGES",
        "year": "2019",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "http://openaccess.thecvf.com/content\\_CVPRW\\_2019/html/Face\\_and\\_Gesture\\_Analysis\\_for\\_Health\\_Informatics/Kumar\\_CLASSIFICATION\\_OF\\_FACIAL\\_MICRO-EXPRESSIONS\\_USING\\_MOTION\\_MAGNIFIED\\_EMOTION\\_AVATAR\\_IMAGES\\_CVPRW\\_2019\\_paper.html",
        "doi": null
    },
    "kumawat_lbvcnn_2019": {
        "title": "LBVCNN: Local Binary Volume Convolutional Neural Network for Facial Expression Recognition From Image Sequences",
        "year": "2019",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "http://openaccess.thecvf.com/content\\_CVPRW\\_2019/html/AMFG/Kumawat\\_LBVCNN\\_Local\\_Binary\\_Volume\\_Convolutional\\_Neural\\_Network\\_for\\_Facial\\_Expression\\_CVPRW\\_2019\\_paper.html",
        "doi": "10.1109/CVPRW.2019.00030"
    },
    "ly_multimodal_2019": {
        "title": "Multimodal 2D and 3D for In-The-Wild Facial Expression Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "http://openaccess.thecvf.com/content\\_CVPRW\\_2019/html/Precognition/Ly\\_Multimodal\\_2D\\_and\\_3D\\_for\\_In-The-Wild\\_Facial\\_Expression\\_Recognition\\_CVPRW\\_2019\\_paper.html",
        "doi": "10.1109/CVPRW.2019.00353"
    },
    "marrero-fernandez_feratt_2019": {
        "title": "FERAtt: Facial Expression Recognition With Attention Net",
        "year": "2019",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "http://openaccess.thecvf.com/content\\_CVPRW\\_2019/html/MBCCV/Fernandez\\_FERAtt\\_Facial\\_Expression\\_Recognition\\_With\\_Attention\\_Net\\_CVPRW\\_2019\\_paper.html",
        "doi": "10.1109/CVPRW.2019.00112"
    },
    "vemulapalli_compact_2019": {
        "title": "A Compact Embedding for Facial Expression Similarity",
        "year": "2019",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "http://openaccess.thecvf.com/content\\_CVPR\\_2019/html/Vemulapalli\\_A\\_Compact\\_Embedding\\_for\\_Facial\\_Expression\\_Similarity\\_CVPR\\_2019\\_paper.html",
        "doi": "10.1109/CVPR.2019.00583"
    },
    "bishay_affdex_2023": {
        "title": "AFFDEX 2.0: A Real-Time Facial Expression Analysis Toolkit",
        "year": "2023",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG57933.2023.10042673",
        "doi": "10.1109/FG57933.2023.10042673"
    },
    "han_learning_2023": {
        "title": "Learning Effective Global Receptive Field for Facial Expression Recognition",
        "year": "2023",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG57933.2023.10042628",
        "doi": "10.1109/FG57933.2023.10042628"
    },
    "ma_relation-aware_2023": {
        "title": "Relation-aware Network for Facial Expression Recognition",
        "year": "2023",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG57933.2023.10042525",
        "doi": "10.1109/FG57933.2023.10042525"
    },
    "principi_florence_2023": {
        "title": "The Florence 4D Facial Expression Dataset",
        "year": "2023",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG57933.2023.10042606",
        "doi": "10.1109/FG57933.2023.10042606"
    },
    "stoychev_latent_2023": {
        "title": "Latent Generative Replay for Resource-Efficient Continual Learning of Facial Expressions",
        "year": "2023",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG57933.2023.10042642",
        "doi": "10.1109/FG57933.2023.10042642"
    },
    "wang_disvae_2023": {
        "title": "DisVAE: Disentangled Variational Autoencoder for High-Quality Facial Expression Features",
        "year": "2023",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG57933.2023.10042668",
        "doi": "10.1109/FG57933.2023.10042668"
    },
    "antoniadis_exploiting_2021": {
        "title": "Exploiting Emotional Dependencies with Graph Convolutional Networks for Facial Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG52635.2021.9667014",
        "doi": "10.1109/FG52635.2021.9667014"
    },
    "jiang_two-stream_2021": {
        "title": "Two-stream Gabor-AGraph Convolutional Networks for Facial Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG52635.2021.9666935",
        "doi": "10.1109/FG52635.2021.9666935"
    },
    "liang_pose-invariant_2021": {
        "title": "Pose-Invariant Facial Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG52635.2021.9666974",
        "doi": "10.1109/FG52635.2021.9666974"
    },
    "rosberg_comparing_2021": {
        "title": "Comparing Facial Expressions for Face Swapping Evaluation with Supervised Contrastive Representation Learning",
        "year": "2021",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG52635.2021.9666958",
        "doi": "10.1109/FG52635.2021.9666958"
    },
    "wang_expression-latent-space-guided_2021": {
        "title": "Expression-Latent-Space-guided GAN for Facial Expression Animation based on Discrete Labels",
        "year": "2021",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG52635.2021.9666959",
        "doi": "10.1109/FG52635.2021.9666959"
    },
    "wen_two-stream_2021": {
        "title": "Two-stream Global-Guided Attention Network for Facial Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG52635.2021.9667041",
        "doi": "10.1109/FG52635.2021.9667041"
    },
    "athar_self-supervised_2020": {
        "title": "Self-supervised Deformation Modeling for Facial Expression Editing",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00115",
        "doi": "10.1109/FG47880.2020.00115"
    },
    "barros_facechannel_2020": {
        "title": "The FaceChannel: A Light-weight Deep Neural Network for Facial Expression Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00070",
        "doi": "10.1109/FG47880.2020.00070"
    },
    "behzad_landmarks-assisted_2020": {
        "title": "Landmarks-assisted Collaborative Deep Framework for Automatic 4D Facial Expression Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00023",
        "doi": "10.1109/FG47880.2020.00023"
    },
    "bernheim_modul_2020": {
        "title": "MoDuL: Deep Modal and Dual Landmark-wise Gated Network for Facial Expression Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00081",
        "doi": "10.1109/FG47880.2020.00081"
    },
    "churamani_clifer_2020": {
        "title": "CLIFER: Continual Learning with Imagination for Facial Expression Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00110",
        "doi": "10.1109/FG47880.2020.00110"
    },
    "egede_emopain_2020": {
        "title": "EMOPAIN Challenge 2020: Multimodal Pain Evaluation from Facial and Bodily Expressions",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00078",
        "doi": "10.1109/FG47880.2020.00078"
    },
    "esposito_seniors_2020": {
        "title": "Seniors' ability to decode differently aged facial emotional expressions",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00077",
        "doi": "10.1109/FG47880.2020.00077"
    },
    "esposito_impairments_2020": {
        "title": "Impairments in decoding facial and vocal emotional expressions in high functioning autistic adults and adolescents",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00055",
        "doi": "10.1109/FG47880.2020.00055"
    },
    "hinduja_recognizing_2020": {
        "title": "Recognizing Perceived Emotions from Facial Expressions",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00025",
        "doi": "10.1109/FG47880.2020.00025"
    },
    "koujan_real-time_2020": {
        "title": "Real-time Facial Expression Recognition \"In The Wild\" by Disentangling 3D Expression from Identity",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00084",
        "doi": "10.1109/FG47880.2020.00084"
    },
    "li_megc2020_2020": {
        "title": "MEGC2020 - The Third Facial Micro-Expression Grand Challenge",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00035",
        "doi": "10.1109/FG47880.2020.00035"
    },
    "liu_facial_2020": {
        "title": "Facial Expression Recognition for In-the-wild Videos",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00102",
        "doi": "10.1109/FG47880.2020.00102"
    },
    "mallol-ragolta_curriculum_2020": {
        "title": "A Curriculum Learning Approach for Pain Intensity Recognition from Facial Expressions",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00083",
        "doi": "10.1109/FG47880.2020.00083"
    },
    "merghani_adaptive_2020": {
        "title": "Adaptive Mask for Region-based Facial Micro-Expression Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00067",
        "doi": "10.1109/FG47880.2020.00067"
    },
    "naven_leveraging_2020": {
        "title": "Leveraging Shared and Divergent Facial Expression Behavior Between Genders in Deception Detection",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00124",
        "doi": "10.1109/FG47880.2020.00124"
    },
    "pessanha_towards_2020": {
        "title": "Towards automatic monitoring of disease progression in sheep: A hierarchical model for sheep facial expressions analysis from video",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00107",
        "doi": "10.1109/FG47880.2020.00107"
    },
    "salman_dynamic_2020": {
        "title": "Dynamic versus Static Facial Expressions in the Presence of Speech",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00119",
        "doi": "10.1109/FG47880.2020.00119"
    },
    "yap_samm_2020": {
        "title": "SAMM Long Videos: A Spontaneous Facial Micro- and Macro-Expressions Dataset",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00029",
        "doi": "10.1109/FG47880.2020.00029"
    },
    "albrici_g2-ver_2019": {
        "title": "G2-VER: Geometry Guided Model Ensemble for Video-based Facial Expression Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756600",
        "doi": "10.1109/FG.2019.8756600"
    },
    "bozorgtabar_using_2019": {
        "title": "Using Photorealistic Face Synthesis and Domain Adaptation to Improve Facial Expression Analysis",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756632",
        "doi": "10.1109/FG.2019.8756632"
    },
    "chen_equipping_2019": {
        "title": "Equipping social robots with culturally-sensitive facial expressions of emotion using data-driven methods",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756570",
        "doi": "10.1109/FG.2019.8756570"
    },
    "happy_characterizing_2019": {
        "title": "Characterizing the State of Apathy with Facial Expression and Motion Analysis",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756545",
        "doi": "10.1109/FG.2019.8756545"
    },
    "hayale_facial_2019": {
        "title": "Facial Expression Recognition Using Deep Siamese Neural Networks with a Supervised Loss function",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756571",
        "doi": "10.1109/FG.2019.8756571"
    },
    "hosseini_gf-capsnet_2019": {
        "title": "GF-CapsNet: Using Gabor Jet and Capsule Networks for Facial Age, Gender, and Expression Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756552",
        "doi": "10.1109/FG.2019.8756552"
    },
    "jyoti_expression_2019": {
        "title": "Expression Empowered ResiDen Network for Facial Action Unit Detection",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756580",
        "doi": "10.1109/FG.2019.8756580"
    },
    "lee_visual_2019": {
        "title": "Visual Scene-aware Hybrid Neural Network Architecture for Video-based Facial Expression Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756551",
        "doi": "10.1109/FG.2019.8756551"
    },
    "lindt_facial_2019": {
        "title": "Facial Expression Editing with Continuous Emotion Labels",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756558",
        "doi": "10.1109/FG.2019.8756558"
    },
    "melo_combining_2019": {
        "title": "Combining Global and Local Convolutional 3D Networks for Detecting Depression from Facial Expressions",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756568",
        "doi": "10.1109/FG.2019.8756568"
    },
    "peng_boost_2019": {
        "title": "A Boost in Revealing Subtle Facial Expressions: A Consolidated Eulerian Framework",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756541",
        "doi": "10.1109/FG.2019.8756541"
    },
    "see_megc_2019": {
        "title": "MEGC 2019 - The Second Facial Micro-Expressions Grand Challenge",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756611",
        "doi": "10.1109/FG.2019.8756611"
    },
    "tavakolian_learning_2019": {
        "title": "Learning to Detect Genuine versus Posed Pain from Facial Expressions using Residual Generative Adversarial Networks",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756540",
        "doi": "10.1109/FG.2019.8756540"
    },
    "werner_generalizing_2019": {
        "title": "Generalizing to Unseen Head Poses in Facial Expression Recognition and Action Unit Intensity Estimation",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756596",
        "doi": "10.1109/FG.2019.8756596"
    },
    "wilhelm_towards_2019": {
        "title": "Towards Facial Expression Analysis in a Driver Assistance System",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756565",
        "doi": "10.1109/FG.2019.8756565"
    },
    "xue_learning_2019": {
        "title": "Learning Interpretable Expression-sensitive Features for 3D Dynamic Facial Expression Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756564",
        "doi": "10.1109/FG.2019.8756564"
    },
    "zhong_graph-structured_2019": {
        "title": "A Graph-Structured Representation with BRNN for Static-based Facial Expression Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756615",
        "doi": "10.1109/FG.2019.8756615"
    },
    "zhu_discriminative_2019": {
        "title": "Discriminative Attention-based Convolutional Neural Network for 3D Facial Expression Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756524",
        "doi": "10.1109/FG.2019.8756524"
    },
    "arnaud_thin_2023": {
        "title": "THIN: THrowable Information Networks and Application for Facial Expression Recognition in the Wild",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3144439",
        "doi": "10.1109/TAFFC.2022.3144439"
    },
    "belmonte_impact_2023": {
        "title": "Impact of Facial Landmark Localization on Facial Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2021.3124142",
        "doi": "10.1109/TAFFC.2021.3124142"
    },
    "cai_probabilistic_2023": {
        "title": "Probabilistic Attribute Tree Structured Convolutional Neural Networks for Facial Expression Recognition in the Wild",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3156920",
        "doi": "10.1109/TAFFC.2022.3156920"
    },
    "chen_learning_2023": {
        "title": "Learning Transferable Sparse Representations for Cross-Corpus Facial Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2021.3077489",
        "doi": "10.1109/TAFFC.2021.3077489"
    },
    "chen_stcam_2023": {
        "title": "STCAM: Spatial-Temporal and Channel Attention Module for Dynamic Facial Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2020.3027340",
        "doi": "10.1109/TAFFC.2020.3027340"
    },
    "churamani_domain-incremental_2023": {
        "title": "Domain-Incremental Continual Learning for Mitigating Bias in Facial Expression and Action Unit Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3181033",
        "doi": "10.1109/TAFFC.2022.3181033"
    },
    "gu_wife_2023": {
        "title": "WiFE: WiFi and Vision Based Unobtrusive Emotion Recognition via Gesture and Facial Expression",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2023.3285777",
        "doi": "10.1109/TAFFC.2023.3285777"
    },
    "hayale_deep_2023": {
        "title": "Deep Siamese Neural Networks for Facial Expression Recognition in the Wild",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2021.3077248",
        "doi": "10.1109/TAFFC.2021.3077248"
    },
    "huang_pidvit_2023": {
        "title": "PIDViT: Pose-Invariant Distilled Vision Transformer for Facial Expression Recognition in the Wild",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3220972",
        "doi": "10.1109/TAFFC.2022.3220972"
    },
    "humpe_rhythm_2023": {
        "title": "The Rhythm of Flow: Detecting Facial Expressions of Flow Experiences Using CNNs",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2021.3087222",
        "doi": "10.1109/TAFFC.2021.3087222"
    },
    "jiang_boosting_2023": {
        "title": "Boosting Facial Expression Recognition by A Semi-Supervised Progressive Teacher",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2021.3131621",
        "doi": "10.1109/TAFFC.2021.3131621"
    },
    "li_spontaneous_2023": {
        "title": "A Spontaneous Driver Emotion Facial Expression (DEFE) Dataset for Intelligent Vehicles: Emotions Triggered by Video-Audio Clips in Driving Scenarios",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2021.3063387",
        "doi": "10.1109/TAFFC.2021.3063387"
    },
    "li_facial_2023": {
        "title": "Facial Expression Recognition in the Wild Using Multi-Level Features and Attention Mechanisms",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2020.3031602",
        "doi": "10.1109/TAFFC.2020.3031602"
    },
    "ma_facial_2023": {
        "title": "Facial Expression Recognition With Visual Transformers and Attentional Selective Fusion",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2021.3122146",
        "doi": "10.1109/TAFFC.2021.3122146"
    },
    "sen_dbates_2023": {
        "title": "DBATES: Dataset for Discerning Benefits of Audio, Textual, and Facial Expression Features in Competitive Debate Speeches",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2021.3103442",
        "doi": "10.1109/TAFFC.2021.3103442"
    },
    "sun_unsupervised_2023": {
        "title": "Unsupervised Cross-View Facial Expression Image Generation and Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2020.3029531",
        "doi": "10.1109/TAFFC.2020.3029531"
    },
    "teng_typical_2023": {
        "title": "Typical Facial Expression Network Using a Facial Feature Decoupler and Spatial-Temporal Learning",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2021.3102245",
        "doi": "10.1109/TAFFC.2021.3102245"
    },
    "wang_bias-based_2023": {
        "title": "Bias-Based Soft Label Learning for Facial Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3220291",
        "doi": "10.1109/TAFFC.2022.3220291"
    },
    "wang_facial_2023": {
        "title": "Facial Expression Animation by Landmark Guided Residual Module",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2021.3100352",
        "doi": "10.1109/TAFFC.2021.3100352"
    },
    "wu_recognizing_2023": {
        "title": "Recognizing, Fast and Slow: Complex Emotion Recognition With Facial Expression Detection and Remote Physiological Measurement",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2023.3253859",
        "doi": "10.1109/TAFFC.2023.3253859"
    },
    "xie_overview_2023": {
        "title": "An Overview of Facial Micro-Expression Analysis: Data, Methodology and Challenge",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3143100",
        "doi": "10.1109/TAFFC.2022.3143100"
    },
    "xue_vision_2023": {
        "title": "Vision Transformer With Attentive Pooling for Robust Facial Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3226473",
        "doi": "10.1109/TAFFC.2022.3226473"
    },
    "yan_fenp_2023": {
        "title": "FENP: A Database of Neonatal Facial Expression for Pain Analysis",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2020.3030296",
        "doi": "10.1109/TAFFC.2020.3030296"
    },
    "zhang_joint_2023": {
        "title": "Joint Local-Global Discriminative Subspace Transfer Learning for Facial Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3168834",
        "doi": "10.1109/TAFFC.2022.3168834"
    },
    "zhao_geometry-aware_2023": {
        "title": "Geometry-Aware Facial Expression Recognition via Attentive Graph Convolutional Networks",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2021.3088895",
        "doi": "10.1109/TAFFC.2021.3088895"
    },
    "zhao_spatial-temporal_2023": {
        "title": "Spatial-Temporal Graphs Plus Transformers for Geometry-Guided Facial Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3181736",
        "doi": "10.1109/TAFFC.2022.3181736"
    },
    "allaert_micro_2022": {
        "title": "Micro and Macro Facial Expression Recognition Using Advanced Local Motion Patterns",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2019.2949559",
        "doi": "10.1109/TAFFC.2019.2949559"
    },
    "baddar_--fly_2022": {
        "title": "On-the-Fly Facial Expression Prediction Using LSTM Encoded Appearance-Suppressed Dynamics",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2019.2957465",
        "doi": "10.1109/TAFFC.2019.2957465"
    },
    "chen_semantic-rich_2022": {
        "title": "Semantic-Rich Facial Emotional Expression Recognition",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3201290",
        "doi": "10.1109/TAFFC.2022.3201290"
    },
    "dindar_leaders_2022": {
        "title": "Leaders and Followers Identified by Emotional Mimicry During Collaborative Learning: A Facial Expression Recognition Study on Emotional Valence",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2020.3003243",
        "doi": "10.1109/TAFFC.2020.3003243"
    },
    "fan_facial_2022": {
        "title": "Facial Expression Recognition With Deeply-Supervised Attention Network",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2020.2988264",
        "doi": "10.1109/TAFFC.2020.2988264"
    },
    "iqbal_facial_2022": {
        "title": "Facial Expression Recognition with Active Local Shape Pattern and Learned-Size Block Representations",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2020.2995432",
        "doi": "10.1109/TAFFC.2020.2995432"
    },
    "jampour_multiview_2022": {
        "title": "Multiview Facial Expression Recognition, A Survey",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3184995",
        "doi": "10.1109/TAFFC.2022.3184995"
    },
    "jiang_disentangling_2022": {
        "title": "Disentangling Identity and Pose for Facial Expression Recognition",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3197761",
        "doi": "10.1109/TAFFC.2022.3197761"
    },
    "li_deeper_2022": {
        "title": "A Deeper Look at Facial Expression Dataset Bias",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2020.2973158",
        "doi": "10.1109/TAFFC.2020.2973158"
    },
    "li_deep_2022": {
        "title": "Deep Facial Expression Recognition: A Survey",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2020.2981446",
        "doi": "10.1109/TAFFC.2020.2981446"
    },
    "mohan_flepnet_2022": {
        "title": "FLEPNet: Feature Level Ensemble Parallel Network for Facial Expression Recognition",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3208309",
        "doi": "10.1109/TAFFC.2022.3208309"
    },
    "nguyen_facial_2022": {
        "title": "Facial Expression Recognition Using a Temporal Ensemble of Multi-Level Convolutional Neural Networks",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2019.2946540",
        "doi": "10.1109/TAFFC.2019.2946540"
    },
    "oveneke_leveraging_2022": {
        "title": "Leveraging the Deep Learning Paradigm for Continuous Affect Estimation from Facial Expressions",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2019.2944603",
        "doi": "10.1109/TAFFC.2019.2944603"
    },
    "savchenko_classifying_2022": {
        "title": "Classifying Emotions and Engagement in Online Learning Based on a Single Facial Expression Recognition Neural Network",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3188390",
        "doi": "10.1109/TAFFC.2022.3188390"
    },
    "tang_facial_2022": {
        "title": "Facial Expression Translation Using Landmark Guided GANs",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3207007",
        "doi": "10.1109/TAFFC.2022.3207007"
    },
    "turan_facial_2022": {
        "title": "Facial Expressions of Comprehension (FEC)",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2019.2954498",
        "doi": "10.1109/TAFFC.2019.2954498"
    },
    "wang_phase_2022": {
        "title": "Phase Space Reconstruction Driven Spatio-Temporal Feature Learning for Dynamic Facial Expression Recognition",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2020.3007531",
        "doi": "10.1109/TAFFC.2020.3007531"
    },
    "aylett_architecture_2021": {
        "title": "An Architecture for Emotional Facial Expressions as Social Signals",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2019.2906200",
        "doi": "10.1109/TAFFC.2019.2906200"
    },
    "hong_exploring_2021": {
        "title": "Exploring Macroscopic and Microscopic Fluctuations of Elicited Facial Expressions for Mood Disorder Classification",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2019.2909873",
        "doi": "10.1109/TAFFC.2019.2909873"
    },
    "li_facial_2021": {
        "title": "Facial Expression Recognition with Identity and Emotion Joint Learning",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2018.2880201",
        "doi": "10.1109/TAFFC.2018.2880201"
    },
    "srinivasan_cross-cultural_2021": {
        "title": "Cross-Cultural and Cultural-Specific Production and Perception of Facial Expressions of Emotion in the Wild",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2018.2887267",
        "doi": "10.1109/TAFFC.2018.2887267"
    },
    "dibeklioglu_automatic_2020": {
        "title": "Automatic Estimation of Taste Liking Through Facial Expression Dynamics",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2018.2832044",
        "doi": "10.1109/TAFFC.2018.2832044"
    },
    "iqbal_facial_2020": {
        "title": "Facial Expression Recognition with Neighborhood-Aware Edge Directional Pattern (NEDP)",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2018.2829707",
        "doi": "10.1109/TAFFC.2018.2829707"
    },
    "khan_co-clustering_2020": {
        "title": "Co-Clustering to Reveal Salient Facial Features for Expression Recognition",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2017.2780838",
        "doi": "10.1109/TAFFC.2017.2780838"
    },
    "wang_exploring_2020": {
        "title": "Exploring Domain Knowledge for Facial Expression-Assisted Action Unit Activation Recognition",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2018.2822303",
        "doi": "10.1109/TAFFC.2018.2822303"
    },
    "weber_unsupervised_2020": {
        "title": "Unsupervised Adaptation of a Person-Specific Manifold of Facial Expressions",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2018.2807430",
        "doi": "10.1109/TAFFC.2018.2807430"
    },
    "dapogny_dynamic_2019": {
        "title": "Dynamic Pose-Robust Facial Expression Recognition by Multi-View Pairwise Conditional Random Forests",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2017.2708106",
        "doi": "10.1109/TAFFC.2017.2708106"
    },
    "gupta_multi-velocity_2019": {
        "title": "Multi-Velocity Neural Networks for Facial Expression Recognition in Videos",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2017.2713355",
        "doi": "10.1109/TAFFC.2017.2713355"
    },
    "huang_discriminative_2019": {
        "title": "Discriminative Spatiotemporal Local Binary Pattern with Revisited Integral Projection for Spontaneous Facial Micro-Expression Recognition",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2017.2713359",
        "doi": "10.1109/TAFFC.2017.2713359"
    },
    "kim_multi-objective_2019": {
        "title": "Multi-Objective Based Spatio-Temporal Feature Representation Learning Robust to Expression Intensity Variations for Facial Expression Recognition",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2017.2695999",
        "doi": "10.1109/TAFFC.2017.2695999"
    },
    "mcduff_am-fed_2019": {
        "title": "AM-FED+: An Extended Dataset of Naturalistic Facial Expressions Collected in Everyday Settings",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2018.2801311",
        "doi": "10.1109/TAFFC.2018.2801311"
    },
    "mollahosseini_affectnet_2019": {
        "title": "AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2017.2740923",
        "doi": "10.1109/TAFFC.2017.2740923"
    },
    "zhen_magnifying_2019": {
        "title": "Magnifying Subtle Facial Motions for Effective 4D Expression Recognition",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2017.2747553",
        "doi": "10.1109/TAFFC.2017.2747553"
    },
    "hu_2cet-gan_2023": {
        "title": "2CET-GAN: Pixel-Level GAN Model for Human Facial Expression Transfer",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Proceedings of the 1st International Workshop on Multimedia Content Generation and Evaluation: New Methods and Practice, McGE 2023, Ottawa, ON, Canada, 29 October 2023",
        "url": "https://doi.org/10.1145/3607541.3616822",
        "doi": "10.1145/3607541.3616822"
    },
    "davison_fme_2023": {
        "title": "FME '23: 3rd Facial Micro-Expression Workshop",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3610948",
        "doi": "10.1145/3581783.3610948"
    },
    "liu_learning_2023": {
        "title": "Learning from More: Combating Uncertainty Cross-multidomain for Facial Expression Recognition",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3611702",
        "doi": "10.1145/3581783.3611702"
    },
    "moroto_personalized_2023": {
        "title": "Personalized Content Recommender System via Non-verbal Interaction Using Face Mesh and Facial Expression",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3612675",
        "doi": "10.1145/3581783.3612675"
    },
    "sun_mae-dfer_2023": {
        "title": "MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3612365",
        "doi": "10.1145/3581783.3612365"
    },
    "tao_freq-hd_2023": {
        "title": "Freq-HD: An Interpretable Frequency-based High-Dynamics Affective Clip Selection Method for in-the-Wild Facial Expression Recognition in Videos",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3611972",
        "doi": "10.1145/3581783.3611972"
    },
    "wu_generative_2023": {
        "title": "Generative Neutral Features-Disentangled Learning for Facial Expression Recognition",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3612014",
        "doi": "10.1145/3581783.3612014"
    },
    "wu_patch-aware_2023": {
        "title": "Patch-Aware Representation Learning for Facial Expression Recognition",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3612342",
        "doi": "10.1145/3581783.3612342"
    },
    "zhao_facial_2023": {
        "title": "Facial Auto Rigging from 4D Expressions via Skinning Decomposition",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3612934",
        "doi": "10.1145/3581783.3612934"
    },
    "zhu_variance-aware_2023": {
        "title": "Variance-Aware Bi-Attention Expression Transformer for Open-Set Facial Expression Recognition in the Wild",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3612546",
        "doi": "10.1145/3581783.3612546"
    },
    "davison_proceedings_2023": {
        "title": "Proceedings of the 3rd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis, FME 2023, Ottawa, ON, Canada, 29 October 2023",
        "year": "2023",
        "type": "book",
        "venue": "ACM",
        "url": "https://doi.org/10.1145/3607829",
        "doi": "10.1145/3607829"
    },
    "liong_mtsn_2022": {
        "title": "MTSN: A Multi-Temporal Stream Network for Spotting Facial Macro- and Micro-Expression with Hard and Soft Pseudo-labels",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM FME",
        "url": "https://doi.org/10.1145/3552465.3555040",
        "doi": "10.1145/3552465.3555040"
    },
    "lu_more_2022": {
        "title": "A More Objective Quantification of Micro-Expression Intensity through Facial Electromyography",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM FME",
        "url": "https://doi.org/10.1145/3552465.3555038",
        "doi": "10.1145/3552465.3555038"
    },
    "liu_mafw_2022": {
        "title": "MAFW: A Large-scale, Multi-modal, Compound Affective Database for Dynamic Facial Expression Recognition in the Wild",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3548190",
        "doi": "10.1145/3503161.3548190"
    },
    "donadio_engaging_2022": {
        "title": "Engaging Museum Visitors with Gamification of Body and Facial Expressions",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3547744",
        "doi": "10.1145/3503161.3547744"
    },
    "fan_adaptive_2022": {
        "title": "Adaptive Dual Motion Model for Facial Micro-Expression Generation",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3551592",
        "doi": "10.1145/3503161.3551592"
    },
    "li_fme_2022": {
        "title": "FME '22: 2nd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3554777",
        "doi": "10.1145/3503161.3554777"
    },
    "shao_self-paced_2022": {
        "title": "Self-Paced Label Distribution Learning for In-The-Wild Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3547960",
        "doi": "10.1145/3503161.3547960"
    },
    "wang_ease_2022": {
        "title": "EASE: Robust Facial Expression Recognition via Emotion Ambiguity-SEnsitive Cooperative Networks",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3548005",
        "doi": "10.1145/3503161.3548005"
    },
    "wang_emotional_2022": {
        "title": "Emotional Reaction Analysis based on Multi-Label Graph Convolutional Networks and Dynamic Facial Expression Recognition Transformer",
        "year": "2022",
        "type": "inproceedings",
        "venue": "MuSe@MM 2022: Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge, Lisboa, Portugal, 10 October 2022",
        "url": "https://doi.org/10.1145/3551876.3554810",
        "doi": "10.1145/3551876.3554810"
    },
    "wang_dpcnet_2022": {
        "title": "DPCNet: Dual Path Multi-Excitation Collaborative Network for Facial Expression Representation Learning in Videos",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3547865",
        "doi": "10.1145/3503161.3547865"
    },
    "xing_co-completion_2022": {
        "title": "Co-Completion for Occluded Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3548183",
        "doi": "10.1145/3503161.3548183"
    },
    "yap_3d-cnn_2022": {
        "title": "3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video Sequences using Temporal Oriented Reference Frame",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3551570",
        "doi": "10.1145/3503161.3551570"
    },
    "yu_facial_2022": {
        "title": "Facial Expression Spotting Based on Optical Flow Features",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3551608",
        "doi": "10.1145/3503161.3551608"
    },
    "li_proceedings_2022": {
        "title": "Proceedings of the 2nd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis, FME 2022, Lisboa, Portugal, 14 October 2022",
        "year": "2022",
        "type": "book",
        "venue": "ACM",
        "url": "https://doi.org/10.1145/3552465",
        "doi": "10.1145/3552465"
    },
    "fan_facial_2021": {
        "title": "Facial Micro-Expression Generation based on Deep Motion Retargeting and Transfer Learning",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3474085.3479210",
        "doi": "10.1145/3474085.3479210"
    },
    "guan_transfer_2021": {
        "title": "Transfer Spatio-Temporal Knowledge from Emotion-Related Tasks for Facial Expression Spotting",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM FME",
        "url": "https://doi.org/10.1145/3476100.3484461",
        "doi": "10.1145/3476100.3484461"
    },
    "li_jdman_2021": {
        "title": "JDMAN: Joint Discriminative and Mutual Adaptation Networks for Cross-Domain Facial Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3474085.3475484",
        "doi": "10.1145/3474085.3475484"
    },
    "li_fme21_2021": {
        "title": "FME'21: 1st Workshop on Facial Micro-Expression: Advanced Techniques for Facial Expressions Generation and Spotting",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3474085.3478579",
        "doi": "10.1145/3474085.3478579"
    },
    "mo_d3net_2021": {
        "title": "D\\textbackslash(\\textasciicircum3\\textbackslash)Net: Dual-Branch Disturbance Disentangling Network for Facial Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3474085.3475249",
        "doi": "10.1145/3474085.3475249"
    },
    "yang_facial_2021": {
        "title": "Facial Action Unit-based Deep Learning Framework for Spotting Macro- and Micro-expressions in Long Video Sequences",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3474085.3479209",
        "doi": "10.1145/3474085.3479209"
    },
    "zhang_facial_2021": {
        "title": "Facial Action Unit Detection with Local Key Facial Sub-region based Multi-label Classification for Micro-expression Analysis",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM FME",
        "url": "https://doi.org/10.1145/3476100.3484462",
        "doi": "10.1145/3476100.3484462"
    },
    "zhang_facial_2021-1": {
        "title": "Facial Prior Based First Order Motion Model for Micro-expression Generation",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3474085.3479211",
        "doi": "10.1145/3474085.3479211"
    },
    "zhao_former-dfer_2021": {
        "title": "Former-DFER: Dynamic Facial Expression Recognition Transformer",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3474085.3475292",
        "doi": "10.1145/3474085.3475292"
    },
    "cheng_fme21_2021": {
        "title": "FME'21: Proceedings of the 1st Workshop on Facial Micro-Expression: Advanced Techniques for Facial Expressions Generation and Spotting, Virtual Event, China, 24 October 2021",
        "year": "2021",
        "type": "book",
        "venue": "ACM",
        "url": "https://doi.org/10.1145/3476100",
        "doi": "10.1145/3476100"
    },
    "xie_adversarial_2020": {
        "title": "Adversarial Graph Representation Adaptation for Cross-Domain Facial Expression Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3413822",
        "doi": "10.1145/3394171.3413822"
    },
    "xia_occluded_2020": {
        "title": "Occluded Facial Expression Recognition with Step-Wise Assistance from Unpaired Non-Occluded Images",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3413773",
        "doi": "10.1145/3394171.3413773"
    },
    "jiang_dfew_2020": {
        "title": "DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions in the Wild",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3413620",
        "doi": "10.1145/3394171.3413620"
    },
    "liu_emotiontracker_2020": {
        "title": "EmotionTracker: A Mobile Real-time Facial Expression Tracking System with the Assistant of Public AI-as-a-Service",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3414447",
        "doi": "10.1145/3394171.3414447"
    },
    "ruan_deep_2020": {
        "title": "Deep Disturbance-Disentangled Learning for Facial Expression Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3413907",
        "doi": "10.1145/3394171.3413907"
    },
    "wang_r-fenet_2020": {
        "title": "R-FENet: A Region-based Facial Expression Recognition Method Inspired by Semantic Information of Action Units",
        "year": "2020",
        "type": "inproceedings",
        "venue": "HuMA'20: Proceedings of the 1st International Workshop on Human-centric Multimedia Analysis, Seattle, WA, USA, October12, 2020",
        "url": "https://doi.org/10.1145/3422852.3423482",
        "doi": "10.1145/3422852.3423482"
    },
    "yi_animating_2020": {
        "title": "Animating Through Warping: An Efficient Method for High-Quality Facial Expression Animation",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3413926",
        "doi": "10.1145/3394171.3413926"
    },
    "zhou_uncertainty-aware_2020": {
        "title": "Uncertainty-aware Cross-dataset Facial Expression Recognition via Regularized Conditional Alignment",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3413515",
        "doi": "10.1145/3394171.3413515"
    },
    "zhu_iexpressnet_2020": {
        "title": "IExpressNet: Facial Expression Recognition with Incremental Classes",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3413718",
        "doi": "10.1145/3394171.3413718"
    },
    "pan_occluded_2019": {
        "title": "Occluded Facial Expression Recognition Enhanced through Privileged Information",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3343031.3351049",
        "doi": "10.1145/3343031.3351049"
    },
    "wang_comp-gan_2019": {
        "title": "Comp-GAN: Compositional Generative Adversarial Network in Synthesizing and Recognizing Facial Expression",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3343031.3351032",
        "doi": "10.1145/3343031.3351032"
    },
    "wang_identity-_2019": {
        "title": "Identity- and Pose-Robust Facial Expression Recognition through Adversarial Feature Learning",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3343031.3350872",
        "doi": "10.1145/3343031.3350872"
    },
    "bi_figcons_2023": {
        "title": "FIGCONs: Exploiting FIne-Grained CONstructs of Facial Expressions for Efficient and Accurate Estimation of In-Vehicle Drivers' Statistics",
        "year": "2023",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-35908-8\\_1",
        "doi": "10.1007/978-3-031-35908-8_1"
    },
    "bissinger_emotion_2023": {
        "title": "Emotion Recognition via Facial Expressions to Improve Virtual Communication in Videoconferences",
        "year": "2023",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-35599-8\\_10",
        "doi": "10.1007/978-3-031-35599-8_10"
    },
    "boschetti_never_2023": {
        "title": "Never Correct: The Novel Analysis of Differing Visual (Facial Expression) and Acoustic (Vocalization) Bimodal Displays of the Affective States \"Pain\", \"Pleasure\", and \"Neutral\"",
        "year": "2023",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-35979-8\\_11",
        "doi": "10.1007/978-3-031-35979-8_11"
    },
    "diaz_study_2023": {
        "title": "Study on Different Methods for Recognition of Facial Expressions from the Data Generated by Modern HMDs",
        "year": "2023",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-35989-7\\_38",
        "doi": "10.1007/978-3-031-35989-7_38"
    },
    "yun_analyzing_2023": {
        "title": "Analyzing Learners' Emotion from an HRI Experiment Using Facial Expression Recognition Systems",
        "year": "2023",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-34550-0\\_29",
        "doi": "10.1007/978-3-031-34550-0_29"
    },
    "alharbi_analyzing_2022": {
        "title": "Analyzing Facial Expressions and Body Gestures Through Multimodal Metaphors: An Intelligent E-feedback Interface",
        "year": "2022",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-05409-9\\_22",
        "doi": "10.1007/978-3-031-05409-9_22"
    },
    "binter_quantifying_2022": {
        "title": "Quantifying the Rating Performance of Ambiguous and Unambiguous Facial Expression Perceptions Under Conditions of Stress by Using Wearable Sensors",
        "year": "2022",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-17615-9\\_36",
        "doi": "10.1007/978-3-031-17615-9_36"
    },
    "lee_measurements_2022": {
        "title": "Measurements and Interventions to Improve Student Engagement Through Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-05887-5\\_20",
        "doi": "10.1007/978-3-031-05887-5_20"
    },
    "montedori_novel_2022": {
        "title": "A Novel System Based on a Smart Toy Responding to Child's Facial Expressions: Potential Use in Early Treatment of Autism Spectrum Disorders",
        "year": "2022",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-19679-9\\_24",
        "doi": "10.1007/978-3-031-19679-9_24"
    },
    "wang_assessing_2022": {
        "title": "Assessing the Effectiveness of Digital Advertising for Green Products: A Facial Expression Evaluation Approach",
        "year": "2022",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-05544-7\\_17",
        "doi": "10.1007/978-3-031-05544-7_17"
    },
    "zhang_facial_2022": {
        "title": "Facial Expression Change Recognition on Neutral-Negative Axis Based on Siamese-Structure Deep Neural Network",
        "year": "2022",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-06053-3\\_40",
        "doi": "10.1007/978-3-031-06053-3_40"
    },
    "giroux_guidelines_2021": {
        "title": "Guidelines for Collecting Automatic Facial Expression Detection Data Synchronized with a Dynamic Stimulus in Remote Moderated User Tests",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78462-1\\_18",
        "doi": "10.1007/978-3-030-78462-1_18"
    },
    "hayashi_touchless_2021": {
        "title": "Touchless Information Provision and Facial Expression Training Using Kinect",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78642-7\\_13",
        "doi": "10.1007/978-3-030-78642-7_13"
    },
    "masui_performeyebrow_2021": {
        "title": "PerformEyebrow: Design and Implementation of an Artificial Eyebrow Device Enabling Augmented Facial Expression",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78468-3\\_40",
        "doi": "10.1007/978-3-030-78468-3_40"
    },
    "namikawa_emojicam_2021": {
        "title": "EmojiCam: Emoji-Assisted Video Communication System Leveraging Facial Expressions",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78468-3\\_42",
        "doi": "10.1007/978-3-030-78468-3_42"
    },
    "nonis_building_2021": {
        "title": "Building an Ecologically Valid Facial Expression Database - Behind the Scenes",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78092-0\\_42",
        "doi": "10.1007/978-3-030-78092-0_42"
    },
    "raja_effect_2021": {
        "title": "Effect of Emotion Synchronization in Robot Facial Expressions",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78642-7\\_24",
        "doi": "10.1007/978-3-030-78642-7_24"
    },
    "sanda_effectiveness_2021": {
        "title": "Effectiveness of Manga Technique in Expressing Facial Expressions of Welfare Robot",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78642-7\\_25",
        "doi": "10.1007/978-3-030-78642-7_25"
    },
    "shinto_new_2021": {
        "title": "A New Algorithm to Find Isometric Maps for Comparison and Exchange of Facial Expression Perceptions",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78462-1\\_46",
        "doi": "10.1007/978-3-030-78462-1_46"
    },
    "shinto_definition_2021": {
        "title": "Definition and Estimation of Dimension in Facial Expression Space",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78462-1\\_47",
        "doi": "10.1007/978-3-030-78462-1_47"
    },
    "yu_measuring_2021": {
        "title": "Measuring and Integrating Facial Expressions and Head Pose as Indicators of Engagement and Affect in Tutoring Systems",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-77873-6\\_16",
        "doi": "10.1007/978-3-030-77873-6_16"
    },
    "ceccacci_preliminary_2020": {
        "title": "A Preliminary Investigation Towards the Application of Facial Expression Analysis to Enable an Emotion-Aware Car Interface",
        "year": "2020",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-49108-6\\_36",
        "doi": "10.1007/978-3-030-49108-6_36"
    },
    "fleury_tool_2020": {
        "title": "A Tool to Support Players Affective States Assessment Based on Facial Expressions Analysis",
        "year": "2020",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-50164-8\\_3",
        "doi": "10.1007/978-3-030-50164-8_3"
    },
    "kajihara_emotion_2020": {
        "title": "Emotion Synchronization Method for Robot Facial Expression",
        "year": "2020",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-49062-1\\_44",
        "doi": "10.1007/978-3-030-49062-1_44"
    },
    "kanaya_cross-cultural_2020": {
        "title": "Cross-Cultural Design of Facial Expressions of Robots",
        "year": "2020",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-49788-0\\_45",
        "doi": "10.1007/978-3-030-49788-0_45"
    },
    "nguyen_effectiveness_2020": {
        "title": "Effectiveness of Banner Ads: An Eye Tracking and Facial Expression Analysis",
        "year": "2020",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-50341-3\\_34",
        "doi": "10.1007/978-3-030-50341-3_34"
    },
    "selvig_non-intrusive_2020": {
        "title": "Non-intrusive Measurement of Player Engagement and Emotions - Real-Time Deep Neural Network Analysis of Facial Expressions During Game Play",
        "year": "2020",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-50164-8\\_24",
        "doi": "10.1007/978-3-030-50164-8_24"
    },
    "banire_attention_2019": {
        "title": "Attention Assessment: Evaluation of Facial Expressions of Children with Autism Spectrum Disorder",
        "year": "2019",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-23563-5\\_4",
        "doi": "10.1007/978-3-030-23563-5_4"
    },
    "khanal_classification_2019": {
        "title": "Classification of Physical Exercise Intensity Based on Facial Expression Using Deep Neural Network",
        "year": "2019",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-23563-5\\_36",
        "doi": "10.1007/978-3-030-23563-5_36"
    },
    "kurono_preliminary_2019": {
        "title": "A Preliminary Experiment on the Estimation of Emotion Using Facial Expression and Biological Signals",
        "year": "2019",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-22643-5\\_10",
        "doi": "10.1007/978-3-030-22643-5_10"
    },
    "okubo_proposal_2019": {
        "title": "A Proposal of Video Evaluation Method Using Facial Expression for Video Recommendation System",
        "year": "2019",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-22649-7\\_21",
        "doi": "10.1007/978-3-030-22649-7_21"
    },
    "shinto_how_2019": {
        "title": "How to Compare and Exchange Facial Expression Perceptions Between Different Individuals with Riemann Geometry",
        "year": "2019",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-22643-5\\_12",
        "doi": "10.1007/978-3-030-22643-5_12"
    },
    "sumiya_transform_2019": {
        "title": "Transform Facial Expression Space to Euclidean Space Using Riemann Normal Coordinates and Its Applications",
        "year": "2019",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-22643-5\\_13",
        "doi": "10.1007/978-3-030-22643-5_13"
    },
    "zheng_facial_2019": {
        "title": "Facial Expression Recognition for Children: Can Existing Methods Tuned for Adults Be Adopted for Children?",
        "year": "2019",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-22643-5\\_16",
        "doi": "10.1007/978-3-030-22643-5_16"
    },
    "liao_sequence-level_2024": {
        "title": "Sequence-level affective level estimation based on pyramidal facial expression features",
        "year": "2024",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2023.109958",
        "doi": "10.1016/J.PATCOG.2023.109958"
    },
    "al-sumaidaee_spatio-temporal_2023": {
        "title": "Spatio-temporal modelling with multi-gradient features and elongated quinary pattern descriptor for dynamic facial expression recognition",
        "year": "2023",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2023.109647",
        "doi": "10.1016/J.PATCOG.2023.109647"
    },
    "li_multi-scale_2023": {
        "title": "Multi-Scale correlation module for video-based facial expression recognition in the wild",
        "year": "2023",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2023.109691",
        "doi": "10.1016/J.PATCOG.2023.109691"
    },
    "liu_joint_2023": {
        "title": "Joint spatial and scale attention network for multi-view facial expression recognition",
        "year": "2023",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2023.109496",
        "doi": "10.1016/J.PATCOG.2023.109496"
    },
    "liu_expression_2023": {
        "title": "Expression snippet transformer for robust video-based facial expression recognition",
        "year": "2023",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2023.109368",
        "doi": "10.1016/J.PATCOG.2023.109368"
    },
    "sun_discriminatively_2023": {
        "title": "A discriminatively deep fusion approach with improved conditional GAN (im-cGAN) for facial expression recognition",
        "year": "2023",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2022.109157",
        "doi": "10.1016/J.PATCOG.2022.109157"
    },
    "chen_orthogonal_2022": {
        "title": "Orthogonal channel attention-based multi-task learning for multi-view facial expression recognition",
        "year": "2022",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2022.108753",
        "doi": "10.1016/J.PATCOG.2022.108753"
    },
    "yu_co-attentive_2022": {
        "title": "Co-attentive multi-task convolutional neural network for facial expression recognition",
        "year": "2022",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2021.108401",
        "doi": "10.1016/J.PATCOG.2021.108401"
    },
    "zhang_improving_2022": {
        "title": "Improving the Facial Expression Recognition and Its Interpretability via Generating Expression Pattern-map",
        "year": "2022",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2022.108737",
        "doi": "10.1016/J.PATCOG.2022.108737"
    },
    "liu_mutual_2021": {
        "title": "Mutual information regularized identity-aware facial expression recognition in compressed video",
        "year": "2021",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2021.108105",
        "doi": "10.1016/J.PATCOG.2021.108105"
    },
    "chen_residual_2021": {
        "title": "Residual multi-task learning for facial landmark localization and expression recognition",
        "year": "2021",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2021.107893",
        "doi": "10.1016/J.PATCOG.2021.107893"
    },
    "wang_oaenet_2021": {
        "title": "OAENet: Oriented attention ensemble for accurate facial expression recognition",
        "year": "2021",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2020.107694",
        "doi": "10.1016/J.PATCOG.2020.107694"
    },
    "alexandre_systematic_2020": {
        "title": "Systematic review of 3D facial expression recognition methods",
        "year": "2020",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2019.107108",
        "doi": "10.1016/J.PATCOG.2019.107108"
    },
    "bozorgtabar_exprada_2020": {
        "title": "ExprADA: Adversarial domain adaptation for facial expression analysis",
        "year": "2020",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2019.107111",
        "doi": "10.1016/J.PATCOG.2019.107111"
    },
    "liu_hard_2019": {
        "title": "Hard negative generation for identity-disentangled facial expression recognition",
        "year": "2019",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2018.11.001",
        "doi": "10.1016/J.PATCOG.2018.11.001"
    },
    "xie_deep_2019": {
        "title": "Deep multi-path convolutional neural network joint with salient region attention for facial expression recognition",
        "year": "2019",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2019.03.019",
        "doi": "10.1016/J.PATCOG.2019.03.019"
    },
    "xie_sparse_2019": {
        "title": "Sparse deep feature learning for facial expression recognition",
        "year": "2019",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2019.106966",
        "doi": "10.1016/J.PATCOG.2019.106966"
    },
    "liang_privacy-protected_2023": {
        "title": "Privacy-Protected Facial Expression Recognition Augmented by High-Resolution Facial Images",
        "year": "2023",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia and Expo, ICME 2023, Brisbane, Australia, July 10-14, 2023",
        "url": "https://doi.org/10.1109/ICME55011.2023.00236",
        "doi": "10.1109/ICME55011.2023.00236"
    },
    "sun_semi-supervised_2023": {
        "title": "Semi-Supervised Facial Expression Recognition by Exploring False Pseudo-Labels",
        "year": "2023",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia and Expo, ICME 2023, Brisbane, Australia, July 10-14, 2023",
        "url": "https://doi.org/10.1109/ICME55011.2023.00048",
        "doi": "10.1109/ICME55011.2023.00048"
    },
    "zhang_expression-guided_2023": {
        "title": "Expression-Guided Attention GAN for Fine-Grained Facial Expression Editing",
        "year": "2023",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia and Expo, ICME 2023, Brisbane, Australia, July 10-14, 2023",
        "url": "https://doi.org/10.1109/ICME55011.2023.00045",
        "doi": "10.1109/ICME55011.2023.00045"
    },
    "wang_dmbox2s_2022": {
        "title": "D\\textbackslash(\\textasciicircum\\textbackslashmbox2\\textbackslash)S: Dynamic Distribution Supervision for Multi-Label Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia and Expo, ICME 2022, Taipei, Taiwan, July 18-22, 2022",
        "url": "https://doi.org/10.1109/ICME52920.2022.9859687",
        "doi": "10.1109/ICME52920.2022.9859687"
    },
    "yan_mitigating_2022": {
        "title": "Mitigating Label-Noise for Facial Expression Recognition in the Wild",
        "year": "2022",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia and Expo, ICME 2022, Taipei, Taiwan, July 18-22, 2022",
        "url": "https://doi.org/10.1109/ICME52920.2022.9859818",
        "doi": "10.1109/ICME52920.2022.9859818"
    },
    "zhu_cmanet_2022": {
        "title": "CMANET: Curvature-Aware Soft Mask Guided Attention Fusion Network for 2D+3D Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia and Expo, ICME 2022, Taipei, Taiwan, July 18-22, 2022",
        "url": "https://doi.org/10.1109/ICME52920.2022.9859837",
        "doi": "10.1109/ICME52920.2022.9859837"
    },
    "chen_multi-modal_2021": {
        "title": "Multi-Modal Fusion Enhanced Model For Driver's Facial Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "2021 IEEE International Conference on Multimedia \\& Expo Workshops, ICME Workshops, Shenzhen, China, July 5-9, 2021",
        "url": "https://doi.org/10.1109/ICMEW53276.2021.9455983",
        "doi": "10.1109/ICMEW53276.2021.9455983"
    },
    "lo_facial_2021": {
        "title": "Facial Chirality: Using Self-Face Reflection to Learn Discriminative Features for Facial Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "2021 IEEE International Conference on Multimedia and Expo, ICME 2021, Shenzhen, China, July 5-9, 2021",
        "url": "https://doi.org/10.1109/ICME51207.2021.9428120",
        "doi": "10.1109/ICME51207.2021.9428120"
    },
    "su_key_2021": {
        "title": "Key Facial Components Guided Micro-Expression Recognition Based on First \\& Second-Order Motion",
        "year": "2021",
        "type": "inproceedings",
        "venue": "2021 IEEE International Conference on Multimedia and Expo, ICME 2021, Shenzhen, China, July 5-9, 2021",
        "url": "https://doi.org/10.1109/ICME51207.2021.9428407",
        "doi": "10.1109/ICME51207.2021.9428407"
    },
    "sui_ffnet-m_2021": {
        "title": "FFNet-M: Feature Fusion Network with Masks for Multimodal Facial Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "2021 IEEE International Conference on Multimedia and Expo, ICME 2021, Shenzhen, China, July 5-9, 2021",
        "url": "https://doi.org/10.1109/ICME51207.2021.9428100",
        "doi": "10.1109/ICME51207.2021.9428100"
    },
    "wang_attention_2021": {
        "title": "Attention Based Facial Expression Manipulation",
        "year": "2021",
        "type": "inproceedings",
        "venue": "2021 IEEE International Conference on Multimedia \\& Expo Workshops, ICME Workshops, Shenzhen, China, July 5-9, 2021",
        "url": "https://doi.org/10.1109/ICMEW53276.2021.9456007",
        "doi": "10.1109/ICMEW53276.2021.9456007"
    },
    "li_pooling_2019": {
        "title": "Pooling Map Adaptation in Convolutional Neural Network for Facial Expression Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia and Expo, ICME 2019, Shanghai, China, July 8-12, 2019",
        "url": "https://doi.org/10.1109/ICME.2019.00194",
        "doi": "10.1109/ICME.2019.00194"
    },
    "xu_fully_2019": {
        "title": "Fully Automatic Photorealistic Facial Expression and Eye Gaze Transfer with a Single Image",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia \\& Expo Workshops, ICME Workshops 2019, Shanghai, China, July 8-12, 2019",
        "url": "https://doi.org/10.1109/ICMEW.2019.00095",
        "doi": "10.1109/ICMEW.2019.00095"
    },
    "yan_feafa_2019": {
        "title": "FEAFA: A Well-Annotated Dataset for Facial Expression Analysis and 3D Facial Animation",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia \\& Expo Workshops, ICME Workshops 2019, Shanghai, China, July 8-12, 2019",
        "url": "https://doi.org/10.1109/ICMEW.2019.0-104",
        "doi": "10.1109/ICMEW.2019.0-104"
    },
    "imamura_analyzing_2023": {
        "title": "Analyzing Synergetic Functional Spectrum from Head Movements and Facial Expressions in Conversations",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Proceedings of the 25th International Conference on Multimodal Interaction, ICMI 2023, Paris, France, October 9-13, 2023",
        "url": "https://doi.org/10.1145/3577190.3614153",
        "doi": "10.1145/3577190.3614153"
    },
    "kolahdouzi_toward_2023": {
        "title": "Toward Fair Facial Expression Recognition with Improved Distribution Alignment",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Proceedings of the 25th International Conference on Multimodal Interaction, ICMI 2023, Paris, France, October 9-13, 2023",
        "url": "https://doi.org/10.1145/3577190.3614141",
        "doi": "10.1145/3577190.3614141"
    },
    "takir_exploring_2023": {
        "title": "Exploring the Potential of Multimodal Emotion Recognition for Hearing-Impaired Children Using Physiological Signals and Facial Expressions",
        "year": "2023",
        "type": "inproceedings",
        "venue": "International Conference on Multimodal Interaction, ICMI 2023, Companion Volume, Paris, France, October 9-13, 2023",
        "url": "https://doi.org/10.1145/3610661.3616240",
        "doi": "10.1145/3610661.3616240"
    },
    "delbosc_automatic_2022": {
        "title": "Automatic facial expressions, gaze direction and head movements generation of a virtual agent",
        "year": "2022",
        "type": "inproceedings",
        "venue": "International Conference on Multimodal Interaction, ICMI 2022, Companion Volume, Bengaluru, India, November 7-11, 2022",
        "url": "https://doi.org/10.1145/3536220.3558806",
        "doi": "10.1145/3536220.3558806"
    },
    "salman_privacy_2022": {
        "title": "Privacy Preserving Personalization for Video Facial Expression Recognition Using Federated Learning",
        "year": "2022",
        "type": "inproceedings",
        "venue": "International Conference on Multimodal Interaction, ICMI 2022, Bengaluru, India, November 7-11, 2022",
        "url": "https://doi.org/10.1145/3536221.3556614",
        "doi": "10.1145/3536221.3556614"
    },
    "alsofyani_attachment_2021": {
        "title": "Attachment Recognition in School Age Children Based on Automatic Analysis of Facial Expressions and Nonverbal Vocal Behaviour",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICMI '21: International Conference on Multimodal Interaction, Montr\u00e9al, QC, Canada, October 18-22, 2021",
        "url": "https://doi.org/10.1145/3462244.3479905",
        "doi": "10.1145/3462244.3479905"
    },
    "gashi_hierarchical_2021": {
        "title": "Hierarchical Classification and Transfer Learning to Recognize Head Gestures and Facial Expressions Using Earbuds",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICMI '21: International Conference on Multimodal Interaction, Montr\u00e9al, QC, Canada, October 18-22, 2021",
        "url": "https://doi.org/10.1145/3462244.3479921",
        "doi": "10.1145/3462244.3479921"
    },
    "roy_self-supervised_2021": {
        "title": "Self-supervised Contrastive Learning of Multi-view Facial Expressions",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICMI '21: International Conference on Multimodal Interaction, Montr\u00e9al, QC, Canada, October 18-22, 2021",
        "url": "https://doi.org/10.1145/3462244.3479955",
        "doi": "10.1145/3462244.3479955"
    },
    "zhang_facial_2021-2": {
        "title": "Facial Micro-Expression Recognition Based on Multi-Scale Temporal and Spatial Features",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICMI '21 Companion: Companion Publication of the 2021 International Conference on Multimodal Interaction, Montreal, QC, Canada, October 18 - 22, 2021",
        "url": "https://doi.org/10.1145/3461615.3491107",
        "doi": "10.1145/3461615.3491107"
    },
    "shidara_analysis_2020": {
        "title": "Analysis of Mood Changes and Facial Expressions during Cognitive Behavior Therapy through a Virtual Agent",
        "year": "2020",
        "type": "inproceedings",
        "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction, ICMI Companion 2020, Virtual Event, The Netherlands, October, 2020",
        "url": "https://doi.org/10.1145/3395035.3425223",
        "doi": "10.1145/3395035.3425223"
    },
    "ueno_estimating_2020": {
        "title": "Estimating the Intensity of Facial Expressions Accompanying Feedback Responses in Multiparty Video-Mediated Communication",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ICMI '20: International Conference on Multimodal Interaction, Virtual Event, The Netherlands, October 25-29, 2020",
        "url": "https://doi.org/10.1145/3382507.3418878",
        "doi": "10.1145/3382507.3418878"
    },
    "fahim_effect_2019": {
        "title": "Effect of Feedback on Users' Immediate Emotions: Analysis of Facial Expressions during a Simulated Target Detection Task",
        "year": "2019",
        "type": "inproceedings",
        "venue": "International Conference on Multimodal Interaction, ICMI 2019, Suzhou, China, October 14-18, 2019",
        "url": "https://doi.org/10.1145/3340555.3353732",
        "doi": "10.1145/3340555.3353732"
    },
    "song_facial_2019": {
        "title": "Facial Expression Recognition via Relation-based Conditional Generative Adversarial Network",
        "year": "2019",
        "type": "inproceedings",
        "venue": "International Conference on Multimodal Interaction, ICMI 2019, Suzhou, China, October 14-18, 2019",
        "url": "https://doi.org/10.1145/3340555.3353753",
        "doi": "10.1145/3340555.3353753"
    },
    "wu_continuous_2019": {
        "title": "Continuous Emotion Recognition in Videos by Fusing Facial Expression, Head Pose and Eye Gaze",
        "year": "2019",
        "type": "inproceedings",
        "venue": "International Conference on Multimodal Interaction, ICMI 2019, Suzhou, China, October 14-18, 2019",
        "url": "https://doi.org/10.1145/3340555.3353739",
        "doi": "10.1145/3340555.3353739"
    },
    "chen_understanding_2021": {
        "title": "Understanding and Mitigating Annotation Bias in Facial Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICCV",
        "url": "https://doi.org/10.1109/ICCV48922.2021.01471",
        "doi": "10.1109/ICCV48922.2021.01471"
    },
    "xue_transfer_2021": {
        "title": "TransFER: Learning Relation-aware Facial Expression Representations with Transformers",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICCV",
        "url": "https://doi.org/10.1109/ICCV48922.2021.00358",
        "doi": "10.1109/ICCV48922.2021.00358"
    },
    "jin_mtmsn_2021": {
        "title": "MTMSN: Multi-Task and Multi-Modal Sequence Network for Facial Action Unit and Expression Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICCVW",
        "url": "https://doi.org/10.1109/ICCVW54120.2021.00401",
        "doi": "10.1109/ICCVW54120.2021.00401"
    },
    "shome_fedaffect_2021": {
        "title": "FedAffect: Few-shot federated learning for facial expression recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICCVW",
        "url": "https://doi.org/10.1109/ICCVW54120.2021.00463",
        "doi": "10.1109/ICCVW54120.2021.00463"
    },
    "zheng_student-teacher_2021": {
        "title": "Student-Teacher Oneness: A Storage-efficient approach that improves facial expression recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICCVW",
        "url": "https://doi.org/10.1109/ICCVW54120.2021.00453",
        "doi": "10.1109/ICCVW54120.2021.00453"
    },
    "bao_single-image_2019": {
        "title": "Single-Image Facial Expression Recognition Using Deep 3D Re-Centralization",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ICCVW",
        "url": "https://doi.org/10.1109/ICCVW.2019.00202",
        "doi": "10.1109/ICCVW.2019.00202"
    },
    "onizuka_landmark-guided_2019": {
        "title": "Landmark-Guided Deformation Transfer of Template Facial Expressions for Automatic Generation of Avatar Blendshapes",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ICCVW",
        "url": "https://doi.org/10.1109/ICCVW.2019.00265",
        "doi": "10.1109/ICCVW.2019.00265"
    },
    "shahar_micro_2019": {
        "title": "Micro Expression Classification using Facial Color and Deep Learning Methods",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ICCVW",
        "url": "https://doi.org/10.1109/ICCVW.2019.00207",
        "doi": "10.1109/ICCVW.2019.00207"
    },
    "hung_boosting_2022": {
        "title": "Boosting Facial Emotion Recognition by Using GANs to Augment Small Facial Expression Dataset",
        "year": "2022",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN55064.2022.9892096",
        "doi": "10.1109/IJCNN55064.2022.9892096"
    },
    "lakhani_facial_2022": {
        "title": "Facial Expression Recognition of Animated Characters using Deep Learning",
        "year": "2022",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN55064.2022.9892186",
        "doi": "10.1109/IJCNN55064.2022.9892186"
    },
    "parikh_multi-view_2022": {
        "title": "Multi-view Geometry Consistency Network for Facial Micro-Expression Recognition From Various Perspectives",
        "year": "2022",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN55064.2022.9892565",
        "doi": "10.1109/IJCNN55064.2022.9892565"
    },
    "xu_sample_2022": {
        "title": "Sample Self-Revised Network for Cross-Dataset Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN55064.2022.9892500",
        "doi": "10.1109/IJCNN55064.2022.9892500"
    },
    "liu_multi-scale_2021": {
        "title": "Multi-scale Feature Relation Modeling for Facial Expression Restoration",
        "year": "2021",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN52387.2021.9533592",
        "doi": "10.1109/IJCNN52387.2021.9533592"
    },
    "lu_multi-view_2021": {
        "title": "Multi-view Geometry Consistency Network for Facial Micro-Expression Recognition From Various Perspectives",
        "year": "2021",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN52387.2021.9533434",
        "doi": "10.1109/IJCNN52387.2021.9533434"
    },
    "wang_information_2021": {
        "title": "Information Reuse Attention in Convolutional Neural Networks for Facial Expression Recognition in the Wild",
        "year": "2021",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN52387.2021.9534217",
        "doi": "10.1109/IJCNN52387.2021.9534217"
    },
    "yu_visual_2021": {
        "title": "A visual self-attention network for facial expression recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN52387.2021.9534071",
        "doi": "10.1109/IJCNN52387.2021.9534071"
    },
    "abbas_impact_2019": {
        "title": "The Impact of Image Resolution on Facial Expression Analysis with CNNs",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN.2019.8852264",
        "doi": "10.1109/IJCNN.2019.8852264"
    },
    "reddy_spontaneous_2019": {
        "title": "Spontaneous Facial Micro-Expression Recognition using 3D Spatiotemporal Convolutional Neural Networks",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN.2019.8852419",
        "doi": "10.1109/IJCNN.2019.8852419"
    },
    "zamzmi_pain_2019": {
        "title": "Pain Assessment From Facial Expression: Neonatal Convolutional Neural Network (N-CNN)",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN.2019.8851879",
        "doi": "10.1109/IJCNN.2019.8851879"
    },
    "cheong_counterfactual_2022": {
        "title": "Counterfactual Fairness for Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-031-25072-9\\_16",
        "doi": "10.1007/978-3-031-25072-9_16"
    },
    "jeong_ensemble_2022": {
        "title": "Ensemble of Multi-task Learning Networks for Facial Expression Recognition In-the-Wild with Learning from Synthetic Data",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-031-25075-0\\_5",
        "doi": "10.1007/978-3-031-25075-0_5"
    },
    "lei_facial_2022": {
        "title": "Facial Expression Recognition with Mid-level Representation Enhancement and Graph Embedded Uncertainty Suppressing",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-031-25075-0\\_7",
        "doi": "10.1007/978-3-031-25075-0_7"
    },
    "li_facial_2022": {
        "title": "Facial Expression Recognition In-the-Wild with Deep Pre-trained Models",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-031-25075-0\\_14",
        "doi": "10.1007/978-3-031-25075-0_14"
    },
    "lukov_teaching_2022": {
        "title": "Teaching with Soft Label Smoothing for Mitigating Noisy Labels in Facial Expressions",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-031-19775-8\\_38",
        "doi": "10.1007/978-3-031-19775-8_38"
    },
    "papaioannou_mimicme_2022": {
        "title": "MimicME: A Large Scale Diverse 4D Database for Facial Expression Analysis",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-031-20074-8\\_27",
        "doi": "10.1007/978-3-031-20074-8_27"
    },
    "sreenivas_improved_2022": {
        "title": "Improved Cross-Dataset Facial Expression Recognition by Handling Data Imbalance and Feature Confusion",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-031-25072-9\\_17",
        "doi": "10.1007/978-3-031-25072-9_17"
    },
    "zhang_learn_2022": {
        "title": "Learn from All: Erasing Attention Consistency for Noisy Label Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-031-19809-0\\_24",
        "doi": "10.1007/978-3-031-19809-0_24"
    },
    "zou_learn--decompose_2022": {
        "title": "Learn-to-Decompose: Cascaded Decomposition Network for Cross-Domain Few-Shot Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-031-19800-7\\_40",
        "doi": "10.1007/978-3-031-19800-7_40"
    },
    "kumar_noisy_2020": {
        "title": "Noisy Student Training Using Body Language Dataset Improves Facial Expression Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-030-66415-2\\_53",
        "doi": "10.1007/978-3-030-66415-2_53"
    },
    "ling_toward_2020": {
        "title": "Toward Fine-Grained Facial Expression Manipulation",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-030-58604-1\\_3",
        "doi": "10.1007/978-3-030-58604-1_3"
    },
    "potamias_learning_2020": {
        "title": "Learning to Generate Customized Dynamic 3D Facial Expressions",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-030-58526-6\\_17",
        "doi": "10.1007/978-3-030-58526-6_17"
    },
    "silva_recognition_2020": {
        "title": "Recognition of Affective and Grammatical Facial Expressions: A Study for Brazilian Sign Language",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-030-66096-3\\_16",
        "doi": "10.1007/978-3-030-66096-3_16"
    },
    "xu_investigating_2020": {
        "title": "Investigating Bias and Fairness in Facial Expression Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-030-65414-6\\_35",
        "doi": "10.1007/978-3-030-65414-6_35"
    },
    "li_cross-domain_2023": {
        "title": "Cross-Domain Facial Expression Recognition via Contrastive Warm up and Complexity-Aware Self-Training",
        "year": "2023",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2023.3318955",
        "doi": "10.1109/TIP.2023.3318955"
    },
    "liu_facial_2023-1": {
        "title": "Facial Expression Recognition on the High Aggregation Subgraphs",
        "year": "2023",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2023.3290520",
        "doi": "10.1109/TIP.2023.3290520"
    },
    "zhu_knowledge_2023": {
        "title": "Knowledge Conditioned Variational Learning for One-Class Facial Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2023.3293775",
        "doi": "10.1109/TIP.2023.3293775"
    },
    "li_crs-cont_2022": {
        "title": "CRS-CONT: A Well-Trained General Encoder for Facial Expression Analysis",
        "year": "2022",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2022.3186536",
        "doi": "10.1109/TIP.2022.3186536"
    },
    "poux_dynamic_2022": {
        "title": "Dynamic Facial Expression Recognition Under Partial Occlusion With Optical Flow Reconstruction",
        "year": "2022",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2021.3129120",
        "doi": "10.1109/TIP.2021.3129120"
    },
    "jin_learning_2021": {
        "title": "Learning Dynamic Relationships for Facial Expression Recognition Based on Graph Convolutional Network",
        "year": "2021",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2021.3101820",
        "doi": "10.1109/TIP.2021.3101820"
    },
    "li_adaptively_2021": {
        "title": "Adaptively Learning Facial Expression Representation via C-F Labels and Distillation",
        "year": "2021",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2021.3049955",
        "doi": "10.1109/TIP.2021.3049955"
    },
    "shao_explicit_2021": {
        "title": "Explicit Facial Expression Transfer via Fine-Grained Representations",
        "year": "2021",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2021.3073857",
        "doi": "10.1109/TIP.2021.3073857"
    },
    "tang_facial_2021": {
        "title": "Facial Expression Recognition Using Frequency Neural Network",
        "year": "2021",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2020.3037467",
        "doi": "10.1109/TIP.2020.3037467"
    },
    "zhao_learning_2021": {
        "title": "Learning Deep Global Multi-Scale and Local Attention Features for Facial Expression Recognition in the Wild",
        "year": "2021",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2021.3093397",
        "doi": "10.1109/TIP.2021.3093397"
    },
    "fu_semantic_2020": {
        "title": "Semantic Neighborhood-Aware Deep Facial Expression Recognition",
        "year": "2020",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2020.2991510",
        "doi": "10.1109/TIP.2020.2991510"
    },
    "lee_multi-modal_2020": {
        "title": "Multi-Modal Recurrent Attention Networks for Facial Expression Recognition",
        "year": "2020",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2020.2996086",
        "doi": "10.1109/TIP.2020.2996086"
    },
    "pei_attended_2020": {
        "title": "Attended End-to-End Architecture for Age Estimation From Facial Expression Videos",
        "year": "2020",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2019.2948288",
        "doi": "10.1109/TIP.2019.2948288"
    },
    "perveen_facial_2020": {
        "title": "Facial Expression Recognition in Videos Using Dynamic Kernels",
        "year": "2020",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2020.3011846",
        "doi": "10.1109/TIP.2020.3011846"
    },
    "wang_region_2020": {
        "title": "Region Attention Networks for Pose and Occlusion Robust Facial Expression Recognition",
        "year": "2020",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2019.2956143",
        "doi": "10.1109/TIP.2019.2956143"
    },
    "zhang_geometry_2020": {
        "title": "Geometry Guided Pose-Invariant Facial Expression Recognition",
        "year": "2020",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2020.2972114",
        "doi": "10.1109/TIP.2020.2972114"
    },
    "zhang_unified_2020": {
        "title": "A Unified Deep Model for Joint Facial Expression Recognition, Face Synthesis, and Face Alignment",
        "year": "2020",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2020.2991549",
        "doi": "10.1109/TIP.2020.2991549"
    },
    "li_reliable_2019": {
        "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition",
        "year": "2019",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2018.2868382",
        "doi": "10.1109/TIP.2018.2868382"
    },
    "li_occlusion_2019": {
        "title": "Occlusion Aware Facial Expression Recognition Using CNN With Attention Mechanism",
        "year": "2019",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2018.2886767",
        "doi": "10.1109/TIP.2018.2886767"
    },
    "choi_combining_2023": {
        "title": "Combining Deep Convolutional Neural Networks With Stochastic Ensemble Weight Optimization for Facial Expression Recognition in the Wild",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2021.3121547",
        "doi": "10.1109/TMM.2021.3121547"
    },
    "huang_facial_2023": {
        "title": "Facial Expression Guided Diagnosis of Parkinson's Disease via High-Quality Data Augmentation",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2022.3216961",
        "doi": "10.1109/TMM.2022.3216961"
    },
    "li_deep_2023": {
        "title": "Deep Margin-Sensitive Representation Learning for Cross-Domain Facial Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2022.3141604",
        "doi": "10.1109/TMM.2022.3141604"
    },
    "huang_identity-aware_2022": {
        "title": "Identity-Aware Facial Expression Recognition Via Deep Metric Learning Based on Synthesized Images",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2021.3096068",
        "doi": "10.1109/TMM.2021.3096068"
    },
    "zhang_weakly-supervised_2022": {
        "title": "Weakly-Supervised Facial Expression Recognition in the Wild With Noisy Data",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2021.3072786",
        "doi": "10.1109/TMM.2021.3072786"
    },
    "lin_orthogonalization-guided_2021": {
        "title": "Orthogonalization-Guided Feature Fusion Network for Multimodal 2D+3D Facial Expression Recognition",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2020.3001497",
        "doi": "10.1109/TMM.2020.3001497"
    },
    "pei_monocular_2021": {
        "title": "Monocular 3D Facial Expression Features for Continuous Affect Recognition",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2020.3026894",
        "doi": "10.1109/TMM.2020.3026894"
    },
    "lou_realistic_2020": {
        "title": "Realistic Facial Expression Reconstruction for VR HMD Users",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2019.2933338",
        "doi": "10.1109/TMM.2019.2933338"
    },
    "wen_dynamic_2020": {
        "title": "Dynamic Objectives Learning for Facial Expression Recognition",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2020.2966858",
        "doi": "10.1109/TMM.2020.2966858"
    },
    "yan_joint_2020": {
        "title": "Joint Deep Learning of Facial Expression Synthesis and Recognition",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2019.2962317",
        "doi": "10.1109/TMM.2019.2962317"
    },
    "agarwal_synthesis_2019": {
        "title": "Synthesis of Realistic Facial Expressions Using Expression Map",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2018.2871417",
        "doi": "10.1109/TMM.2018.2871417"
    },
    "xie_facial_2019": {
        "title": "Facial Expression Recognition Using Hierarchical Features With Deep Comprehensive Multipatches Aggregation Convolutional Neural Networks",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2018.2844085",
        "doi": "10.1109/TMM.2018.2844085"
    },
    "li_casmembox3_2023": {
        "title": "CAS(ME)\\textbackslash(\\textasciicircum\\textbackslashmbox3\\textbackslash): A Third Generation Facial Spontaneous Micro-Expression Database With Depth Information and High Ecological Validity",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2022.3174895",
        "doi": "10.1109/TPAMI.2022.3174895"
    },
    "ben_video-based_2022": {
        "title": "Video-Based Facial Micro-Expression Analysis: A Survey of Datasets, Features and Algorithms",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2021.3067464",
        "doi": "10.1109/TPAMI.2021.3067464"
    },
    "chen_cross-domain_2022": {
        "title": "Cross-Domain Facial Expression Recognition: A Unified Evaluation Benchmark and Adversarial Graph Learning",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2021.3131222",
        "doi": "10.1109/TPAMI.2021.3131222"
    },
    "otberdout_dynamic_2022": {
        "title": "Dynamic Facial Expression Generation on Hilbert Hypersphere With Conditional Wasserstein Generative Adversarial Nets",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2020.3002500",
        "doi": "10.1109/TPAMI.2020.3002500"
    },
    "hassan_automatic_2021": {
        "title": "Automatic Detection of Pain from Facial Expressions: A Survey",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2019.2958341",
        "doi": "10.1109/TPAMI.2019.2958341"
    },
    "tanfous_sparse_2020": {
        "title": "Sparse Coding of Shape Trajectories for Facial Expression and Action Recognition",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2019.2932979",
        "doi": "10.1109/TPAMI.2019.2932979"
    },
    "wang_novel_2020": {
        "title": "A Novel Dynamic Model Capturing Spatial and Temporal Patterns for Facial Expression Analysis",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2019.2911937",
        "doi": "10.1109/TPAMI.2019.2911937"
    },
    "du_lion_2023": {
        "title": "LION: Label Disambiguation for Semi-supervised Facial Expression Recognition with Progressive Negative Learning",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China",
        "url": "https://doi.org/10.24963/ijcai.2023/78",
        "doi": "10.24963/IJCAI.2023/78"
    },
    "liu_cross-domain_2023": {
        "title": "Cross-Domain Facial Expression Recognition via Disentangling Identity Representation",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China",
        "url": "https://doi.org/10.24963/ijcai.2023/135",
        "doi": "10.24963/IJCAI.2023/135"
    },
    "dominguez-catena_assessing_2022": {
        "title": "Assessing Demographic Bias Transfer from Dataset to Model: A Case Study in Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "Proceedings of the Workshop on Artificial Intelligence Safety 2022 (AISafety 2022) co-located with the Thirty-First International Joint Conference on Artificial Intelligence and the Twenty-Fifth European Conference on Artificial Intelligence (IJCAI-ECAI-2022), Vienna, Austria, July 24-25, 2022",
        "url": "https://ceur-ws.org/Vol-3215/12.pdf",
        "doi": null
    },
    "tran_automatic_2022": {
        "title": "Automatic Multimodal Emotion Recognition Using Facial Expression, Voice, and Text",
        "year": "2022",
        "type": "inproceedings",
        "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022",
        "url": "https://doi.org/10.24963/ijcai.2022/843",
        "doi": "10.24963/IJCAI.2022/843"
    },
    "zhang_weakly_2020": {
        "title": "Weakly Supervised Local-Global Relation Network for Facial Expression Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020",
        "url": "https://doi.org/10.24963/ijcai.2020/145",
        "doi": "10.24963/IJCAI.2020/145"
    },
    "li_intensity-aware_2023": {
        "title": "Intensity-Aware Loss for Dynamic Facial Expression Recognition in the Wild",
        "year": "2023",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v37i1.25077",
        "doi": "10.1609/AAAI.V37I1.25077"
    },
    "luo_learning_2023": {
        "title": "Learning Deep Hierarchical Features with Spatial Regularization for One-Class Facial Expression Recognition",
        "year": "2023",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v37i5.25749",
        "doi": "10.1609/AAAI.V37I5.25749"
    },
    "wang_music--facial_2023": {
        "title": "Music-to-Facial Expressions: Emotion-Based Music Visualization for the Hearing Impaired",
        "year": "2023",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v37i13.26912",
        "doi": "10.1609/AAAI.V37I13.26912"
    },
    "zheng_attack_2023": {
        "title": "Attack Can Benefit: An Adversarial Approach to Recognizing Facial Expressions under Noisy Annotations",
        "year": "2023",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v37i3.25477",
        "doi": "10.1609/AAAI.V37I3.25477"
    },
    "mo_towards_2022": {
        "title": "Towards Accurate Facial Motion Retargeting with Identity-Consistent and Expression-Exclusive Constraints",
        "year": "2022",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v36i2.20093",
        "doi": "10.1609/AAAI.V36I2.20093"
    },
    "zou_when_2022": {
        "title": "When Facial Expression Recognition Meets Few-Shot Learning: A Joint and Alternate Learning Framework",
        "year": "2022",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v36i5.20474",
        "doi": "10.1609/AAAI.V36I5.20474"
    },
    "das_interpretable_2021": {
        "title": "Interpretable Self-Supervised Facial Micro-Expression Learning to Predict Cognitive State and Neurological Disorders",
        "year": "2021",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v35i1.16164",
        "doi": "10.1609/AAAI.V35I1.16164"
    },
    "zhao_robust_2021": {
        "title": "Robust Lightweight Facial Expression Recognition Network with Label Distribution Training",
        "year": "2021",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v35i4.16465",
        "doi": "10.1609/AAAI.V35I4.16465"
    },
    "baddar_mode_2019": {
        "title": "Mode Variational LSTM Robust to Unseen Modes of Variation: Application to Facial Expression Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v33i01.33013215",
        "doi": "10.1609/AAAI.V33I01.33013215"
    },
    "fan_controllable_2019": {
        "title": "Controllable Image-to-Video Translation: A Case Study on Facial Expression Generation",
        "year": "2019",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v33i01.33013510",
        "doi": "10.1609/AAAI.V33I01.33013510"
    },
    "ding_exprgan_2018": {
        "title": "ExprGAN: Facial Expression Editing With Controllable Expression Intensity",
        "year": "2018",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v32i1.12277",
        "doi": "10.1609/AAAI.V32I1.12277"
    },
    "saxena_relating_2018": {
        "title": "Relating Children's Automatically Detected Facial Expressions to Their Behavior in RoboTutor",
        "year": "2018",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v32i1.12190",
        "doi": "10.1609/AAAI.V32I1.12190"
    },
    "akram_sargan_2023": {
        "title": "SARGAN: Spatial Attention-Based Residuals for Facial Expression Manipulation",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2023.3255243",
        "doi": "10.1109/TCSVT.2023.3255243"
    },
    "chen_multi-relations_2023": {
        "title": "Multi-Relations Aware Network for In-the-Wild Facial Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2023.3234312",
        "doi": "10.1109/TCSVT.2023.3234312"
    },
    "gu_toward_2023": {
        "title": "Toward Facial Expression Recognition in the Wild via Noise-Tolerant Network",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2022.3220669",
        "doi": "10.1109/TCSVT.2022.3220669"
    },
    "huang_convolution_2022": {
        "title": "Convolution by Multiplication: Accelerated Two- Stream Fourier Domain Convolutional Neural Network for Facial Expression Recognition",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2021.3073558",
        "doi": "10.1109/TCSVT.2021.3073558"
    },
    "li_self-supervised_2022": {
        "title": "Self-Supervised Exclusive-Inclusive Interactive Learning for Multi-Label Facial Expression Recognition in the Wild",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2021.3103782",
        "doi": "10.1109/TCSVT.2021.3103782"
    },
    "li_learning_2022": {
        "title": "Learning Informative and Discriminative Features for Facial Expression Recognition in the Wild",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2021.3103760",
        "doi": "10.1109/TCSVT.2021.3103760"
    },
    "liu_adaptive_2022": {
        "title": "Adaptive Multilayer Perceptual Attention Network for Facial Expression Recognition",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2022.3165321",
        "doi": "10.1109/TCSVT.2022.3165321"
    },
    "wang_light_2022": {
        "title": "Light Attention Embedding for Facial Expression Recognition",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2021.3083326",
        "doi": "10.1109/TCSVT.2021.3083326"
    },
    "xia_local_2022": {
        "title": "Local and Global Perception Generative Adversarial Network for Facial Expression Synthesis",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2021.3074032",
        "doi": "10.1109/TCSVT.2021.3074032"
    },
    "xie_triplet_2022": {
        "title": "Triplet Loss With Multistage Outlier Suppression and Class-Pair Margins for Facial Expression Recognition",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2021.3063052",
        "doi": "10.1109/TCSVT.2021.3063052"
    },
    "zhang_joint_2022": {
        "title": "Joint Expression Synthesis and Representation Learning for Facial Expression Recognition",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2021.3056098",
        "doi": "10.1109/TCSVT.2021.3056098"
    },
    "xie_facial_2021": {
        "title": "Facial Expression Recognition With Two-Branch Disentangled Generative Adversarial Network",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2020.3024201",
        "doi": "10.1109/TCSVT.2020.3024201"
    },
    "taha_learned_2020": {
        "title": "Learned 3D Shape Representations Using Fused Geometrically Augmented Images: Application to Facial Expression and Action Unit Detection",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2020.2984241",
        "doi": "10.1109/TCSVT.2020.2984241"
    },
    "gjoreski_ocosense_2023": {
        "title": "OCOsense Glasses - Monitoring Facial Gestures and Expressions for Augmented Human-Computer Interaction: OCOsense Glasses for Monitoring Facial Gestures and Expressions",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems, CHI EA 2023, Hamburg, Germany, April 23-28, 2023",
        "url": "https://doi.org/10.1145/3544549.3583918",
        "doi": "10.1145/3544549.3583918"
    },
    "guo_texonmask_2023": {
        "title": "TexonMask: Facial Expression Recognition Using Textile Electrodes on Commodity Facemasks",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023, Hamburg, Germany, April 23-28, 2023",
        "url": "https://doi.org/10.1145/3544548.3581295",
        "doi": "10.1145/3544548.3581295"
    },
    "kar_expressense_2023": {
        "title": "ExpresSense: Exploring a Standalone Smartphone to Sense Engagement of Users from Facial Expressions Using Acoustic Sensing",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023, Hamburg, Germany, April 23-28, 2023",
        "url": "https://doi.org/10.1145/3544548.3581235",
        "doi": "10.1145/3544548.3581235"
    },
    "kimmel_lets_2023": {
        "title": "Let's Face It: Influence of Facial Expressions on Social Presence in Collaborative Virtual Reality",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023, Hamburg, Germany, April 23-28, 2023",
        "url": "https://doi.org/10.1145/3544548.3580707",
        "doi": "10.1145/3544548.3580707"
    },
    "yang_affective_2023": {
        "title": "Affective Profile Pictures: Exploring the Effects of Changing Facial Expressions in Profile Pictures on Text-Based Communication",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023, Hamburg, Germany, April 23-28, 2023",
        "url": "https://doi.org/10.1145/3544548.3581061",
        "doi": "10.1145/3544548.3581061"
    },
    "hong_evoker_2022": {
        "title": "Evoker: Narrative-based Facial Expression Game for Emotional Development of Adolescents",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CHI '22: CHI Conference on Human Factors in Computing Systems, New Orleans, LA, USA, 29 April 2022 - 5 May 2022, Extended Abstracts",
        "url": "https://doi.org/10.1145/3491101.3516486",
        "doi": "10.1145/3491101.3516486"
    },
    "herdel_drone_2021": {
        "title": "Drone in Love: Emotional Perception of Facial Expressions on Flying Robots",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CHI '21: CHI Conference on Human Factors in Computing Systems, Virtual Event / Yokohama, Japan, May 8-13, 2021",
        "url": "https://doi.org/10.1145/3411764.3445495",
        "doi": "10.1145/3411764.3445495"
    },
    "yan_frownonerror_2020": {
        "title": "FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions",
        "year": "2020",
        "type": "inproceedings",
        "venue": "CHI '20: CHI Conference on Human Factors in Computing Systems, Honolulu, HI, USA, April 25-30, 2020",
        "url": "https://doi.org/10.1145/3313831.3376810",
        "doi": "10.1145/3313831.3376810"
    },
    "eskimez_speech_2022": {
        "title": "Speech Driven Talking Face Generation From a Single Image and an Emotion Condition",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2021.3099900",
        "doi": "10.1109/TMM.2021.3099900"
    },
    "yang_image-text_2021": {
        "title": "Image-Text Multimodal Emotion Classification via Multi-View Attentional Network",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2020.3035277",
        "doi": "10.1109/TMM.2020.3035277"
    },
    "yao_apse_2021": {
        "title": "APSE: Attention-Aware Polarity-Sensitive Embedding for Emotion-Based Image Retrieval",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2020.3042664",
        "doi": "10.1109/TMM.2020.3042664"
    },
    "zhang_emotion_2021": {
        "title": "Emotion Attention-Aware Collaborative Deep Reinforcement Learning for Image Cropping",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2020.3013350",
        "doi": "10.1109/TMM.2020.3013350"
    },
    "zhang_weakly_2021": {
        "title": "Weakly Supervised Emotion Intensity Prediction for Recomi/tmi40.htmlgnition of Emotions in Images",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2020.3007352",
        "doi": "10.1109/TMM.2020.3007352"
    },
    "zhang_exploring_2020": {
        "title": "Exploring Discriminative Representations for Image Emotion Recognition With CNNs",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2019.2928998",
        "doi": "10.1109/TMM.2019.2928998"
    },
    "dernelakis_transformation_2022": {
        "title": "Transformation of Emotions in Images Using Poisson Blended Generative Adversarial Networks (Student Abstract)",
        "year": "2022",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v36i11.21603",
        "doi": "10.1609/AAAI.V36I11.21603"
    },
    "garg_video_2022": {
        "title": "From Video to Images: Contrastive Pretraining for Emotion Recognition from Single Image (Student Abstract)",
        "year": "2022",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v36i11.21612",
        "doi": "10.1609/AAAI.V36I11.21612"
    },
    "nguyen_hcilab_2022": {
        "title": "HCILab at Memotion 2.0 2022: Analysis of Sentiment, Emotion and Intensity of Emotion Classes from Meme Images using Single and Multi Modalities (short paper)",
        "year": "2022",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://ceur-ws.org/Vol-3199/paper12.pdf",
        "doi": null
    },
    "xiong_structured_2019": {
        "title": "Structured and Sparse Annotations for Image Emotion Distribution Learning",
        "year": "2019",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v33i01.3301363",
        "doi": "10.1609/AAAI.V33I01.3301363"
    },
    "zhao_cycleemotiongan_2019": {
        "title": "CycleEmotionGAN: Emotional Semantic Consistency Preserved CycleGAN for Adapting Image Emotions",
        "year": "2019",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v33i01.33012620",
        "doi": "10.1609/AAAI.V33I01.33012620"
    },
    "gangji_using_2017": {
        "title": "Using Co-Captured Face, Gaze, and Verbal Reactions to Images of Varying Emotional Content for Analysis and Semantic Alignment",
        "year": "2017",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "http://aaai.org/ocs/index.php/WS/AAAIW17/paper/view/15116",
        "doi": null
    },
    "yang_social_2016": {
        "title": "Social Role-Aware Emotion Contagion in Image Social Networks",
        "year": "2016",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v30i1.10003",
        "doi": "10.1609/AAAI.V30I1.10003"
    },
    "you_building_2016": {
        "title": "Building a Large Scale Dataset for Image Emotion Recognition: The Fine Print and The Benchmark",
        "year": "2016",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v30i1.9987",
        "doi": "10.1609/AAAI.V30I1.9987"
    },
    "zhao_affective_2016": {
        "title": "Affective Computing and Applications of Image Emotion Perceptions",
        "year": "2016",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v30i1.9808",
        "doi": "10.1609/AAAI.V30I1.9808"
    },
    "zhao_user-centric_2016": {
        "title": "User-Centric Affective Computing of Image Emotion Perceptions",
        "year": "2016",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v30i1.9947",
        "doi": "10.1609/AAAI.V30I1.9947"
    },
    "narula_preserving_2020": {
        "title": "Preserving Privacy in Image-based Emotion Recognition through User Anonymization",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ICMI '20: International Conference on Multimodal Interaction, Virtual Event, The Netherlands, October 25-29, 2020",
        "url": "https://doi.org/10.1145/3382507.3418833",
        "doi": "10.1145/3382507.3418833"
    },
    "pons_et-cyclegan_2020": {
        "title": "ET-CycleGAN: Generating Thermal Images from Images in the Visible Spectrum for Facial Emotion Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction, ICMI Companion 2020, Virtual Event, The Netherlands, October, 2020",
        "url": "https://doi.org/10.1145/3395035.3425258",
        "doi": "10.1145/3395035.3425258"
    },
    "wang_cascade_2018": {
        "title": "Cascade Attention Networks For Group Emotion Recognition with Face, Body and Image Cues",
        "year": "2018",
        "type": "inproceedings",
        "venue": "Proceedings of the 2018 on International Conference on Multimodal Interaction, ICMI 2018, Boulder, CO, USA, October 16-20, 2018",
        "url": "https://doi.org/10.1145/3242969.3264991",
        "doi": "10.1145/3242969.3264991"
    },
    "guo_group-level_2017": {
        "title": "Group-level emotion recognition using deep models on image scene, faces, and skeletons",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction, ICMI 2017, Glasgow, United Kingdom, November 13 - 17, 2017",
        "url": "https://doi.org/10.1145/3136755.3143017",
        "doi": "10.1145/3136755.3143017"
    },
    "tan_group_2017": {
        "title": "Group emotion recognition with individual facial emotion CNNs and global image based CNNs",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction, ICMI 2017, Glasgow, United Kingdom, November 13 - 17, 2017",
        "url": "https://doi.org/10.1145/3136755.3143008",
        "doi": "10.1145/3136755.3143008"
    },
    "bargal_emotion_2016": {
        "title": "Emotion recognition in the wild from videos using images",
        "year": "2016",
        "type": "inproceedings",
        "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI 2016, Tokyo, Japan, November 12-16, 2016",
        "url": "https://doi.org/10.1145/2993148.2997627",
        "doi": "10.1145/2993148.2997627"
    },
    "huang_deep_2016": {
        "title": "Deep learning driven hypergraph representation for image-based emotion recognition",
        "year": "2016",
        "type": "inproceedings",
        "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI 2016, Tokyo, Japan, November 12-16, 2016",
        "url": "https://doi.org/10.1145/2993148.2993185",
        "doi": "10.1145/2993148.2993185"
    },
    "dhall_video_2015": {
        "title": "Video and Image based Emotion Recognition Challenges in the Wild: EmotiW 2015",
        "year": "2015",
        "type": "inproceedings",
        "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, Seattle, WA, USA, November 09 - 13, 2015",
        "url": "https://doi.org/10.1145/2818346.2829994",
        "doi": "10.1145/2818346.2829994"
    },
    "mohamed_it_2022": {
        "title": "It is Okay to Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning by Contrastive Data Collection",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52688.2022.02058",
        "doi": "10.1109/CVPR52688.2022.02058"
    },
    "he_image2audio_2020-1": {
        "title": "Image2Audio: Facilitating Semi-supervised Audio Emotion Recognition with Facial Expression Image",
        "year": "2020",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://openaccess.thecvf.com/content\\_CVPRW\\_2020/html/w54/He\\_Image2Audio\\_Facilitating\\_Semi-Supervised\\_Audio\\_Emotion\\_Recognition\\_With\\_Facial\\_Expression\\_Image\\_CVPRW\\_2020\\_paper.html",
        "doi": "10.1109/CVPRW50498.2020.00464"
    },
    "kumar_classification_2019-1": {
        "title": "CLASSIFICATION OF FACIAL MICRO-EXPRESSIONS USING MOTION MAGNIFIED EMOTION AVATAR IMAGES",
        "year": "2019",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "http://openaccess.thecvf.com/content\\_CVPRW\\_2019/html/Face\\_and\\_Gesture\\_Analysis\\_for\\_Health\\_Informatics/Kumar\\_CLASSIFICATION\\_OF\\_FACIAL\\_MICRO-EXPRESSIONS\\_USING\\_MOTION\\_MAGNIFIED\\_EMOTION\\_AVATAR\\_IMAGES\\_CVPRW\\_2019\\_paper.html",
        "doi": null
    },
    "fan_emotional_2018": {
        "title": "Emotional Attention: A Study of Image Sentiment and Visual Attention",
        "year": "2018",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "http://openaccess.thecvf.com/content\\_cvpr\\_2018/html/Fan\\_Emotional\\_Attention\\_A\\_CVPR\\_2018\\_paper.html",
        "doi": "10.1109/CVPR.2018.00785"
    },
    "chen_factual_2018": {
        "title": "\"Factual\" or \"Emotional\": Stylized Image Captioning with Adaptive Learning and Attention",
        "year": "2018",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-030-01249-6\\_32",
        "doi": "10.1007/978-3-030-01249-6_32"
    },
    "masuda_prediction_2023": {
        "title": "Prediction of Human Color Emotion on the Images Using Convolutional Neural Network",
        "year": "2023",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-36004-6\\_65",
        "doi": "10.1007/978-3-031-36004-6_65"
    },
    "wu_image_2022": {
        "title": "Image Preference of Intelligent Voice Assistant for the Elderly Based on PAD Emotion Model",
        "year": "2022",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-06050-2\\_30",
        "doi": "10.1007/978-3-031-06050-2_30"
    },
    "prossinger_using_2021": {
        "title": "Using Neural-Network-Driven Image Recognition Software to Detect Emotional Reactions in the Face of a Player While Playing a Horror Video Game",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-77277-2\\_20",
        "doi": "10.1007/978-3-030-77277-2_20"
    },
    "tang_research_2018": {
        "title": "Research on Image Emotional Tag Generation Mechanism Based on the \"Cloud Pet Keeping\" Phenomenon",
        "year": "2018",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-91806-8\\_7",
        "doi": "10.1007/978-3-319-91806-8_7"
    },
    "liang_research_2017": {
        "title": "Research on Image Emotional Semantic Retrieval Mechanism Based on Cognitive Quantification Model",
        "year": "2017",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-58640-3\\_10",
        "doi": "10.1007/978-3-319-58640-3_10"
    },
    "sakurai_basic_2015": {
        "title": "Basic Study of Evoking Emotion Through Extending One's Body Image by Integration of Internal Sense and External Sense",
        "year": "2015",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-20612-7\\_42",
        "doi": "10.1007/978-3-319-20612-7_42"
    },
    "shima_common_2009": {
        "title": "Common Understanding of Graphic Image Enhance \"Emotional Design\"",
        "year": "2009",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-642-02806-9\\_63",
        "doi": "10.1007/978-3-642-02806-9_63"
    },
    "barros_ciao_2022": {
        "title": "CIAO! A Contrastive Adaptation Mechanism for Non-Universal Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "10th International Conference on Affective Computing and Intelligent Interaction, ACII 2022, Nara, Japan, October 18-21, 2022",
        "url": "https://doi.org/10.1109/ACII55700.2022.9953863",
        "doi": "10.1109/ACII55700.2022.9953863"
    },
    "khan_computational_2022": {
        "title": "Computational Recognition of Facial Expressions in Sculpture",
        "year": "2022",
        "type": "inproceedings",
        "venue": "10th International Conference on Affective Computing and Intelligent Interaction, ACII 2022 - Workshops and Demos, Nara, Japan, October 17-21, 2022",
        "url": "https://doi.org/10.1109/ACIIW57231.2022.10086035",
        "doi": "10.1109/ACIIW57231.2022.10086035"
    },
    "novielli_sensor-based_2022": {
        "title": "Sensor-Based Emotion Recognition in Software Development: Facial Expressions as Gold Standard",
        "year": "2022",
        "type": "inproceedings",
        "venue": "10th International Conference on Affective Computing and Intelligent Interaction, ACII 2022, Nara, Japan, October 18-21, 2022",
        "url": "https://doi.org/10.1109/ACII55700.2022.9953808",
        "doi": "10.1109/ACII55700.2022.9953808"
    },
    "roy_analysis_2022": {
        "title": "Analysis of Semi-Supervised Methods for Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "10th International Conference on Affective Computing and Intelligent Interaction, ACII 2022, Nara, Japan, October 18-21, 2022",
        "url": "https://doi.org/10.1109/ACII55700.2022.9953876",
        "doi": "10.1109/ACII55700.2022.9953876"
    },
    "suresh_using_2022": {
        "title": "Using Positive Matching Contrastive Loss with Facial Action Units to mitigate bias in Facial Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "10th International Conference on Affective Computing and Intelligent Interaction, ACII 2022, Nara, Japan, October 18-21, 2022",
        "url": "https://doi.org/10.1109/ACII55700.2022.9953865",
        "doi": "10.1109/ACII55700.2022.9953865"
    },
    "roy_spatiotemporal_2021": {
        "title": "Spatiotemporal Contrastive Learning of Facial Expressions in Videos",
        "year": "2021",
        "type": "inproceedings",
        "venue": "9th International Conference on Affective Computing and Intelligent Interaction, ACII 2021, Nara, Japan, September 28 - Oct. 1, 2021",
        "url": "https://doi.org/10.1109/ACII52823.2021.9597460",
        "doi": "10.1109/ACII52823.2021.9597460"
    },
    "dai_real-time_2019": {
        "title": "Real-time pain detection in facial expressions for health robotics",
        "year": "2019",
        "type": "inproceedings",
        "venue": "8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos, ACII Workshops 2019, Cambridge, United Kingdom, September 3-6, 2019",
        "url": "https://doi.org/10.1109/ACIIW.2019.8925192",
        "doi": "10.1109/ACIIW.2019.8925192"
    },
    "guo_deep_2019": {
        "title": "Deep Neural Networks for Depression Recognition Based on Facial Expressions Caused by Stimulus Tasks",
        "year": "2019",
        "type": "inproceedings",
        "venue": "8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos, ACII Workshops 2019, Cambridge, United Kingdom, September 3-6, 2019",
        "url": "https://doi.org/10.1109/ACIIW.2019.8925293",
        "doi": "10.1109/ACIIW.2019.8925293"
    },
    "hasan_facial_2019": {
        "title": "Facial Expression Based Imagination Index and a Transfer Learning Approach to Detect Deception",
        "year": "2019",
        "type": "inproceedings",
        "venue": "8th International Conference on Affective Computing and Intelligent Interaction, ACII 2019, Cambridge, United Kingdom, September 3-6, 2019",
        "url": "https://doi.org/10.1109/ACII.2019.8925473",
        "doi": "10.1109/ACII.2019.8925473"
    },
    "hu_towards_2019": {
        "title": "Towards Facial De-Expression and Expression Recognition in the Wild",
        "year": "2019",
        "type": "inproceedings",
        "venue": "8th International Conference on Affective Computing and Intelligent Interaction, ACII 2019, Cambridge, United Kingdom, September 3-6, 2019",
        "url": "https://doi.org/10.1109/ACII.2019.8925461",
        "doi": "10.1109/ACII.2019.8925461"
    },
    "kalischek_deep_2019": {
        "title": "Deep Domain Adaptation for Facial Expression Analysis",
        "year": "2019",
        "type": "inproceedings",
        "venue": "8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos, ACII Workshops 2019, Cambridge, United Kingdom, September 3-6, 2019",
        "url": "https://doi.org/10.1109/ACIIW.2019.8925055",
        "doi": "10.1109/ACIIW.2019.8925055"
    },
    "seuss_emotion_2019": {
        "title": "Emotion Expression from Different Angles: A Video Database for Facial Expressions of Actors Shot by a Camera Array",
        "year": "2019",
        "type": "inproceedings",
        "venue": "8th International Conference on Affective Computing and Intelligent Interaction, ACII 2019, Cambridge, United Kingdom, September 3-6, 2019",
        "url": "https://doi.org/10.1109/ACII.2019.8925458",
        "doi": "10.1109/ACII.2019.8925458"
    },
    "spaulding_frustratingly_2019": {
        "title": "Frustratingly Easy Personalization for Real-time Affect Interpretation of Facial Expression",
        "year": "2019",
        "type": "inproceedings",
        "venue": "8th International Conference on Affective Computing and Intelligent Interaction, ACII 2019, Cambridge, United Kingdom, September 3-6, 2019",
        "url": "https://doi.org/10.1109/ACII.2019.8925515",
        "doi": "10.1109/ACII.2019.8925515"
    },
    "teng_facial_2019": {
        "title": "Facial Expression Recognition with Identity and Spatial-temporal Integrated Learning",
        "year": "2019",
        "type": "inproceedings",
        "venue": "8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos, ACII Workshops 2019, Cambridge, United Kingdom, September 3-6, 2019",
        "url": "https://doi.org/10.1109/ACIIW.2019.8925212",
        "doi": "10.1109/ACIIW.2019.8925212"
    },
    "sepas-moghaddam_deep_2019": {
        "title": "A Deep Framework for Facial Emotion Recognition using Light Field Images",
        "year": "2019",
        "type": "inproceedings",
        "venue": "8th International Conference on Affective Computing and Intelligent Interaction, ACII 2019, Cambridge, United Kingdom, September 3-6, 2019",
        "url": "https://doi.org/10.1109/ACII.2019.8925445",
        "doi": "10.1109/ACII.2019.8925445"
    },
    "masuda_prediction_2023-1": {
        "title": "Prediction of Human Color Emotion on the Images Using Convolutional Neural Network",
        "year": "2023",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-36004-6\\_65",
        "doi": "10.1007/978-3-031-36004-6_65"
    },
    "wu_image_2022-1": {
        "title": "Image Preference of Intelligent Voice Assistant for the Elderly Based on PAD Emotion Model",
        "year": "2022",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-06050-2\\_30",
        "doi": "10.1007/978-3-031-06050-2_30"
    },
    "prossinger_using_2021-1": {
        "title": "Using Neural-Network-Driven Image Recognition Software to Detect Emotional Reactions in the Face of a Player While Playing a Horror Video Game",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-77277-2\\_20",
        "doi": "10.1007/978-3-030-77277-2_20"
    },
    "tang_research_2018-1": {
        "title": "Research on Image Emotional Tag Generation Mechanism Based on the \"Cloud Pet Keeping\" Phenomenon",
        "year": "2018",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-91806-8\\_7",
        "doi": "10.1007/978-3-319-91806-8_7"
    },
    "liang_research_2017-1": {
        "title": "Research on Image Emotional Semantic Retrieval Mechanism Based on Cognitive Quantification Model",
        "year": "2017",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-58640-3\\_10",
        "doi": "10.1007/978-3-319-58640-3_10"
    },
    "sakurai_basic_2015-1": {
        "title": "Basic Study of Evoking Emotion Through Extending One's Body Image by Integration of Internal Sense and External Sense",
        "year": "2015",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-20612-7\\_42",
        "doi": "10.1007/978-3-319-20612-7_42"
    },
    "shima_common_2009-1": {
        "title": "Common Understanding of Graphic Image Enhance \"Emotional Design\"",
        "year": "2009",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-642-02806-9\\_63",
        "doi": "10.1007/978-3-642-02806-9_63"
    },
    "narula_preserving_2020-1": {
        "title": "Preserving Privacy in Image-based Emotion Recognition through User Anonymization",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ICMI '20: International Conference on Multimodal Interaction, Virtual Event, The Netherlands, October 25-29, 2020",
        "url": "https://doi.org/10.1145/3382507.3418833",
        "doi": "10.1145/3382507.3418833"
    },
    "pons_et-cyclegan_2020-1": {
        "title": "ET-CycleGAN: Generating Thermal Images from Images in the Visible Spectrum for Facial Emotion Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction, ICMI Companion 2020, Virtual Event, The Netherlands, October, 2020",
        "url": "https://doi.org/10.1145/3395035.3425258",
        "doi": "10.1145/3395035.3425258"
    },
    "wang_cascade_2018-1": {
        "title": "Cascade Attention Networks For Group Emotion Recognition with Face, Body and Image Cues",
        "year": "2018",
        "type": "inproceedings",
        "venue": "Proceedings of the 2018 on International Conference on Multimodal Interaction, ICMI 2018, Boulder, CO, USA, October 16-20, 2018",
        "url": "https://doi.org/10.1145/3242969.3264991",
        "doi": "10.1145/3242969.3264991"
    },
    "guo_group-level_2017-1": {
        "title": "Group-level emotion recognition using deep models on image scene, faces, and skeletons",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction, ICMI 2017, Glasgow, United Kingdom, November 13 - 17, 2017",
        "url": "https://doi.org/10.1145/3136755.3143017",
        "doi": "10.1145/3136755.3143017"
    },
    "tan_group_2017-1": {
        "title": "Group emotion recognition with individual facial emotion CNNs and global image based CNNs",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction, ICMI 2017, Glasgow, United Kingdom, November 13 - 17, 2017",
        "url": "https://doi.org/10.1145/3136755.3143008",
        "doi": "10.1145/3136755.3143008"
    },
    "bargal_emotion_2016-1": {
        "title": "Emotion recognition in the wild from videos using images",
        "year": "2016",
        "type": "inproceedings",
        "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI 2016, Tokyo, Japan, November 12-16, 2016",
        "url": "https://doi.org/10.1145/2993148.2997627",
        "doi": "10.1145/2993148.2997627"
    },
    "huang_deep_2016-1": {
        "title": "Deep learning driven hypergraph representation for image-based emotion recognition",
        "year": "2016",
        "type": "inproceedings",
        "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI 2016, Tokyo, Japan, November 12-16, 2016",
        "url": "https://doi.org/10.1145/2993148.2993185",
        "doi": "10.1145/2993148.2993185"
    },
    "dhall_video_2015-1": {
        "title": "Video and Image based Emotion Recognition Challenges in the Wild: EmotiW 2015",
        "year": "2015",
        "type": "inproceedings",
        "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, Seattle, WA, USA, November 09 - 13, 2015",
        "url": "https://doi.org/10.1145/2818346.2829994",
        "doi": "10.1145/2818346.2829994"
    },
    "veltmeijer_automatic_2022": {
        "title": "Automatic Recognition of Emotional Subgroups in Images",
        "year": "2022",
        "type": "inproceedings",
        "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022",
        "url": "https://doi.org/10.24963/ijcai.2022/190",
        "doi": "10.24963/IJCAI.2022/190"
    },
    "yang_joint_2017": {
        "title": "Joint Image Emotion Classification and Distribution Learning via Deep Convolutional Neural Network",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017",
        "url": "https://doi.org/10.24963/ijcai.2017/456",
        "doi": "10.24963/IJCAI.2017/456"
    },
    "zhao_approximating_2017": {
        "title": "Approximating Discrete Probability Distribution of Image Emotions by Multi-Modal Features Fusion",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017",
        "url": "https://doi.org/10.24963/ijcai.2017/651",
        "doi": "10.24963/IJCAI.2017/651"
    },
    "jing_styleedl_2023": {
        "title": "StyleEDL: Style-Guided High-order Attention Network for Image Emotion Distribution Learning",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3612040",
        "doi": "10.1145/3581783.3612040"
    },
    "pan_progressive_2023": {
        "title": "Progressive Visual Content Understanding Network for Image Emotion Classification",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3612186",
        "doi": "10.1145/3581783.3612186"
    },
    "ruotsalo_feeling_2023": {
        "title": "Feeling Positive? Predicting Emotional Image Similarity from Brain Signals",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3613442",
        "doi": "10.1145/3581783.3613442"
    },
    "li_similar_2021": {
        "title": "Similar Scenes Arouse Similar Emotions: Parallel Data Augmentation for Stylized Image Captioning",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3474085.3475662",
        "doi": "10.1145/3474085.3475662"
    },
    "zhao_emotion-based_2020": {
        "title": "Emotion-Based End-to-End Matching Between Image and Music in Valence-Arousal Space",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3413776",
        "doi": "10.1145/3394171.3413776"
    },
    "prajwal_towards_2019": {
        "title": "Towards Increased Accessibility of Meme Images with the Help of Rich Face Emotion Captions",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3343031.3350939",
        "doi": "10.1145/3343031.3350939"
    },
    "peng_give_2018": {
        "title": "Give Me One Portrait Image, I Will Tell You Your Emotion and Personality",
        "year": "2018",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3240508.3241384",
        "doi": "10.1145/3240508.3241384"
    },
    "zhao_emotiongan_2018": {
        "title": "EmotionGAN: Unsupervised Domain Adaptation for Learning Discrete Probability Distributions of Image Emotions",
        "year": "2018",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3240508.3240591",
        "doi": "10.1145/3240508.3240591"
    },
    "webb_emotion_2020": {
        "title": "Emotion Recognition from Face Images in an Unconstrained Environment for usage on Social Robots",
        "year": "2020",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN48605.2020.9207494",
        "doi": "10.1109/IJCNN48605.2020.9207494"
    },
    "chen_study_2022": {
        "title": "The Study of City Color Imagery, Affective Appraisal and Place Attachment",
        "year": "2022",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-06053-3\\_22",
        "doi": "10.1007/978-3-031-06053-3_22"
    },
    "jiang_proposal_2022": {
        "title": "Proposal for Visualization of Affective Image in Three Regions of China",
        "year": "2022",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-06424-1\\_16",
        "doi": "10.1007/978-3-031-06424-1_16"
    },
    "mota_imagetic_2022": {
        "title": "Imagetic and affective measures of memory reverberation diverge at sleep onset in association with theta rhythm",
        "year": "2022",
        "type": "article",
        "venue": "NeuroImage",
        "url": "https://doi.org/10.1016/j.neuroimage.2022.119690",
        "doi": "10.1016/J.NEUROIMAGE.2022.119690"
    },
    "zhang_decoding_2023": {
        "title": "Decoding the temporal representation of facial expression in face-selective regions",
        "year": "2023",
        "type": "article",
        "venue": "NeuroImage",
        "url": "https://doi.org/10.1016/j.neuroimage.2023.120442",
        "doi": "10.1016/J.NEUROIMAGE.2023.120442"
    },
    "zhao_differential_2023": {
        "title": "Differential responses in the mirror neuron system during imitation of individual emotional facial expressions and association with autistic traits",
        "year": "2023",
        "type": "article",
        "venue": "NeuroImage",
        "url": "https://doi.org/10.1016/j.neuroimage.2023.120263",
        "doi": "10.1016/J.NEUROIMAGE.2023.120263"
    },
    "ding_dissociation_2022": {
        "title": "Dissociation and hierarchy of human visual pathways for simultaneously coding facial identity and expression",
        "year": "2022",
        "type": "article",
        "venue": "NeuroImage",
        "url": "https://doi.org/10.1016/j.neuroimage.2022.119769",
        "doi": "10.1016/J.NEUROIMAGE.2022.119769"
    },
    "kim_neural_2022": {
        "title": "Neural signatures of individual variability in context-dependent perception of ambiguous facial expression",
        "year": "2022",
        "type": "article",
        "venue": "NeuroImage",
        "url": "https://doi.org/10.1016/j.neuroimage.2022.119355",
        "doi": "10.1016/J.NEUROIMAGE.2022.119355"
    },
    "muukkonen_representational_2022": {
        "title": "Representational structure of fMRI/EEG responses to dynamic facial expressions",
        "year": "2022",
        "type": "article",
        "venue": "NeuroImage",
        "url": "https://doi.org/10.1016/j.neuroimage.2022.119631",
        "doi": "10.1016/J.NEUROIMAGE.2022.119631"
    },
    "tanaka_initial_2022": {
        "title": "The initial decrease in 7T-BOLD signals detected by hyperalignment contains information to decode facial expressions",
        "year": "2022",
        "type": "article",
        "venue": "NeuroImage",
        "url": "https://doi.org/10.1016/j.neuroimage.2022.119537",
        "doi": "10.1016/J.NEUROIMAGE.2022.119537"
    },
    "xie_converging_2021": {
        "title": "Converging neural and behavioral evidence for a rapid, generalized response to threat-related facial expressions in 3-year-old children",
        "year": "2021",
        "type": "article",
        "venue": "NeuroImage",
        "url": "https://doi.org/10.1016/j.neuroimage.2021.117732",
        "doi": "10.1016/J.NEUROIMAGE.2021.117732"
    },
    "smith_decoding_2019": {
        "title": "Decoding the dynamic representation of facial expressions of emotion in explicit and incidental tasks",
        "year": "2019",
        "type": "article",
        "venue": "NeuroImage",
        "url": "https://doi.org/10.1016/j.neuroimage.2019.03.065",
        "doi": "10.1016/J.NEUROIMAGE.2019.03.065"
    },
    "zhang_multiscale_2023": {
        "title": "Multiscale Emotion Representation Learning for Affective Image Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2022.3144804",
        "doi": "10.1109/TMM.2022.3144804"
    },
    "yao_adaptive_2021": {
        "title": "Adaptive Deep Metric Learning for Affective Image Retrieval and Classification",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2020.3001527",
        "doi": "10.1109/TMM.2020.3001527"
    },
    "zhao_affective_2018": {
        "title": "Affective Image Content Analysis: A Comprehensive Survey",
        "year": "2018",
        "type": "inproceedings",
        "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden",
        "url": "https://doi.org/10.24963/ijcai.2018/780",
        "doi": "10.24963/IJCAI.2018/780"
    },
    "yao_attention-aware_2019-1": {
        "title": "Attention-Aware Polarity Sensitive Embedding for Affective Image Retrieval",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ICCV",
        "url": "https://doi.org/10.1109/ICCV.2019.00123",
        "doi": "10.1109/ICCV.2019.00123"
    },
    "beaudoin-gagnon_funii_2019": {
        "title": "The FUNii Database: A Physiological, Behavioral, Demographic and Subjective Video Game Database for Affective Gaming and Player Experience Research",
        "year": "2019",
        "type": "inproceedings",
        "venue": "8th International Conference on Affective Computing and Intelligent Interaction, ACII 2019, Cambridge, United Kingdom, September 3-6, 2019",
        "url": "https://doi.org/10.1109/ACII.2019.8925502",
        "doi": "10.1109/ACII.2019.8925502"
    },
    "ooms_feelthenews_2023": {
        "title": "FeelTheNews: Augmenting Affective Perceptions of News Videos with Thermal and Vibrotactile Stimulation",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems, CHI EA 2023, Hamburg, Germany, April 23-28, 2023",
        "url": "https://doi.org/10.1145/3544549.3585638",
        "doi": "10.1145/3544549.3585638"
    },
    "savchenko_video-based_2022": {
        "title": "Video-based Frame-level Facial Analysis of Affective Behavior on Mobile Devices using EfficientNets",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW56347.2022.00263",
        "doi": "10.1109/CVPRW56347.2022.00263"
    },
    "garcia_neurochat_2021": {
        "title": "Neurochat: Artistic Affective State Facial Filters in Online Video Communication",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78114-9\\_2",
        "doi": "10.1007/978-3-030-78114-9_2"
    },
    "delgado_affective_2019": {
        "title": "Affective Video Games: A Systematic Mapping Study",
        "year": "2019",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-22602-2\\_9",
        "doi": "10.1007/978-3-030-22602-2_9"
    },
    "bonarini_affective_2011": {
        "title": "Affective Videogames: The Problem of Wearability and Comfort",
        "year": "2011",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-642-21619-0\\_77",
        "doi": "10.1007/978-3-642-21619-0_77"
    },
    "gan_multimodal_2017": {
        "title": "A Multimodal Deep Regression Bayesian Network for Affective Video Content Analyses",
        "year": "2017",
        "type": "inproceedings",
        "venue": "ICCV",
        "url": "https://doi.org/10.1109/ICCV.2017.547",
        "doi": "10.1109/ICCV.2017.547"
    },
    "parameshwara_efficient_2023": {
        "title": "Efficient Labelling of Affective Video Datasets via Few-Shot \\& Multi-Task Contrastive Learning",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3613784",
        "doi": "10.1145/3581783.3613784"
    },
    "ru_sensing_2023": {
        "title": "Sensing Micro-Motion Human Patterns using Multimodal mmRadar and Video Signal for Affective and Psychological Intelligence",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3611754",
        "doi": "10.1145/3581783.3611754"
    },
    "tao_freq-hd_2023-1": {
        "title": "Freq-HD: An Interpretable Frequency-based High-Dynamics Affective Clip Selection Method for in-the-Wild Facial Expression Recognition in Videos",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3611972",
        "doi": "10.1145/3581783.3611972"
    },
    "pan_representation_2022": {
        "title": "Representation Learning through Multimodal Attention and Time-Sync Comments for Affective Video Content Analysis",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3548018",
        "doi": "10.1145/3503161.3548018"
    },
    "zhu_multimodal_2019": {
        "title": "Multimodal Deep Denoise Framework for Affective Video Content Analysis",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3343031.3350997",
        "doi": "10.1145/3343031.3350997"
    },
    "chen_exploring_2017": {
        "title": "Exploring Domain Knowledge for Affective Video Content Analyses",
        "year": "2017",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3123266.3123352",
        "doi": "10.1145/3123266.3123352"
    },
    "baveye_protocol_2014": {
        "title": "A Protocol for Cross-Validating Large Crowdsourced Data: The Case of the LIRIS-ACCEDE Affective Video Dataset",
        "year": "2014",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/2660114.2660115",
        "doi": "10.1145/2660114.2660115"
    },
    "espinosa_fusing_2014": {
        "title": "Fusing Affective Dimensions and Audio-Visual Features from Segmented Video for Depression Recognition: INAOE-BUAP's Participation at AVEC'14 Challenge",
        "year": "2014",
        "type": "inproceedings",
        "venue": "Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge, AVEC '14, Orlando, Florida, USA, November 7, 2014",
        "url": "https://doi.org/10.1145/2661806.2661815",
        "doi": "10.1145/2661806.2661815"
    },
    "gievska_impact_2014": {
        "title": "The Impact of Affective Verbal Content on Predicting Personality Impressions in YouTube Videos",
        "year": "2014",
        "type": "inproceedings",
        "venue": "Proceedings of the 2014 ACM Multi Media on Workshop on Computational Personality Recognition, WCPR '14, Orlando, Florida, USA, November 7, 2014",
        "url": "https://doi.org/10.1145/2659522.2659529",
        "doi": "10.1145/2659522.2659529"
    },
    "acar_learning_2013": {
        "title": "Learning representations for affective video understanding",
        "year": "2013",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/2502081.2502215",
        "doi": "10.1145/2502081.2502215"
    },
    "yazdani_affective_2011": {
        "title": "Affective content analysis of music video clips",
        "year": "2011",
        "type": "inproceedings",
        "venue": "Proceedings of the 1st international ACM workshop on Music information retrieval with user-centered and multimodal strategies, Scottsdale, AZ, USA, November 28 - December 01, 2011",
        "url": "https://doi.org/10.1145/2072529.2072532",
        "doi": "10.1145/2072529.2072532"
    },
    "zhao_video_2011": {
        "title": "Video indexing and recommendation based on affective analysis of viewers",
        "year": "2011",
        "type": "inproceedings",
        "venue": "Proceedings of the 19th International Conference on Multimedia 2011, Scottsdale, AZ, USA, November 28 - December 1, 2011",
        "url": "https://doi.org/10.1145/2072298.2072043",
        "doi": "10.1145/2072298.2072043"
    },
    "castellano_inter-act_2010": {
        "title": "Inter-ACT: an affective and contextually rich multimodal video corpus for studying interaction with robots",
        "year": "2010",
        "type": "inproceedings",
        "venue": "Proceedings of the 18th International Conference on Multimedia 2010, Firenze, Italy, October 25-29, 2010",
        "url": "https://doi.org/10.1145/1873951.1874142",
        "doi": "10.1145/1873951.1874142"
    },
    "wang_p2sl_2022": {
        "title": "P2SL: Private-Shared Subspaces Learning for Affective Video Content Analysis",
        "year": "2022",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia and Expo, ICME 2022, Taipei, Taiwan, July 18-22, 2022",
        "url": "https://doi.org/10.1109/ICME52920.2022.9859902",
        "doi": "10.1109/ICME52920.2022.9859902"
    },
    "li_affective_2019": {
        "title": "Affective Video Content Analyses by Using Cross-Modal Embedding Learning Features",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia and Expo, ICME 2019, Shanghai, China, July 8-12, 2019",
        "url": "https://doi.org/10.1109/ICME.2019.00150",
        "doi": "10.1109/ICME.2019.00150"
    },
    "yoshida_towards_2013": {
        "title": "Towards semantic and affective content-based video recommendation",
        "year": "2013",
        "type": "inproceedings",
        "venue": "2013 IEEE International Conference on Multimedia and Expo Workshops, San Jose, CA, USA, July 15-19, 2013",
        "url": "https://doi.org/10.1109/ICMEW.2013.6618331",
        "doi": "10.1109/ICMEW.2013.6618331"
    },
    "irie_affective_2009": {
        "title": "Affective video segment retrieval for consumer generated videos based on correlation between emotions and emotional audio events",
        "year": "2009",
        "type": "inproceedings",
        "venue": "Proceedings of the 2009 IEEE International Conference on Multimedia and Expo, ICME 2009, June 28 - July 2, 2009, New York City, NY, USA",
        "url": "https://doi.org/10.1109/ICME.2009.5202548",
        "doi": "10.1109/ICME.2009.5202548"
    },
    "sun_improved_2009": {
        "title": "An improved valence-arousal emotion space for video affective content representation and recognition",
        "year": "2009",
        "type": "inproceedings",
        "venue": "Proceedings of the 2009 IEEE International Conference on Multimedia and Expo, ICME 2009, June 28 - July 2, 2009, New York City, NY, USA",
        "url": "https://doi.org/10.1109/ICME.2009.5202559",
        "doi": "10.1109/ICME.2009.5202559"
    },
    "jaimes_affective_2005": {
        "title": "Affective Meeting Video Analysis",
        "year": "2005",
        "type": "inproceedings",
        "venue": "Proceedings of the 2005 IEEE International Conference on Multimedia and Expo, ICME 2005, July 6-9, 2005, Amsterdam, The Netherlands",
        "url": "https://doi.org/10.1109/ICME.2005.1521695",
        "doi": "10.1109/ICME.2005.1521695"
    },
    "xu_affective_2005": {
        "title": "Affective content analysis in comedy and horror videos by audio emotional event detection",
        "year": "2005",
        "type": "inproceedings",
        "venue": "Proceedings of the 2005 IEEE International Conference on Multimedia and Expo, ICME 2005, July 6-9, 2005, Amsterdam, The Netherlands",
        "url": "https://doi.org/10.1109/ICME.2005.1521500",
        "doi": "10.1109/ICME.2005.1521500"
    },
    "yi_affective_2020": {
        "title": "Affective Video Content Analysis With Adaptive Fusion Recurrent Network",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2019.2955300",
        "doi": "10.1109/TMM.2019.2955300"
    },
    "ou_multimodal_2021": {
        "title": "Multimodal Local-Global Attention Network for Affective Video Content Analysis",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2020.3014889",
        "doi": "10.1109/TCSVT.2020.3014889"
    },
    "zhang_learning_2018": {
        "title": "Learning Affective Features With a Hybrid Deep Model for Audio-Visual Emotion Recognition",
        "year": "2018",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2017.2719043",
        "doi": "10.1109/TCSVT.2017.2719043"
    },
    "canini_affective_2013": {
        "title": "Affective Recommendation of Movies Based on Selected Connotative Features",
        "year": "2013",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2012.2211935",
        "doi": "10.1109/TCSVT.2012.2211935"
    },
    "wang_affective_2006": {
        "title": "Affective understanding in film",
        "year": "2006",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2006.873781",
        "doi": "10.1109/TCSVT.2006.873781"
    },
    "kopru_use_2023": {
        "title": "Use of Affective Visual Information for Summarization of Human-Centric Videos",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3222882",
        "doi": "10.1109/TAFFC.2022.3222882"
    },
    "zhu_affective_2022": {
        "title": "Affective Video Content Analysis via Multimodal Deep Quality Embedding Network",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2020.3004114",
        "doi": "10.1109/TAFFC.2020.3004114"
    },
    "wang_video_2021": {
        "title": "Video Affective Content Analysis by Exploring Domain Knowledge",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2019.2912377",
        "doi": "10.1109/TAFFC.2019.2912377"
    },
    "baveye_affective_2018": {
        "title": "Affective Video Content Analysis: A Multidisciplinary Insight",
        "year": "2018",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2017.2661284",
        "doi": "10.1109/TAFFC.2017.2661284"
    },
    "baveye_liris-accede_2015": {
        "title": "LIRIS-ACCEDE: A Video Database for Affective Content Analysis",
        "year": "2015",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2015.2396531",
        "doi": "10.1109/TAFFC.2015.2396531"
    },
    "wang_video_2015": {
        "title": "Video Affective Content Analysis: A Survey of State-of-the-Art Methods",
        "year": "2015",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2015.2432791",
        "doi": "10.1109/TAFFC.2015.2432791"
    },
    "thao_emomv_2023": {
        "title": "EmoMV: Affective music-video correspondence learning datasets for classification and retrieval",
        "year": "2023",
        "type": "article",
        "venue": "Inf. Fusion",
        "url": "https://doi.org/10.1016/j.inffus.2022.10.002",
        "doi": "10.1016/J.INFFUS.2022.10.002"
    },
    "guo_affective_2019": {
        "title": "Affective video content analysis based on multimodal data fusion in heterogeneous networks",
        "year": "2019",
        "type": "article",
        "venue": "Inf. Fusion",
        "url": "https://doi.org/10.1016/j.inffus.2019.02.007",
        "doi": "10.1016/J.INFFUS.2019.02.007"
    },
    "chen_qoe-aware_2019": {
        "title": "QoE-Aware wireless video communications for emotion-aware intelligent systems: A multi-layered collaboration approach",
        "year": "2019",
        "type": "article",
        "venue": "Inf. Fusion",
        "url": "https://doi.org/10.1016/j.inffus.2018.06.007",
        "doi": "10.1016/J.INFFUS.2018.06.007"
    },
    "chan_decoding_2020": {
        "title": "Decoding dynamic affective responses to naturalistic videos with shared neural patterns",
        "year": "2020",
        "type": "article",
        "venue": "NeuroImage",
        "url": "https://doi.org/10.1016/j.neuroimage.2020.116618",
        "doi": "10.1016/J.NEUROIMAGE.2020.116618"
    },
    "dudzik_exploring_2020": {
        "title": "Exploring Personal Memories and Video Content as Context for Facial Behavior in Predictions of Video-Induced Emotions",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ICMI '20: International Conference on Multimodal Interaction, Virtual Event, The Netherlands, October 25-29, 2020",
        "url": "https://doi.org/10.1145/3382507.3418814",
        "doi": "10.1145/3382507.3418814"
    },
    "liu_group_2020": {
        "title": "Group Level Audio-Video Emotion Recognition Using Hybrid Networks",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ICMI '20: International Conference on Multimodal Interaction, Virtual Event, The Netherlands, October 25-29, 2020",
        "url": "https://doi.org/10.1145/3382507.3417968",
        "doi": "10.1145/3382507.3417968"
    },
    "wang_multi-attention_2019": {
        "title": "Multi-Attention Fusion Network for Video-based Emotion Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "International Conference on Multimodal Interaction, ICMI 2019, Suzhou, China, October 14-18, 2019",
        "url": "https://doi.org/10.1145/3340555.3355720",
        "doi": "10.1145/3340555.3355720"
    },
    "wu_continuous_2019-1": {
        "title": "Continuous Emotion Recognition in Videos by Fusing Facial Expression, Head Pose and Eye Gaze",
        "year": "2019",
        "type": "inproceedings",
        "venue": "International Conference on Multimodal Interaction, ICMI 2019, Suzhou, China, October 14-18, 2019",
        "url": "https://doi.org/10.1145/3340555.3353739",
        "doi": "10.1145/3340555.3353739"
    },
    "zhou_exploring_2019": {
        "title": "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "International Conference on Multimodal Interaction, ICMI 2019, Suzhou, China, October 14-18, 2019",
        "url": "https://doi.org/10.1145/3340555.3355713",
        "doi": "10.1145/3340555.3355713"
    },
    "fan_video-based_2018": {
        "title": "Video-based Emotion Recognition Using Deeply-Supervised Neural Networks",
        "year": "2018",
        "type": "inproceedings",
        "venue": "Proceedings of the 2018 on International Conference on Multimodal Interaction, ICMI 2018, Boulder, CO, USA, October 16-20, 2018",
        "url": "https://doi.org/10.1145/3242969.3264978",
        "doi": "10.1145/3242969.3264978"
    },
    "liu_multi-feature_2018": {
        "title": "Multi-Feature Based Emotion Recognition for Video Clips",
        "year": "2018",
        "type": "inproceedings",
        "venue": "Proceedings of the 2018 on International Conference on Multimodal Interaction, ICMI 2018, Boulder, CO, USA, October 16-20, 2018",
        "url": "https://doi.org/10.1145/3242969.3264989",
        "doi": "10.1145/3242969.3264989"
    },
    "lu_multiple_2018": {
        "title": "Multiple Spatio-temporal Feature Learning for Video-based Emotion Recognition in the Wild",
        "year": "2018",
        "type": "inproceedings",
        "venue": "Proceedings of the 2018 on International Conference on Multimodal Interaction, ICMI 2018, Boulder, CO, USA, October 16-20, 2018",
        "url": "https://doi.org/10.1145/3242969.3264992",
        "doi": "10.1145/3242969.3264992"
    },
    "taguchi_effects_2018": {
        "title": "Effects of face and voice deformation on participant emotion in video-mediated communication",
        "year": "2018",
        "type": "inproceedings",
        "venue": "Proceedings of the International Conference on Multimodal Interaction: Adjunct, ICMI 2018, Boulder, CO, USA, October 16-20, 2018",
        "url": "https://doi.org/10.1145/3281151.3281159",
        "doi": "10.1145/3281151.3281159"
    },
    "vielzeuf_temporal_2017": {
        "title": "Temporal multimodal fusion for video emotion classification in the wild",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction, ICMI 2017, Glasgow, United Kingdom, November 13 - 17, 2017",
        "url": "https://doi.org/10.1145/3136755.3143011",
        "doi": "10.1145/3136755.3143011"
    },
    "bargal_emotion_2016-2": {
        "title": "Emotion recognition in the wild from videos using images",
        "year": "2016",
        "type": "inproceedings",
        "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI 2016, Tokyo, Japan, November 12-16, 2016",
        "url": "https://doi.org/10.1145/2993148.2997627",
        "doi": "10.1145/2993148.2997627"
    },
    "chen_video_2016": {
        "title": "Video emotion recognition in the wild based on fusion of multimodal features",
        "year": "2016",
        "type": "inproceedings",
        "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI 2016, Tokyo, Japan, November 12-16, 2016",
        "url": "https://doi.org/10.1145/2993148.2997629",
        "doi": "10.1145/2993148.2997629"
    },
    "dhall_emotiw_2016": {
        "title": "EmotiW 2016: video and group-level emotion recognition challenges",
        "year": "2016",
        "type": "inproceedings",
        "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI 2016, Tokyo, Japan, November 12-16, 2016",
        "url": "https://doi.org/10.1145/2993148.2997638",
        "doi": "10.1145/2993148.2997638"
    },
    "ding_audio_2016": {
        "title": "Audio and face video emotion recognition in the wild using deep neural networks and small datasets",
        "year": "2016",
        "type": "inproceedings",
        "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI 2016, Tokyo, Japan, November 12-16, 2016",
        "url": "https://doi.org/10.1145/2993148.2997637",
        "doi": "10.1145/2993148.2997637"
    },
    "fan_video-based_2016": {
        "title": "Video-based emotion recognition using CNN-RNN and C3D hybrid networks",
        "year": "2016",
        "type": "inproceedings",
        "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI 2016, Tokyo, Japan, November 12-16, 2016",
        "url": "https://doi.org/10.1145/2993148.2997632",
        "doi": "10.1145/2993148.2997632"
    },
    "pham_attentivevideo_2016": {
        "title": "AttentiveVideo: quantifying emotional responses to mobile video advertisements",
        "year": "2016",
        "type": "inproceedings",
        "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI 2016, Tokyo, Japan, November 12-16, 2016",
        "url": "https://doi.org/10.1145/2993148.2998533",
        "doi": "10.1145/2993148.2998533"
    },
    "cruz_quantification_2015": {
        "title": "Quantification of Cinematography Semiotics for Video-based Facial Emotion Recognition in the EmotiW 2015 Grand Challenge",
        "year": "2015",
        "type": "inproceedings",
        "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, Seattle, WA, USA, November 09 - 13, 2015",
        "url": "https://doi.org/10.1145/2818346.2830592",
        "doi": "10.1145/2818346.2830592"
    },
    "dhall_video_2015-2": {
        "title": "Video and Image based Emotion Recognition Challenges in the Wild: EmotiW 2015",
        "year": "2015",
        "type": "inproceedings",
        "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, Seattle, WA, USA, November 09 - 13, 2015",
        "url": "https://doi.org/10.1145/2818346.2829994",
        "doi": "10.1145/2818346.2829994"
    },
    "kahou_recurrent_2015": {
        "title": "Recurrent Neural Networks for Emotion Recognition in Video",
        "year": "2015",
        "type": "inproceedings",
        "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, Seattle, WA, USA, November 09 - 13, 2015",
        "url": "https://doi.org/10.1145/2818346.2830596",
        "doi": "10.1145/2818346.2830596"
    },
    "li_deep_2015": {
        "title": "A Deep Feature based Multi-kernel Learning Approach for Video Emotion Recognition",
        "year": "2015",
        "type": "inproceedings",
        "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, Seattle, WA, USA, November 09 - 13, 2015",
        "url": "https://doi.org/10.1145/2818346.2830583",
        "doi": "10.1145/2818346.2830583"
    },
    "wache_implicit_2015": {
        "title": "Implicit User-centric Personality Recognition Based on Physiological Responses to Emotional Videos",
        "year": "2015",
        "type": "inproceedings",
        "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, Seattle, WA, USA, November 09 - 13, 2015",
        "url": "https://doi.org/10.1145/2818346.2820736",
        "doi": "10.1145/2818346.2820736"
    },
    "chen_initial_2014": {
        "title": "An Initial Analysis of Structured Video Interviews by Using Multimodal Emotion Detection",
        "year": "2014",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1145/2668056.2668057",
        "doi": "10.1145/2668056.2668057"
    },
    "kahou_combining_2013": {
        "title": "Combining modality specific deep neural networks for emotion recognition in video",
        "year": "2013",
        "type": "inproceedings",
        "venue": "2013 International Conference on Multimodal Interaction, ICMI '13, Sydney, NSW, Australia, December 9-13, 2013",
        "url": "https://doi.org/10.1145/2522848.2531745",
        "doi": "10.1145/2522848.2531745"
    },
    "biel_facetube_2012": {
        "title": "FaceTube: predicting personality from facial expressions of emotion in online conversational video",
        "year": "2012",
        "type": "inproceedings",
        "venue": "International Conference on Multimodal Interaction, ICMI '12, Santa Monica, CA, USA, October 22-26, 2012",
        "url": "https://doi.org/10.1145/2388676.2388689",
        "doi": "10.1145/2388676.2388689"
    },
    "song_emotion-prior_2023": {
        "title": "Emotion-Prior Awareness Network for Emotional Video Captioning",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3611726",
        "doi": "10.1145/3581783.3611726"
    },
    "li_dilated_2022": {
        "title": "Dilated Context Integrated Network with Cross-Modal Consensus for Temporal Emotion Localization in Videos",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3547886",
        "doi": "10.1145/3503161.3547886"
    },
    "qi_feeling_2022": {
        "title": "Feeling Without Sharing: A Federated Video Emotion Recognition Framework Via Privacy-Agnostic Hybrid Aggregation",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3548278",
        "doi": "10.1145/3503161.3548278"
    },
    "vaiani_viper_2022": {
        "title": "ViPER: Video-based Perceiver for Emotion Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "MuSe@MM 2022: Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge, Lisboa, Portugal, 10 October 2022",
        "url": "https://doi.org/10.1145/3551876.3554806",
        "doi": "10.1145/3551876.3554806"
    },
    "gao_pairwise_2021": {
        "title": "Pairwise Emotional Relationship Recognition in Drama Videos: Dataset and Benchmark",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3474085.3475493",
        "doi": "10.1145/3474085.3475493"
    },
    "magnusson_invertable_2021": {
        "title": "Invertable Frowns: Video-to-Video Facial Emotion Translation",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ADGD '21: Proceedings of the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfake Generation and Detection, Virtual Event, China, 24 October 2021",
        "url": "https://doi.org/10.1145/3476099.3484317",
        "doi": "10.1145/3476099.3484317"
    },
    "qi_zero-shot_2021": {
        "title": "Zero-shot Video Emotion Recognition via Multimodal Protagonist-aware Transformer Network",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3474085.3475647",
        "doi": "10.1145/3474085.3475647"
    },
    "shen_memor_2020": {
        "title": "MEmoR: A Dataset for Multimodal Emotion Reasoning in Videos",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3413909",
        "doi": "10.1145/3394171.3413909"
    },
    "brady_multi-modal_2016": {
        "title": "Multi-Modal Audio, Video and Physiological Sensor Learning for Continuous Emotion Prediction",
        "year": "2016",
        "type": "inproceedings",
        "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, AVEC@MM 2016, Amsterdam, The Netherlands, October 16, 2016",
        "url": "https://doi.org/10.1145/2988257.2988264",
        "doi": "10.1145/2988257.2988264"
    },
    "chen_emotion_2016": {
        "title": "Emotion in Context: Deep Semantic Feature Fusion for Video Emotion Recognition",
        "year": "2016",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/2964284.2967196",
        "doi": "10.1145/2964284.2967196"
    },
    "jeong_jockey_2016": {
        "title": "Jockey Time: Making Video Playback to Enhance Emotional Effect",
        "year": "2016",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/2964284.2967183",
        "doi": "10.1145/2964284.2967183"
    },
    "lin_automatic_2016": {
        "title": "Automatic Music Video Generation Based on Emotion-Oriented Pseudo Song Prediction and Matching",
        "year": "2016",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/2964284.2967245",
        "doi": "10.1145/2964284.2967245"
    },
    "weber_high-level_2016": {
        "title": "High-Level Geometry-based Features of Video Modality for Emotion Prediction",
        "year": "2016",
        "type": "inproceedings",
        "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, AVEC@MM 2016, Amsterdam, The Netherlands, October 16, 2016",
        "url": "https://doi.org/10.1145/2988257.2988262",
        "doi": "10.1145/2988257.2988262"
    },
    "lin_emv-matchmaker_2015": {
        "title": "EMV-matchmaker: Emotional Temporal Course Modeling and Matching for Automatic Music Video Generation",
        "year": "2015",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/2733373.2806359",
        "doi": "10.1145/2733373.2806359"
    },
    "chao_multi-scale_2014": {
        "title": "Multi-scale Temporal Modeling for Dimensional Emotion Recognition in Video",
        "year": "2014",
        "type": "inproceedings",
        "venue": "Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge, AVEC '14, Orlando, Florida, USA, November 7, 2014",
        "url": "https://doi.org/10.1145/2661806.2661811",
        "doi": "10.1145/2661806.2661811"
    },
    "ramalho_immersive_2013": {
        "title": "Immersive 360\u00b0 mobile video with an emotional perspective",
        "year": "2013",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/2512142.2512144",
        "doi": "10.1145/2512142.2512144"
    },
    "wang_acousticvisual_2012": {
        "title": "The acousticvisual emotion guassians model for automatic generation of music video",
        "year": "2012",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/2393347.2396494",
        "doi": "10.1145/2393347.2396494"
    },
    "arifin_computation_2007": {
        "title": "A computation method for video segmentation utilizing the pleasure-arousal-dominance emotional information",
        "year": "2007",
        "type": "inproceedings",
        "venue": "Proceedings of the 15th International Conference on Multimedia 2007, Augsburg, Germany, September 24-29, 2007",
        "url": "https://doi.org/10.1145/1291233.1291251",
        "doi": "10.1145/1291233.1291251"
    },
    "bissinger_emotion_2023-1": {
        "title": "Emotion Recognition via Facial Expressions to Improve Virtual Communication in Videoconferences",
        "year": "2023",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-35599-8\\_10",
        "doi": "10.1007/978-3-031-35599-8_10"
    },
    "espinoza_biofeedback-controlled_2023": {
        "title": "Biofeedback-Controlled Video Games for Emotional Regulation",
        "year": "2023",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-35979-8\\_15",
        "doi": "10.1007/978-3-031-35979-8_15"
    },
    "garcia_emotional_2023": {
        "title": "Emotional Analysis through EEG on In-Store Journey - Pilot Methodology for Evocation of Emotions Through Video Stimuli to Measure Performance Metrics Using EEG Emotiv EPOC+ on In-Store Experiences",
        "year": "2023",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-36049-7\\_12",
        "doi": "10.1007/978-3-031-36049-7_12"
    },
    "zhou_research_2023": {
        "title": "Research on User Emotion Experience of Short Video Based on Cyclic Interaction Model",
        "year": "2023",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-35989-7\\_91",
        "doi": "10.1007/978-3-031-35989-7_91"
    },
    "liu_video_2021": {
        "title": "A Video Experience Design for Emotional Bullying in Public High School in Guangzhou, China",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78224-5\\_20",
        "doi": "10.1007/978-3-030-78224-5_20"
    },
    "prossinger_using_2021-2": {
        "title": "Using Neural-Network-Driven Image Recognition Software to Detect Emotional Reactions in the Face of a Player While Playing a Horror Video Game",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-77277-2\\_20",
        "doi": "10.1007/978-3-030-77277-2_20"
    },
    "recas_emotions_2021": {
        "title": "Emotions Driven Videogame Interactive Music System",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78645-8\\_21",
        "doi": "10.1007/978-3-030-78645-8_21"
    },
    "sandoval-bringas_use_2021": {
        "title": "Use of a Video Game with Tangible Interfaces to Work Emotions in Children with Autism",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-90328-2\\_18",
        "doi": "10.1007/978-3-030-90328-2_18"
    },
    "pinto_emotional_2020": {
        "title": "Emotional Design and Gamification in Educational Processes: Predictor Model to Increase Video Game Efficiency",
        "year": "2020",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-49757-6\\_37",
        "doi": "10.1007/978-3-030-49757-6_37"
    },
    "zhou_deep_2020": {
        "title": "Deep Learning-Based Emotion Recognition from Real-Time Videos",
        "year": "2020",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-49062-1\\_22",
        "doi": "10.1007/978-3-030-49062-1_22"
    },
    "dong_personalized_2018": {
        "title": "Personalized Emotion-Aware Video Streaming for the Elderly",
        "year": "2018",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-91485-5\\_28",
        "doi": "10.1007/978-3-319-91485-5_28"
    },
    "fonnegra_deep_2018": {
        "title": "Deep Learning Based Video Spatio-Temporal Modeling for Emotion Recognition",
        "year": "2018",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-91238-7\\_32",
        "doi": "10.1007/978-3-319-91238-7_32"
    },
    "chen_relationship_2016": {
        "title": "Relationship Between Video Game Events and Player Emotion Based on EEG",
        "year": "2016",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-39513-5\\_35",
        "doi": "10.1007/978-3-319-39513-5_35"
    },
    "al-mutairi_comparison_2015": {
        "title": "Comparison of User Responses to English and Arabic Emotion Elicitation Video Clips",
        "year": "2015",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-20907-4\\_13",
        "doi": "10.1007/978-3-319-20907-4_13"
    },
    "man_analysing_2014": {
        "title": "Analysing Emotional Video Using Consumer EEG Hardware",
        "year": "2014",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-07230-2\\_69",
        "doi": "10.1007/978-3-319-07230-2_69"
    },
    "sakamoto_relationship_2011": {
        "title": "Relationship between Emotional State and Physiological and Psychological Measurements Using Various Types of Video Content during TV Viewing",
        "year": "2011",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-642-22098-2\\_65",
        "doi": "10.1007/978-3-642-22098-2_65"
    },
    "gnacek_avdos_2022": {
        "title": "AVDOS - Affective Video Database Online Study Video database for affective research emotionally validated through an online survey",
        "year": "2022",
        "type": "inproceedings",
        "venue": "10th International Conference on Affective Computing and Intelligent Interaction, ACII 2022, Nara, Japan, October 18-21, 2022",
        "url": "https://doi.org/10.1109/ACII55700.2022.9953891",
        "doi": "10.1109/ACII55700.2022.9953891"
    },
    "jianwattanapaisarn_emotion_2022": {
        "title": "Emotion Recognition from Non-Straight Walking Gaits Induced by Emotional Videos",
        "year": "2022",
        "type": "inproceedings",
        "venue": "10th International Conference on Affective Computing and Intelligent Interaction, ACII 2022 - Workshops and Demos, Nara, Japan, October 17-21, 2022",
        "url": "https://doi.org/10.1109/ACIIW57231.2022.10086022",
        "doi": "10.1109/ACIIW57231.2022.10086022"
    },
    "seuss_emotion_2019-1": {
        "title": "Emotion Expression from Different Angles: A Video Database for Facial Expressions of Actors Shot by a Camera Array",
        "year": "2019",
        "type": "inproceedings",
        "venue": "8th International Conference on Affective Computing and Intelligent Interaction, ACII 2019, Cambridge, United Kingdom, September 3-6, 2019",
        "url": "https://doi.org/10.1109/ACII.2019.8925458",
        "doi": "10.1109/ACII.2019.8925458"
    },
    "bhattacharya_exploring_2023": {
        "title": "Exploring the Contextual Factors Affecting Multimodal Emotion Recognition in Videos",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2021.3071503",
        "doi": "10.1109/TAFFC.2021.3071503"
    },
    "jang_eeg-based_2023": {
        "title": "EEG-Based Emotional Video Classification via Learning Connectivity Structure",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2021.3126263",
        "doi": "10.1109/TAFFC.2021.3126263"
    },
    "li_spontaneous_2023-1": {
        "title": "A Spontaneous Driver Emotion Facial Expression (DEFE) Dataset for Intelligent Vehicles: Emotions Triggered by Video-Audio Clips in Driving Scenarios",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2021.3063387",
        "doi": "10.1109/TAFFC.2021.3063387"
    },
    "lew_eeg-video_2022": {
        "title": "EEG-Video Emotion-Based Summarization: Learning With EEG Auxiliary Signals",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3208259",
        "doi": "10.1109/TAFFC.2022.3208259"
    },
    "ding_inter-brain_2021": {
        "title": "Inter-Brain EEG Feature Extraction and Analysis for Continuous Implicit Emotion Tagging During Video Watching",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2018.2849758",
        "doi": "10.1109/TAFFC.2018.2849758"
    },
    "du_spatio-temporal_2021": {
        "title": "Spatio-Temporal Encoder-Decoder Fully Convolutional Network for Video-Based Dimensional Emotion Recognition",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2019.2940224",
        "doi": "10.1109/TAFFC.2019.2940224"
    },
    "lara-alvarez_induction_2021": {
        "title": "Induction of Emotional States in Educational Video Games Through a Fuzzy Control System",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2018.2840988",
        "doi": "10.1109/TAFFC.2018.2840988"
    },
    "noroozi_audio-visual_2019": {
        "title": "Audio-Visual Emotion Recognition in Video Clips",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2017.2713783",
        "doi": "10.1109/TAFFC.2017.2713783"
    },
    "wang_content-based_2019": {
        "title": "Content-Based Video Emotion Tagging Augmented by Users' Multiple Physiological Responses",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2017.2702749",
        "doi": "10.1109/TAFFC.2017.2702749"
    },
    "xu_heterogeneous_2018": {
        "title": "Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization",
        "year": "2018",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2016.2622690",
        "doi": "10.1109/TAFFC.2016.2622690"
    },
    "kim_computational_2017": {
        "title": "Computational Modeling of Players' Emotional Response Patterns to the Story Events of Video Games",
        "year": "2017",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2016.2519888",
        "doi": "10.1109/TAFFC.2016.2519888"
    },
    "katsimerou_predicting_2015": {
        "title": "Predicting Mood from Punctual Emotion Annotations on Videos",
        "year": "2015",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2015.2397454",
        "doi": "10.1109/TAFFC.2015.2397454"
    },
    "soleymani_multimodal_2012": {
        "title": "Multimodal Emotion Recognition in Response to Videos",
        "year": "2012",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/T-AFFC.2011.37",
        "doi": "10.1109/T-AFFC.2011.37"
    },
    "james_watch_2022": {
        "title": "Watch Your Flavors: Augmenting People's Flavor Perceptions and Associated Emotions based on Videos Watched while Eating",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CHI '22: CHI Conference on Human Factors in Computing Systems, New Orleans, LA, USA, 29 April 2022 - 5 May 2022, Extended Abstracts",
        "url": "https://doi.org/10.1145/3491101.3519846",
        "doi": "10.1145/3491101.3519846"
    },
    "bowman_chinese-language_2021": {
        "title": "A Chinese-Language Validation of the Video Game Demand Scale (VGDS-C): Measuring the Cognitive, Emotional, Physical, and Social Demands of Video Games",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CHI '21: CHI Conference on Human Factors in Computing Systems, Virtual Event / Yokohama, Japan, May 8-13, 2021",
        "url": "https://doi.org/10.1145/3411764.3445348",
        "doi": "10.1145/3411764.3445348"
    },
    "xue_investigating_2021": {
        "title": "Investigating the Relationship between Momentary Emotion Self-reports and Head and Eye Movements in HMD-based 360\u00b0 VR Video Watching",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CHI '21: CHI Conference on Human Factors in Computing Systems, Virtual Event / Yokohama Japan, May 8-13, 2021, Extended Abstracts",
        "url": "https://doi.org/10.1145/3411763.3451627",
        "doi": "10.1145/3411763.3451627"
    },
    "xue_rcea-360vr_2021": {
        "title": "RCEA-360VR: Real-time, Continuous Emotion Annotation in 360\u00b0 VR Videos for Collecting Precise Viewport-dependent Ground Truth Labels",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CHI '21: CHI Conference on Human Factors in Computing Systems, Virtual Event / Yokohama, Japan, May 8-13, 2021",
        "url": "https://doi.org/10.1145/3411764.3445487",
        "doi": "10.1145/3411764.3445487"
    },
    "xue_designing_2020": {
        "title": "Designing Real-time, Continuous Emotion Annotation Techniques for 360\u00b0 VR Videos",
        "year": "2020",
        "type": "inproceedings",
        "venue": "Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems, CHI 2020, Honolulu, HI, USA, April 25-30, 2020",
        "url": "https://doi.org/10.1145/3334480.3382895",
        "doi": "10.1145/3334480.3382895"
    },
    "zhang_rcea_2020": {
        "title": "RCEA: Real-time, Continuous Emotion Annotation for Collecting Precise Mobile Video Ground Truth Labels",
        "year": "2020",
        "type": "inproceedings",
        "venue": "CHI '20: CHI Conference on Human Factors in Computing Systems, Honolulu, HI, USA, April 25-30, 2020",
        "url": "https://doi.org/10.1145/3313831.3376808",
        "doi": "10.1145/3313831.3376808"
    },
    "fedosov_movie_2019": {
        "title": "Movie+: Towards Exploring Social Effects of Emotional Fingerprints for Video Clips and Movies",
        "year": "2019",
        "type": "inproceedings",
        "venue": "Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems, CHI 2019, Glasgow, Scotland, UK, May 04-09, 2019",
        "url": "https://doi.org/10.1145/3290607.3313261",
        "doi": "10.1145/3290607.3313261"
    },
    "hu_detecting_2021": {
        "title": "Detecting Highlighted Video Clips Through Emotion-Enhanced Audio-Visual Cues",
        "year": "2021",
        "type": "inproceedings",
        "venue": "2021 IEEE International Conference on Multimedia and Expo, ICME 2021, Shenzhen, China, July 5-9, 2021",
        "url": "https://doi.org/10.1109/ICME51207.2021.9428252",
        "doi": "10.1109/ICME51207.2021.9428252"
    },
    "qiu_dual_2020": {
        "title": "Dual Focus Attention Network For Video Emotion Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia and Expo, ICME 2020, London, UK, July 6-10, 2020",
        "url": "https://doi.org/10.1109/ICME46284.2020.9102808",
        "doi": "10.1109/ICME46284.2020.9102808"
    },
    "xu_video_2019": {
        "title": "Video Emotion Recognition with Concept Selection",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia and Expo, ICME 2019, Shanghai, China, July 8-12, 2019",
        "url": "https://doi.org/10.1109/ICME.2019.00077",
        "doi": "10.1109/ICME.2019.00077"
    },
    "xue_bayesian_2017": {
        "title": "A Bayesian nonparametric multimodal data modeling framework for video emotion recognition",
        "year": "2017",
        "type": "inproceedings",
        "venue": "2017 IEEE International Conference on Multimedia and Expo, ICME 2017, Hong Kong, China, July 10-14, 2017",
        "url": "https://doi.org/10.1109/ICME.2017.8019480",
        "doi": "10.1109/ICME.2017.8019480"
    },
    "zhu_emotion_2014": {
        "title": "Emotion recognition from users' EEG signals with the help of stimulus VIDEOS",
        "year": "2014",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia and Expo, ICME 2014, Chengdu, China, July 14-18, 2014",
        "url": "https://doi.org/10.1109/ICME.2014.6890161",
        "doi": "10.1109/ICME.2014.6890161"
    },
    "irie_affective_2009-1": {
        "title": "Affective video segment retrieval for consumer generated videos based on correlation between emotions and emotional audio events",
        "year": "2009",
        "type": "inproceedings",
        "venue": "Proceedings of the 2009 IEEE International Conference on Multimedia and Expo, ICME 2009, June 28 - July 2, 2009, New York City, NY, USA",
        "url": "https://doi.org/10.1109/ICME.2009.5202548",
        "doi": "10.1109/ICME.2009.5202548"
    },
    "sun_improved_2009-1": {
        "title": "An improved valence-arousal emotion space for video affective content representation and recognition",
        "year": "2009",
        "type": "inproceedings",
        "venue": "Proceedings of the 2009 IEEE International Conference on Multimedia and Expo, ICME 2009, June 28 - July 2, 2009, New York City, NY, USA",
        "url": "https://doi.org/10.1109/ICME.2009.5202559",
        "doi": "10.1109/ICME.2009.5202559"
    },
    "valstar_biologically_2006": {
        "title": "Biologically vs. Logic Inspired Encoding of Facial Actions and Emotions in Video",
        "year": "2006",
        "type": "inproceedings",
        "venue": "Proceedings of the 2006 IEEE International Conference on Multimedia and Expo, ICME 2006, July 9-12 2006, Toronto, Ontario, Canada",
        "url": "https://doi.org/10.1109/ICME.2006.262464",
        "doi": "10.1109/ICME.2006.262464"
    },
    "wang_identify_2006": {
        "title": "Identify Sports Video Shots with \"Happy\" or \"Sad\" Emotions",
        "year": "2006",
        "type": "inproceedings",
        "venue": "Proceedings of the 2006 IEEE International Conference on Multimedia and Expo, ICME 2006, July 9-12 2006, Toronto, Ontario, Canada",
        "url": "https://doi.org/10.1109/ICME.2006.262641",
        "doi": "10.1109/ICME.2006.262641"
    },
    "xu_affective_2005-1": {
        "title": "Affective content analysis in comedy and horror videos by audio emotional event detection",
        "year": "2005",
        "type": "inproceedings",
        "venue": "Proceedings of the 2005 IEEE International Conference on Multimedia and Expo, ICME 2005, July 6-9, 2005, Amsterdam, The Netherlands",
        "url": "https://doi.org/10.1109/ICME.2005.1521500",
        "doi": "10.1109/ICME.2005.1521500"
    },
    "jegorova_ss-vaerr_2023": {
        "title": "SS-VAERR: Self-Supervised Apparent Emotional Reaction Recognition from Video",
        "year": "2023",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG57933.2023.10042638",
        "doi": "10.1109/FG57933.2023.10042638"
    },
    "javadi_many_2021": {
        "title": "The Many Faces of Anger: A Multicultural Video Dataset of Negative Emotions in the Wild (MFA-Wild)",
        "year": "2021",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG52635.2021.9666994",
        "doi": "10.1109/FG52635.2021.9666994"
    },
    "pikoulis_leveraging_2021": {
        "title": "Leveraging Semantic Scene Characteristics and Multi-Stream Convolutional Architectures in a Contextual Approach for Video-Based Visual Emotion Recognition in the Wild",
        "year": "2021",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG52635.2021.9666957",
        "doi": "10.1109/FG52635.2021.9666957"
    },
    "strizhkova_emotion_2021": {
        "title": "Emotion Editing in Head Reenactment Videos using Latent Space Manipulation",
        "year": "2021",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG52635.2021.9667059",
        "doi": "10.1109/FG52635.2021.9667059"
    },
    "viegas_two_2020": {
        "title": "Two Stage Emotion Recognition using Frame-level and Video-level Features",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00143",
        "doi": "10.1109/FG47880.2020.00143"
    },
    "alashkar_analyzing_2015": {
        "title": "Analyzing trajectories on Grassmann manifold for early emotion detection from depth videos",
        "year": "2015",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2015.7163122",
        "doi": "10.1109/FG.2015.7163122"
    },
    "gratch_felt_2013": {
        "title": "Felt emotion and social context determine the intensity of smiles in a competitive video game",
        "year": "2013",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2013.6553792",
        "doi": "10.1109/FG.2013.6553792"
    },
    "liu_implicit_2013": {
        "title": "Implicit video multi-emotion tagging by exploiting multi-expression relations",
        "year": "2013",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2013.6553767",
        "doi": "10.1109/FG.2013.6553767"
    },
    "wang_emotional_2013": {
        "title": "Emotional tagging of videos by exploring multiple emotions' coexistence",
        "year": "2013",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2013.6553771",
        "doi": "10.1109/FG.2013.6553771"
    },
    "soleymani_continuous_2011": {
        "title": "Continuous emotion detection in response to music videos",
        "year": "2011",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2011.5771352",
        "doi": "10.1109/FG.2011.5771352"
    },
    "hou_semantic_2023": {
        "title": "Semantic Alignment Network for Multi-Modal Emotion Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2023.3247822",
        "doi": "10.1109/TCSVT.2023.3247822"
    },
    "ren_maln_2023": {
        "title": "MALN: Multimodal Adversarial Learning Network for Conversational Emotion Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2023.3273577",
        "doi": "10.1109/TCSVT.2023.3273577"
    },
    "dong_temporal_2022": {
        "title": "Temporal Relation Inference Network for Multimodal Speech Emotion Recognition",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2022.3163445",
        "doi": "10.1109/TCSVT.2022.3163445"
    },
    "duan_def-net_2022": {
        "title": "DEF-Net: A Face Aging Model by Using Different Emotional Learnings",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2021.3096061",
        "doi": "10.1109/TCSVT.2021.3096061"
    },
    "zhang_real-time_2022": {
        "title": "Real-Time Video Emotion Recognition Based on Reinforcement Learning and Domain Knowledge",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2021.3072412",
        "doi": "10.1109/TCSVT.2021.3072412"
    },
    "zhang_graph-based_2022": {
        "title": "Graph-Based Object Semantic Refinement for Visual Emotion Recognition",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2021.3098712",
        "doi": "10.1109/TCSVT.2021.3098712"
    },
    "zhang_learning_2018-1": {
        "title": "Learning Affective Features With a Hybrid Deep Model for Audio-Visual Emotion Recognition",
        "year": "2018",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2017.2719043",
        "doi": "10.1109/TCSVT.2017.2719043"
    },
    "hossain_audio-visual_2015": {
        "title": "Audio-Visual Emotion-Aware Cloud Gaming Framework",
        "year": "2015",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2015.2444731",
        "doi": "10.1109/TCSVT.2015.2444731"
    },
    "yun_deformable_2013": {
        "title": "A Deformable 3-D Facial Expression Model for Dynamic Human Emotional State Recognition",
        "year": "2013",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2012.2203210",
        "doi": "10.1109/TCSVT.2012.2203210"
    },
    "song_contextual_2023": {
        "title": "Contextual Attention Network for Emotional Video Captioning",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2022.3183402",
        "doi": "10.1109/TMM.2022.3183402"
    },
    "xue_ceap-360vr_2023": {
        "title": "CEAP-360VR: A Continuous Physiological and Behavioral Emotion Annotation Dataset for 360\\textbackslashtextdollar\\textbackslashtextbackslashcirc\\textbackslashtextdollar VR Videos",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2021.3124080",
        "doi": "10.1109/TMM.2021.3124080"
    },
    "zhang_recognition_2023": {
        "title": "Recognition of Emotions in User-Generated Videos through Frame-Level Adaptation and Emotion Intensity Learning",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2021.3134167",
        "doi": "10.1109/TMM.2021.3134167"
    },
    "wang_emotion_2022": {
        "title": "Emotion Expression With Fact Transfer for Video Description",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2021.3058555",
        "doi": "10.1109/TMM.2021.3058555"
    },
    "nie_c-gcn_2021": {
        "title": "C-GCN: Correlation Based Graph Convolutional Network for Audio-Video Emotion Recognition",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2020.3032037",
        "doi": "10.1109/TMM.2020.3032037"
    },
    "qi_emotion_2021": {
        "title": "Emotion Knowledge Driven Video Highlight Detection",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2020.3035285",
        "doi": "10.1109/TMM.2020.3035285"
    },
    "li_visual-texual_2020": {
        "title": "Visual-Texual Emotion Analysis With Deep Coupled Video and Danmu Neural Networks",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2019.2946477",
        "doi": "10.1109/TMM.2019.2946477"
    },
    "wang_knowledge-augmented_2020": {
        "title": "Knowledge-Augmented Multimodal Deep Regression Bayesian Networks for Emotion Video Tagging",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2019.2934824",
        "doi": "10.1109/TMM.2019.2934824"
    },
    "zhang_recognition_2018": {
        "title": "Recognition of Emotions in User-Generated Videos With Kernelized Features",
        "year": "2018",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2018.2808760",
        "doi": "10.1109/TMM.2018.2808760"
    },
    "li_multimodal_2023": {
        "title": "Multimodal Feature Extraction and Fusion for Emotional Reaction Intensity Estimation and Expression Classification in Videos with Transformers",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW59228.2023.00620",
        "doi": "10.1109/CVPRW59228.2023.00620"
    },
    "zhang_weakly_2023": {
        "title": "Weakly Supervised Video Emotion Detection and Prediction via Cross-Modal Temporal Erasing Network",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52729.2023.01811",
        "doi": "10.1109/CVPR52729.2023.01811"
    },
    "ouzar_video-based_2022-1": {
        "title": "Video-based multimodal spontaneous emotion recognition using facial expressions and physiological signals",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW56347.2022.00275",
        "doi": "10.1109/CVPRW56347.2022.00275"
    },
    "papantoniou_neural_2022-1": {
        "title": "Neural Emotion Director: Speech-preserving semantic control of facial expressions in \"in-the-wild\" videos",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52688.2022.01822",
        "doi": "10.1109/CVPR52688.2022.01822"
    },
    "liu_imigue_2021": {
        "title": "iMiGUE: An Identity-Free Video Dataset for Micro-Gesture Understanding and Emotion Analysis",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content/CVPR2021/html/Liu\\_iMiGUE\\_An\\_Identity-Free\\_Video\\_Dataset\\_for\\_Micro-Gesture\\_Understanding\\_and\\_Emotion\\_CVPR\\_2021\\_paper.html",
        "doi": "10.1109/CVPR46437.2021.01049"
    },
    "ji_audio-driven_2021": {
        "title": "Audio-Driven Emotional Video Portraits",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content/CVPR2021/html/Ji\\_Audio-Driven\\_Emotional\\_Video\\_Portraits\\_CVPR\\_2021\\_paper.html",
        "doi": "10.1109/CVPR46437.2021.01386"
    },
    "mou_automatic_2016": {
        "title": "Automatic Recognition of Emotions and Membership in Group Videos",
        "year": "2016",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW.2016.185",
        "doi": "10.1109/CVPRW.2016.185"
    },
    "chang_multimodal_2023": {
        "title": "Multimodal Video Emotional Analysis of Time Features Alignment and Information Auxiliary Learning",
        "year": "2023",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN54540.2023.10191930",
        "doi": "10.1109/IJCNN54540.2023.10191930"
    },
    "he_dual_2021": {
        "title": "Dual Multi-Task Network with Bridge-Temporal-Attention for Student Emotion Recognition via Classroom Video",
        "year": "2021",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN52387.2021.9533471",
        "doi": "10.1109/IJCNN52387.2021.9533471"
    },
    "thuseethan_emotion_2019": {
        "title": "Emotion Intensity Estimation from Video Frames using Deep Hybrid Convolutional Neural Networks",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN.2019.8852365",
        "doi": "10.1109/IJCNN.2019.8852365"
    },
    "zhang_deep_2019": {
        "title": "Deep Fusion: An Attention Guided Factorized Bilinear Pooling for Audio-video Emotion Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN.2019.8851942",
        "doi": "10.1109/IJCNN.2019.8851942"
    },
    "yang_physiological-based_2018": {
        "title": "Physiological-Based Emotion Detection and Recognition in a Video Game Context",
        "year": "2018",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN.2018.8489125",
        "doi": "10.1109/IJCNN.2018.8489125"
    },
    "garg_video_2022-1": {
        "title": "From Video to Images: Contrastive Pretraining for Emotion Recognition from Single Image (Student Abstract)",
        "year": "2022",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v36i11.21612",
        "doi": "10.1609/AAAI.V36I11.21612"
    },
    "deng_mimamo_2020": {
        "title": "MIMAMO Net: Integrating Micro- and Macro-Motion for Video Emotion Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v34i03.5646",
        "doi": "10.1609/AAAI.V34I03.5646"
    },
    "zhao_end--end_2020": {
        "title": "An End-to-End Visual-Audio Attention Network for Emotion Recognition in User-Generated Videos",
        "year": "2020",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v34i01.5364",
        "doi": "10.1609/AAAI.V34I01.5364"
    },
    "jiang_predicting_2014": {
        "title": "Predicting Emotions in User-Generated Videos",
        "year": "2014",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v28i1.8724",
        "doi": "10.1609/AAAI.V28I1.8724"
    },
    "quach_non-volume_2022": {
        "title": "Non-volume preserving-based fusion to group-level emotion recognition on crowd videos",
        "year": "2022",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2022.108646",
        "doi": "10.1016/J.PATCOG.2022.108646"
    },
    "lee_collaborative_2016": {
        "title": "Collaborative expression representation using peak expression and intra class variation face images for practical subject-independent emotion recognition in videos",
        "year": "2016",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2015.12.016",
        "doi": "10.1016/J.PATCOG.2015.12.016"
    },
    "filntisis_emotion_2020": {
        "title": "Emotion Understanding in Videos Through Body, Context, and Visual-Semantic Embedding Loss",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-030-66415-2\\_52",
        "doi": "10.1007/978-3-030-66415-2_52"
    },
    "zou_emid_2023": {
        "title": "EMID: An Emotional Aligned Dataset in Audio-Visual Modality",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Proceedings of the 1st International Workshop on Multimedia Content Generation and Evaluation: New Methods and Practice, McGE 2023, Ottawa, ON, Canada, 29 October 2023",
        "url": "https://doi.org/10.1145/3607541.3616821",
        "doi": "10.1145/3607541.3616821"
    },
    "pan_progressive_2023-1": {
        "title": "Progressive Visual Content Understanding Network for Image Emotion Classification",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3612186",
        "doi": "10.1145/3581783.3612186"
    },
    "zhang_multimodal_2023": {
        "title": "Multimodal Emotion Interaction and Visualization Platform",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3612666",
        "doi": "10.1145/3581783.3612666"
    },
    "chen_towards_2022": {
        "title": "Towards Unbiased Visual Emotion Recognition via Causal Intervention",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3547936",
        "doi": "10.1145/3503161.3547936"
    },
    "chien_cross_2020": {
        "title": "Cross Corpus Physiological-based Emotion Recognition Using a Learnable Visual Semantic Graph Convolutional Network",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3413552",
        "doi": "10.1145/3394171.3413552"
    },
    "mittal_emotions_2020": {
        "title": "Emotions Don't Lie: An Audio-Visual Deepfake Detection Method using Affective Cues",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3413570",
        "doi": "10.1145/3394171.3413570"
    },
    "ringeval_avec19_2019": {
        "title": "AVEC'19: Audio/Visual Emotion Challenge and Workshop",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3343031.3350550",
        "doi": "10.1145/3343031.3350550"
    },
    "zhao_pdanet_2019": {
        "title": "PDANet: Polarity-consistent Deep Attention Network for Fine-grained Visual Emotion Regression",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3343031.3351062",
        "doi": "10.1145/3343031.3351062"
    },
    "ringeval_proceedings_2019": {
        "title": "Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop, AVEC@MM 2019, Nice, France, October 21-25, 2019",
        "year": "2019",
        "type": "book",
        "venue": "ACM",
        "url": "https://doi.org/10.1145/3347320",
        "doi": "10.1145/3347320"
    },
    "lee_audio-visual_2018": {
        "title": "Audio-Visual Attention Networks for Emotion Recognition",
        "year": "2018",
        "type": "inproceedings",
        "venue": "Proceedings of the 2018 Workshop on Audio-Visual Scene Understanding for Immersive Multimedia, AVSU@MM 2018, Seoul, Republic of Korea, October 26, 2018",
        "url": "https://doi.org/10.1145/3264869.3264873",
        "doi": "10.1145/3264869.3264873"
    },
    "ringeval_proceedings_2018": {
        "title": "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop, AVEC@MM 2018, Seoul, Republic of Korea, October 22, 2018",
        "year": "2018",
        "type": "book",
        "venue": "ACM",
        "url": "https://doi.org/10.1145/3266302",
        "doi": "10.1145/3266302"
    },
    "zhao_learning_2017": {
        "title": "Learning Visual Emotion Distributions via Multi-Modal Features Fusion",
        "year": "2017",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3123266.3130858",
        "doi": "10.1145/3123266.3130858"
    },
    "ringeval_proceedings_2017": {
        "title": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge, Mountain View, CA, USA, October 23 - 27, 2017",
        "year": "2017",
        "type": "book",
        "venue": "ACM",
        "url": "https://doi.org/10.1145/3133944",
        "doi": "10.1145/3133944"
    },
    "zhou_leveraging_2023": {
        "title": "Leveraging TCN and Transformer for effective visual-audio fusion in continuous emotion recognition",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW59228.2023.00610",
        "doi": "10.1109/CVPRW59228.2023.00610"
    },
    "zhang_continuous_2022": {
        "title": "Continuous Emotion Recognition using Visual-audio-linguistic Information: A Technical Report for ABAW3",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW56347.2022.00265",
        "doi": "10.1109/CVPRW56347.2022.00265"
    },
    "praveen_joint_2022": {
        "title": "A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW56347.2022.00278",
        "doi": "10.1109/CVPRW56347.2022.00278"
    },
    "xu_mdan_2022": {
        "title": "MDAN: Multi-level Dependent Attention Network for Visual Emotion Analysis",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52688.2022.00926",
        "doi": "10.1109/CVPR52688.2022.00926"
    },
    "yang_circular-structured_2021": {
        "title": "A Circular-Structured Representation for Visual Emotion Distribution Learning",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content/CVPR2021/html/Yang\\_A\\_Circular-Structured\\_Representation\\_for\\_Visual\\_Emotion\\_Distribution\\_Learning\\_CVPR\\_2021\\_paper.html",
        "doi": "10.1109/CVPR46437.2021.00422"
    },
    "wei_learning_2020": {
        "title": "Learning Visual Emotion Representations From Web Data",
        "year": "2020",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content\\_CVPR\\_2020/html/Wei\\_Learning\\_Visual\\_Emotion\\_Representations\\_From\\_Web\\_Data\\_CVPR\\_2020\\_paper.html",
        "doi": "10.1109/CVPR42600.2020.01312"
    },
    "fan_emotional_2018-1": {
        "title": "Emotional Attention: A Study of Image Sentiment and Visual Attention",
        "year": "2018",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "http://openaccess.thecvf.com/content\\_cvpr\\_2018/html/Fan\\_Emotional\\_Attention\\_A\\_CVPR\\_2018\\_paper.html",
        "doi": "10.1109/CVPR.2018.00785"
    },
    "jia_smbox2-ver_2022": {
        "title": "S\\textbackslash(\\textasciicircum\\textbackslashmbox2\\textbackslash)-VER: Semi-supervised Visual Emotion Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-031-19836-6\\_28",
        "doi": "10.1007/978-3-031-19836-6_28"
    },
    "filntisis_emotion_2020-1": {
        "title": "Emotion Understanding in Videos Through Body, Context, and Visual-Semantic Embedding Loss",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-030-66415-2\\_52",
        "doi": "10.1007/978-3-030-66415-2_52"
    },
    "wang_mead_2020": {
        "title": "MEAD: A Large-Scale Audio-Visual Dataset for Emotional Talking-Face Generation",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-030-58589-1\\_42",
        "doi": "10.1007/978-3-030-58589-1_42"
    },
    "panda_contemplating_2018": {
        "title": "Contemplating Visual Emotions: Understanding and Overcoming Dataset Bias",
        "year": "2018",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-030-01216-8\\_36",
        "doi": "10.1007/978-3-030-01216-8_36"
    },
    "pikoulis_leveraging_2021-1": {
        "title": "Leveraging Semantic Scene Characteristics and Multi-Stream Convolutional Architectures in a Contextual Approach for Video-Based Visual Emotion Recognition in the Wild",
        "year": "2021",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG52635.2021.9666957",
        "doi": "10.1109/FG52635.2021.9666957"
    },
    "praveen_cross_2021": {
        "title": "Cross Attentional Audio-Visual Fusion for Dimensional Emotion Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG52635.2021.9667055",
        "doi": "10.1109/FG52635.2021.9667055"
    },
    "shahriar_audio-visual_2019": {
        "title": "Audio-Visual Emotion Forecasting: Characterizing and Predicting Future Emotion Using Deep Learning",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756599",
        "doi": "10.1109/FG.2019.8756599"
    },
    "guo_multi-modality_2017": {
        "title": "Multi-modality Network with Visual and Geometrical Information for Micro Emotion Recognition",
        "year": "2017",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2017.103",
        "doi": "10.1109/FG.2017.103"
    },
    "zhang_graph-based_2022-1": {
        "title": "Graph-Based Object Semantic Refinement for Visual Emotion Recognition",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2021.3098712",
        "doi": "10.1109/TCSVT.2021.3098712"
    },
    "zhang_learning_2018-2": {
        "title": "Learning Affective Features With a Hybrid Deep Model for Audio-Visual Emotion Recognition",
        "year": "2018",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2017.2719043",
        "doi": "10.1109/TCSVT.2017.2719043"
    },
    "yang_seeking_2022": {
        "title": "Seeking Subjectivity in Visual Emotion Distribution Learning",
        "year": "2022",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2022.3193749",
        "doi": "10.1109/TIP.2022.3193749"
    },
    "yang_solver_2021": {
        "title": "SOLVER: Scene-Object Interrelated Visual Emotion Reasoning Network",
        "year": "2021",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2021.3118983",
        "doi": "10.1109/TIP.2021.3118983"
    },
    "yang_stimuli-aware_2021": {
        "title": "Stimuli-Aware Visual Emotion Analysis",
        "year": "2021",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2021.3106813",
        "doi": "10.1109/TIP.2021.3106813"
    },
    "li_visual-texual_2020-1": {
        "title": "Visual-Texual Emotion Analysis With Deep Coupled Video and Danmu Neural Networks",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2019.2946477",
        "doi": "10.1109/TMM.2019.2946477"
    },
    "wang_music--facial_2023-1": {
        "title": "Music-to-Facial Expressions: Emotion-Based Music Visualization for the Hearing Impaired",
        "year": "2023",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v37i13.26912",
        "doi": "10.1609/AAAI.V37I13.26912"
    },
    "zhao_end--end_2020-1": {
        "title": "An End-to-End Visual-Audio Attention Network for Emotion Recognition in User-Generated Videos",
        "year": "2020",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v34i01.5364",
        "doi": "10.1609/AAAI.V34I01.5364"
    },
    "sato_visualization_2021": {
        "title": "Visualization of social emotional appraisal process of an agent",
        "year": "2021",
        "type": "inproceedings",
        "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction, ACII 2021 - Workshops and Demos, Nara, Japan, September 28 - Oct. 1, 2021",
        "url": "https://doi.org/10.1109/ACIIW52867.2021.9666329",
        "doi": "10.1109/ACIIW52867.2021.9666329"
    },
    "ghaleb_multimodal_2019": {
        "title": "Multimodal and Temporal Perception of Audio-visual Cues for Emotion Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "8th International Conference on Affective Computing and Intelligent Interaction, ACII 2019, Cambridge, United Kingdom, September 3-6, 2019",
        "url": "https://doi.org/10.1109/ACII.2019.8925444",
        "doi": "10.1109/ACII.2019.8925444"
    },
    "otoole_understanding_2019": {
        "title": "Understanding Chromaesthesia by Strengthening Auditory -Visual-Emotional Associations",
        "year": "2019",
        "type": "inproceedings",
        "venue": "8th International Conference on Affective Computing and Intelligent Interaction, ACII 2019, Cambridge, United Kingdom, September 3-6, 2019",
        "url": "https://doi.org/10.1109/ACII.2019.8925465",
        "doi": "10.1109/ACII.2019.8925465"
    },
    "wang_robust_2019": {
        "title": "Robust Emotion Navigation: Few-shot Visual Sentiment Analysis by Auxiliary Noisy Data",
        "year": "2019",
        "type": "inproceedings",
        "venue": "8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos, ACII Workshops 2019, Cambridge, United Kingdom, September 3-6, 2019",
        "url": "https://doi.org/10.1109/ACIIW.2019.8925021",
        "doi": "10.1109/ACIIW.2019.8925021"
    },
    "lu_investigation_2017": {
        "title": "An investigation into three visual characteristics of complex scenes that evoke human emotion",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Seventh International Conference on Affective Computing and Intelligent Interaction, ACII 2017, San Antonio, TX, USA, October 23-26, 2017",
        "url": "https://doi.org/10.1109/ACII.2017.8273637",
        "doi": "10.1109/ACII.2017.8273637"
    },
    "pataca_visualization_2023": {
        "title": "Visualization of Speech Prosody and Emotion in Captions: Accessibility for Deaf and Hard-of-Hearing Users",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023, Hamburg, Germany, April 23-28, 2023",
        "url": "https://doi.org/10.1145/3544548.3581511",
        "doi": "10.1145/3544548.3581511"
    },
    "chen_bubble_2021": {
        "title": "Bubble Coloring to Visualize the Speech Emotion",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CHI '21: CHI Conference on Human Factors in Computing Systems, Virtual Event / Yokohama Japan, May 8-13, 2021, Extended Abstracts",
        "url": "https://doi.org/10.1145/3411763.3451698",
        "doi": "10.1145/3411763.3451698"
    },
    "kepplinger_see_2020": {
        "title": "See, Feel, Move: Player Behaviour Analysis through Combined Visualization of Gaze, Emotions, and Movement",
        "year": "2020",
        "type": "inproceedings",
        "venue": "CHI '20: CHI Conference on Human Factors in Computing Systems, Honolulu, HI, USA, April 25-30, 2020",
        "url": "https://doi.org/10.1145/3313831.3376401",
        "doi": "10.1145/3313831.3376401"
    },
    "kong_addressing_2020": {
        "title": "Addressing Cognitive and Emotional Barriers in Parent-Clinician Communication through Behavioral Visualization Webtools",
        "year": "2020",
        "type": "inproceedings",
        "venue": "CHI '20: CHI Conference on Human Factors in Computing Systems, Honolulu, HI, USA, April 25-30, 2020",
        "url": "https://doi.org/10.1145/3313831.3376181",
        "doi": "10.1145/3313831.3376181"
    },
    "panwar_detecting_2018": {
        "title": "Detecting Negative Emotion for Mixed Initiative Visual Analytics",
        "year": "2018",
        "type": "inproceedings",
        "venue": "Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems, CHI 2018, Montreal, QC, Canada, April 21-26, 2018",
        "url": "https://doi.org/10.1145/3170427.3188664",
        "doi": "10.1145/3170427.3188664"
    },
    "bernal_emotional_2017": {
        "title": "Emotional Beasts: Visually Expressing Emotions through Avatars in VR",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, Denver, CO, USA, May 06-11, 2017, Extended Abstracts",
        "url": "https://doi.org/10.1145/3027063.3053207",
        "doi": "10.1145/3027063.3053207"
    },
    "zhang_continuous_2021": {
        "title": "Continuous Emotion Recognition with Audio-visual Leader-follower Attentive Fusion",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICCVW",
        "url": "https://doi.org/10.1109/ICCVW54120.2021.00397",
        "doi": "10.1109/ICCVW54120.2021.00397"
    },
    "li_audio-visual_2023": {
        "title": "Audio-Visual Group-based Emotion Recognition using Local and Global Feature Aggregation based Multi-Task Learning",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Proceedings of the 25th International Conference on Multimodal Interaction, ICMI 2023, Paris, France, October 9-13, 2023",
        "url": "https://doi.org/10.1145/3577190.3616544",
        "doi": "10.1145/3577190.3616544"
    },
    "nivedhan_influence_2020": {
        "title": "The Influence of Emotion-Oriented Extrinsic Visual and Auditory Cues on Coffee Perception: A Virtual Reality Experiment",
        "year": "2020",
        "type": "inproceedings",
        "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction, ICMI Companion 2020, Virtual Event, The Netherlands, October, 2020",
        "url": "https://doi.org/10.1145/3395035.3425646",
        "doi": "10.1145/3395035.3425646"
    },
    "ziat_effects_2020": {
        "title": "Effects of Visual Locomotion and Tactile Stimuli Duration on the Emotional Dimensions of the Cutaneous Rabbit Illusion",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ICMI '20: International Conference on Multimodal Interaction, Virtual Event, The Netherlands, October 25-29, 2020",
        "url": "https://doi.org/10.1145/3382507.3418835",
        "doi": "10.1145/3382507.3418835"
    },
    "guo_group-level_2018": {
        "title": "Group-Level Emotion Recognition Using Hybrid Deep Models Based on Faces, Scenes, Skeletons and Visual Attentions",
        "year": "2018",
        "type": "inproceedings",
        "venue": "Proceedings of the 2018 on International Conference on Multimodal Interaction, ICMI 2018, Boulder, CO, USA, October 16-20, 2018",
        "url": "https://doi.org/10.1145/3242969.3264990",
        "doi": "10.1145/3242969.3264990"
    },
    "ouyang_audio-visual_2017": {
        "title": "Audio-visual emotion recognition using deep transfer learning and multiple temporal models",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction, ICMI 2017, Glasgow, United Kingdom, November 13 - 17, 2017",
        "url": "https://doi.org/10.1145/3136755.3143012",
        "doi": "10.1145/3136755.3143012"
    },
    "hu_detecting_2021-1": {
        "title": "Detecting Highlighted Video Clips Through Emotion-Enhanced Audio-Visual Cues",
        "year": "2021",
        "type": "inproceedings",
        "venue": "2021 IEEE International Conference on Multimedia and Expo, ICME 2021, Shenzhen, China, July 5-9, 2021",
        "url": "https://doi.org/10.1109/ICME51207.2021.9428252",
        "doi": "10.1109/ICME51207.2021.9428252"
    },
    "ma_efficient_2021": {
        "title": "An Efficient Approach for Audio-Visual Emotion Recognition With Missing Labels And Missing Modalities",
        "year": "2021",
        "type": "inproceedings",
        "venue": "2021 IEEE International Conference on Multimedia and Expo, ICME 2021, Shenzhen, China, July 5-9, 2021",
        "url": "https://doi.org/10.1109/ICME51207.2021.9428219",
        "doi": "10.1109/ICME51207.2021.9428219"
    },
    "ruan_context-aware_2020": {
        "title": "Context-Aware Generation-Based Net For Multi-Label Visual Emotion Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "IEEE International Conference on Multimedia and Expo, ICME 2020, London, UK, July 6-10, 2020",
        "url": "https://doi.org/10.1109/ICME46284.2020.9102855",
        "doi": "10.1109/ICME46284.2020.9102855"
    },
    "cheng_context-aware_2021": {
        "title": "Context-Aware Based Visual-Audio Feature Fusion for Emotion Recognition",
        "year": "2021",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN52387.2021.9533473",
        "doi": "10.1109/IJCNN52387.2021.9533473"
    },
    "mansouri-benssassi_speech_2019": {
        "title": "Speech Emotion Recognition With Early Visual Cross-modal Enhancement Using Spiking Neural Networks",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN.2019.8852473",
        "doi": "10.1109/IJCNN.2019.8852473"
    },
    "maher_e-ffective_2022": {
        "title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
        "year": "2022",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2021.3114789",
        "doi": "10.1109/TVCG.2021.3114789"
    },
    "zeng_emotioncues_2021": {
        "title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
        "year": "2021",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2019.2963659",
        "doi": "10.1109/TVCG.2019.2963659"
    },
    "zeng_emoco_2020": {
        "title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos",
        "year": "2020",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2019.2934656",
        "doi": "10.1109/TVCG.2019.2934656"
    },
    "nakamura_analyzing_2023": {
        "title": "Analyzing the Effect of Diverse Gaze and Head Direction on Facial Expression Recognition With Photo-Reflective Sensors Embedded in a Head-Mounted Display",
        "year": "2023",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2022.3179766",
        "doi": "10.1109/TVCG.2022.3179766"
    },
    "zhang_facial_2022-1": {
        "title": "Facial Expression Retargeting From Human to Avatar Made Easy",
        "year": "2022",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2020.3013876",
        "doi": "10.1109/TVCG.2020.3013876"
    },
    "zeng_emotioncues_2021-1": {
        "title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
        "year": "2021",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2019.2963659",
        "doi": "10.1109/TVCG.2019.2963659"
    },
    "zeng_emoco_2020-1": {
        "title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos",
        "year": "2020",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2019.2934656",
        "doi": "10.1109/TVCG.2019.2934656"
    },
    "zhang_visual--eeg_2022": {
        "title": "Visual-to-EEG cross-modal knowledge distillation for continuous emotion recognition",
        "year": "2022",
        "type": "article",
        "venue": "Pattern Recognit.",
        "url": "https://doi.org/10.1016/j.patcog.2022.108833",
        "doi": "10.1016/J.PATCOG.2022.108833"
    },
    "hossain_emotion_2019": {
        "title": "Emotion recognition using deep learning approach from audio-visual emotional big data",
        "year": "2019",
        "type": "article",
        "venue": "Inf. Fusion",
        "url": "https://doi.org/10.1016/j.inffus.2018.09.008",
        "doi": "10.1016/J.INFFUS.2018.09.008"
    },
    "ma_audio-visual_2019": {
        "title": "Audio-visual emotion fusion (AVEF): A deep efficient weighted approach",
        "year": "2019",
        "type": "article",
        "venue": "Inf. Fusion",
        "url": "https://doi.org/10.1016/j.inffus.2018.06.003",
        "doi": "10.1016/J.INFFUS.2018.06.003"
    },
    "zhang_visual_2022": {
        "title": "Visual Emotion Representation Learning via Emotion-Aware Pre-training",
        "year": "2022",
        "type": "inproceedings",
        "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022",
        "url": "https://doi.org/10.24963/ijcai.2022/234",
        "doi": "10.24963/IJCAI.2022/234"
    },
    "long_visualizing_2019": {
        "title": "Visualizing Emotional States: A Method Based on Human Brain Activity",
        "year": "2019",
        "type": "inproceedings",
        "venue": "Human Brain and Artificial Intelligence - First International Workshop, HBAI 2019, Held in Conjunction with IJCAI 2019, Macao, China, August 12, 2019, Revised Selected Papers",
        "url": "https://doi.org/10.1007/978-981-15-1398-5\\_18",
        "doi": "10.1007/978-981-15-1398-5_18"
    },
    "zhu_dependency_2017": {
        "title": "Dependency Exploitation: A Unified CNN-RNN Approach for Visual Emotion Recognition",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017",
        "url": "https://doi.org/10.24963/ijcai.2017/503",
        "doi": "10.24963/IJCAI.2017/503"
    },
    "boschetti_never_2023-1": {
        "title": "Never Correct: The Novel Analysis of Differing Visual (Facial Expression) and Acoustic (Vocalization) Bimodal Displays of the Affective States \"Pain\", \"Pleasure\", and \"Neutral\"",
        "year": "2023",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-35979-8\\_11",
        "doi": "10.1007/978-3-031-35979-8_11"
    },
    "jiang_proposal_2022-1": {
        "title": "Proposal for Visualization of Affective Image in Three Regions of China",
        "year": "2022",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-06424-1\\_16",
        "doi": "10.1007/978-3-031-06424-1_16"
    },
    "shang_impact_2021": {
        "title": "The Impact of Facial Attractiveness and Affective Person Knowledge on Visual Awareness",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78462-1\\_33",
        "doi": "10.1007/978-3-030-78462-1_33"
    },
    "ito_affective_2020": {
        "title": "Affective Analysis of Visual and Vibro-Tactile Feedback During Floor Cleaning Task Using Heart Rate Variability",
        "year": "2020",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-60700-5\\_49",
        "doi": "10.1007/978-3-030-60700-5_49"
    },
    "moreira_providing_2020": {
        "title": "Providing a Tangible and Visual Feedback of Affective States Self-expressions",
        "year": "2020",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-49108-6\\_12",
        "doi": "10.1007/978-3-030-49108-6_12"
    },
    "bartram_affective_2017": {
        "title": "Affective Color in Visualization",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, Denver, CO, USA, May 06-11, 2017",
        "url": "https://doi.org/10.1145/3025453.3026041",
        "doi": "10.1145/3025453.3026041"
    },
    "wilson_multi-moji_2017": {
        "title": "Multi-moji: Combining Thermal, Vibrotactile \\& Visual Stimuli to Expand the Affective Range of Feedback",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, Denver, CO, USA, May 06-11, 2017",
        "url": "https://doi.org/10.1145/3025453.3025614",
        "doi": "10.1145/3025453.3025614"
    },
    "narimatsu_cross-linguistic_2022": {
        "title": "Cross-Linguistic Study on Affective Impression and Language for Visual Art Using Neural Speaker",
        "year": "2022",
        "type": "inproceedings",
        "venue": "10th International Conference on Affective Computing and Intelligent Interaction, ACII 2022, Nara, Japan, October 18-21, 2022",
        "url": "https://doi.org/10.1109/ACII55700.2022.9953880",
        "doi": "10.1109/ACII55700.2022.9953880"
    },
    "ohkura_affective_2017": {
        "title": "Affective evaluation for material perception of bead-coated resin surfaces using visual and tactile sensations under virtual and real environments",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos, ACII Workshops 2017, San Antonio, TX, USA, October 23-26, 2017",
        "url": "https://doi.org/10.1109/ACIIW.2017.8272601",
        "doi": "10.1109/ACIIW.2017.8272601"
    },
    "yang_visual_2018": {
        "title": "Visual Sentiment Prediction Based on Automatic Discovery of Affective Regions",
        "year": "2018",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2018.2803520",
        "doi": "10.1109/TMM.2018.2803520"
    },
    "kopru_use_2023-1": {
        "title": "Use of Affective Visual Information for Summarization of Human-Centric Videos",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3222882",
        "doi": "10.1109/TAFFC.2022.3222882"
    },
    "tellamekala_modelling_2023": {
        "title": "Modelling Stochastic Context of Audio-Visual Expressive Behaviour With Affective Processes",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3157141",
        "doi": "10.1109/TAFFC.2022.3157141"
    },
    "zhalehpour_baum-1_2017": {
        "title": "BAUM-1: A Spontaneous Audio-Visual Face Database of Affective and Mental States",
        "year": "2017",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2016.2553038",
        "doi": "10.1109/TAFFC.2016.2553038"
    },
    "achlioptas_affection_2023": {
        "title": "Affection: Learning Affective Explanations for Real-World Visual Data",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52729.2023.00642",
        "doi": "10.1109/CVPR52729.2023.00642"
    },
    "tong_inferring_2023": {
        "title": "Inferring Affective Experience from the Big Picture Metaphor: A Two-dimensional Visual Breadth Model",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW59228.2023.00625",
        "doi": "10.1109/CVPRW59228.2023.00625"
    },
    "achlioptas_artemis_2021": {
        "title": "ArtEmis: Affective Language for Visual Art",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content/CVPR2021/html/Achlioptas\\_ArtEmis\\_Affective\\_Language\\_for\\_Visual\\_Art\\_CVPR\\_2021\\_paper.html",
        "doi": "10.1109/CVPR46437.2021.01140"
    },
    "lee-robbins_affective_2023": {
        "title": "Affective Learning Objectives for Communicative Visualizations",
        "year": "2023",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2022.3209500",
        "doi": "10.1109/TVCG.2022.3209500"
    },
    "anderson_affective_2022": {
        "title": "Affective Congruence in Visualization Design: Influences on Reading Categorical Maps",
        "year": "2022",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2021.3050118",
        "doi": "10.1109/TVCG.2021.3050118"
    },
    "zhang_visualtouch_2019": {
        "title": "VisualTouch: Enhancing Affective Touch Communication with Multi-modality Stimulation",
        "year": "2019",
        "type": "inproceedings",
        "venue": "International Conference on Multimodal Interaction, ICMI 2019, Suzhou, China, October 14-18, 2019",
        "url": "https://doi.org/10.1145/3340555.3353733",
        "doi": "10.1145/3340555.3353733"
    },
    "zhang_learning_2018-3": {
        "title": "Learning Affective Features With a Hybrid Deep Model for Audio-Visual Emotion Recognition",
        "year": "2018",
        "type": "article",
        "venue": "IEEE Trans. Circuits Syst. Video Technol.",
        "url": "https://doi.org/10.1109/TCSVT.2017.2719043",
        "doi": "10.1109/TCSVT.2017.2719043"
    },
    "mittal_emotions_2020-1": {
        "title": "Emotions Don't Lie: An Audio-Visual Deepfake Detection Method using Affective Cues",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3413570",
        "doi": "10.1145/3394171.3413570"
    },
    "gao_high-fidelity_2023": {
        "title": "High-Fidelity and Freely Controllable Talking Head Video Generation",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52729.2023.00543",
        "doi": "10.1109/CVPR52729.2023.00543"
    },
    "li_one-shot_2023": {
        "title": "One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52729.2023.01723",
        "doi": "10.1109/CVPR52729.2023.01723"
    },
    "vahdati_defending_2023": {
        "title": "Defending Low-Bandwidth Talking Head Videoconferencing Systems From Real-Time Puppeteering Attacks",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://doi.org/10.1109/CVPRW59228.2023.00105",
        "doi": "10.1109/CVPRW59228.2023.00105"
    },
    "wang_progressive_2023": {
        "title": "Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52729.2023.01724",
        "doi": "10.1109/CVPR52729.2023.01724"
    },
    "zhang_metaportrait_2023": {
        "title": "MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52729.2023.02116",
        "doi": "10.1109/CVPR52729.2023.02116"
    },
    "hong_depth-aware_2022": {
        "title": "Depth-Aware Generative Adversarial Network for Talking Head Video Generation",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52688.2022.00339",
        "doi": "10.1109/CVPR52688.2022.00339"
    },
    "liang_expressive_2022": {
        "title": "Expressive Talking Head Generation with Granular Audio-Visual Control",
        "year": "2022",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR52688.2022.00338",
        "doi": "10.1109/CVPR52688.2022.00338"
    },
    "wang_one-shot_2021": {
        "title": "One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content/CVPR2021/html/Wang\\_One-Shot\\_Free-View\\_Neural\\_Talking-Head\\_Synthesis\\_for\\_Video\\_Conferencing\\_CVPR\\_2021\\_paper.html",
        "doi": "10.1109/CVPR46437.2021.00991"
    },
    "zhang_davd-net_2020": {
        "title": "DAVD-Net: Deep Audio-Aided Video Decompression of Talking Heads",
        "year": "2020",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content\\_CVPR\\_2020/html/Zhang\\_DAVD-Net\\_Deep\\_Audio-Aided\\_Video\\_Decompression\\_of\\_Talking\\_Heads\\_CVPR\\_2020\\_paper.html",
        "doi": "10.1109/CVPR42600.2020.01235"
    },
    "guo_ad-nerf_2021": {
        "title": "AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICCV",
        "url": "https://doi.org/10.1109/ICCV48922.2021.00573",
        "doi": "10.1109/ICCV48922.2021.00573"
    },
    "meshry_learned_2021": {
        "title": "Learned Spatial Representations for Few-shot Talking-Head Synthesis",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICCV",
        "url": "https://doi.org/10.1109/ICCV48922.2021.01357",
        "doi": "10.1109/ICCV48922.2021.01357"
    },
    "zakharov_few-shot_2019": {
        "title": "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ICCV",
        "url": "https://doi.org/10.1109/ICCV.2019.00955",
        "doi": "10.1109/ICCV.2019.00955"
    },
    "jeon_faketalkerdetect_2019": {
        "title": "FakeTalkerDetect: Effective and Practical Realistic Neural Talking Head Detection with a Highly Unbalanced Dataset",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ICCVW",
        "url": "https://doi.org/10.1109/ICCVW.2019.00163",
        "doi": "10.1109/ICCVW.2019.00163"
    },
    "yang_context-aware_2023": {
        "title": "Context-Aware Talking-Head Video Editing",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3611765",
        "doi": "10.1145/3581783.3611765"
    },
    "alghamdi_talking_2022": {
        "title": "Talking Head from Speech Audio using a Pre-trained Image Generator",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3548101",
        "doi": "10.1145/3503161.3548101"
    },
    "huang_audio-driven_2022": {
        "title": "Audio-driven Talking Head Generation with Transformer and 3D Morphable Model",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3551574",
        "doi": "10.1145/3503161.3551574"
    },
    "lv_generating_2022": {
        "title": "Generating Smooth and Facial-Details-Enhanced Talking Head Video: A Perspective of Pre and Post Processes",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3551583",
        "doi": "10.1145/3503161.3551583"
    },
    "wijnants_talking_2019": {
        "title": "Talking Video Heads: Saving Streaming Bitrate by Adaptively Applying Object-based Video Principles to Interview-like Footage",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3343031.3351045",
        "doi": "10.1145/3343031.3351045"
    },
    "ma_styletalk_2023": {
        "title": "StyleTalk: One-Shot Talking Head Generation with Controllable Speaking Styles",
        "year": "2023",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v37i2.25280",
        "doi": "10.1609/AAAI.V37I2.25280"
    },
    "li_write--speaker_2021": {
        "title": "Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation",
        "year": "2021",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v35i3.16286",
        "doi": "10.1609/AAAI.V35I3.16286"
    },
    "wang_anyonenet_2023": {
        "title": "AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary Persons",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2022.3214100",
        "doi": "10.1109/TMM.2022.3214100"
    },
    "wang_audio2head_2021": {
        "title": "Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion",
        "year": "2021",
        "type": "inproceedings",
        "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021",
        "url": "https://doi.org/10.24963/ijcai.2021/152",
        "doi": "10.24963/IJCAI.2021/152"
    },
    "shen_learning_2022": {
        "title": "Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-031-19775-8\\_39",
        "doi": "10.1007/978-3-031-19775-8_39"
    },
    "chen_talking-head_2020": {
        "title": "Talking-Head Generation with Rhythmic Head Motion",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-030-58545-7\\_3",
        "doi": "10.1007/978-3-030-58545-7_3"
    },
    "fujiwara_features_2023": {
        "title": "Features Focusing on the Direction of Body Movement in Emotion Estimation Based on Laban Movement Analysis",
        "year": "2023",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-35599-8\\_12",
        "doi": "10.1007/978-3-031-35599-8_12"
    },
    "wang_research_2022": {
        "title": "Research on Emotional Design of Human Body Temperature Screening Instrument Based on AHP Method",
        "year": "2022",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-031-19679-9\\_60",
        "doi": "10.1007/978-3-031-19679-9_60"
    },
    "voigt-antons_automatic_2021": {
        "title": "Automatic Recognition of Experienced Emotional State from Body Movement",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78462-1\\_49",
        "doi": "10.1007/978-3-030-78462-1_49"
    },
    "cheng_effects_2020": {
        "title": "The Effects of Body Gestures and Gender on Viewer's Perception of Animated Pedagogical Agent's Emotions",
        "year": "2020",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-49062-1\\_11",
        "doi": "10.1007/978-3-030-49062-1_11"
    },
    "yuan_emotion_2017": {
        "title": "Emotion Evaluation Through Body Movements Based on Silhouette Extraction",
        "year": "2017",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-58071-5\\_55",
        "doi": "10.1007/978-3-319-58071-5_55"
    },
    "kaza_body_2016": {
        "title": "Body Motion Analysis for Emotion Recognition in Serious Games",
        "year": "2016",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-40244-4\\_4",
        "doi": "10.1007/978-3-319-40244-4_4"
    },
    "sakurai_basic_2015-2": {
        "title": "Basic Study of Evoking Emotion Through Extending One's Body Image by Integration of Internal Sense and External Sense",
        "year": "2015",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-20612-7\\_42",
        "doi": "10.1007/978-3-319-20612-7_42"
    },
    "mancini_effects_2014": {
        "title": "Effects of Gender Mapping on the Perception of Emotion from Upper Body Movement in Virtual Characters",
        "year": "2014",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-07458-0\\_25",
        "doi": "10.1007/978-3-319-07458-0_25"
    },
    "sakurai_evoking_2014": {
        "title": "Evoking Emotions in a Story Using Tactile Sensations as Pseudo-body Responses with Contextual Cues",
        "year": "2014",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-07731-4\\_25",
        "doi": "10.1007/978-3-319-07731-4_25"
    },
    "park_vision_2013": {
        "title": "Vision Based Body Dither Measurement for Estimating Human Emotion Parameters",
        "year": "2013",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-642-39342-6\\_38",
        "doi": "10.1007/978-3-642-39342-6_38"
    },
    "huang_emotion_2021": {
        "title": "Emotion Recognition Based on Body and Context Fusion in the Wild",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICCVW",
        "url": "https://doi.org/10.1109/ICCVW54120.2021.00403",
        "doi": "10.1109/ICCVW54120.2021.00403"
    },
    "filntisis_emotion_2020-2": {
        "title": "Emotion Understanding in Videos Through Body, Context, and Visual-Semantic Embedding Loss",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-030-66415-2\\_52",
        "doi": "10.1007/978-3-030-66415-2_52"
    },
    "waltemate_impact_2018": {
        "title": "The Impact of Avatar Personalization and Immersion on Virtual Body Ownership, Presence, and Emotional Response",
        "year": "2018",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2018.2794629",
        "doi": "10.1109/TVCG.2018.2794629"
    },
    "yang_facial_2021-1": {
        "title": "Facial Action Unit-based Deep Learning Framework for Spotting Macro- and Micro-expressions in Long Video Sequences",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3474085.3479209",
        "doi": "10.1145/3474085.3479209"
    },
    "zhang_facial_2021-3": {
        "title": "Facial Action Unit Detection with Local Key Facial Sub-region based Multi-label Classification for Micro-expression Analysis",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM FME",
        "url": "https://doi.org/10.1145/3476100.3484462",
        "doi": "10.1145/3476100.3484462"
    },
    "li_multi-person_2020": {
        "title": "Multi-Person Action Recognition in Microwave Sensors",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3394171.3413801",
        "doi": "10.1145/3394171.3413801"
    },
    "lei_micro-expression_2021-1": {
        "title": "Micro-Expression Recognition Based on Facial Graph Representation Learning and Facial Action Unit Fusion",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CVPRW",
        "url": "https://openaccess.thecvf.com/content/CVPR2021W/AUVi/html/Lei\\_Micro-Expression\\_Recognition\\_Based\\_on\\_Facial\\_Graph\\_Representation\\_Learning\\_and\\_Facial\\_CVPRW\\_2021\\_paper.html",
        "doi": "10.1109/CVPRW53098.2021.00173"
    },
    "yonetani_recognizing_2016": {
        "title": "Recognizing Micro-Actions and Reactions from Paired Egocentric Videos",
        "year": "2016",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPR.2016.288",
        "doi": "10.1109/CVPR.2016.288"
    },
    "saha_amtnet_2017": {
        "title": "AMTnet: Action-Micro-Tube Regression by End-to-end Trainable Deep Architecture",
        "year": "2017",
        "type": "inproceedings",
        "venue": "ICCV",
        "url": "https://doi.org/10.1109/ICCV.2017.473",
        "doi": "10.1109/ICCV.2017.473"
    },
    "chen_smg_2023": {
        "title": "SMG: A Micro-gesture Dataset Towards Spontaneous Body Gestures for Emotional Stress State Analysis",
        "year": "2023",
        "type": "article",
        "venue": "IJCV",
        "url": "https://doi.org/10.1007/s11263-023-01761-6",
        "doi": "10.1007/S11263-023-01761-6"
    },
    "wang_survey_2023": {
        "title": "A survey on emotional visualization and visual analysis",
        "year": "2023",
        "type": "article",
        "venue": "J. Vis.",
        "url": "https://doi.org/10.1007/s12650-022-00872-5",
        "doi": "10.1007/S12650-022-00872-5"
    },
    "shoumy_multimodal_2020": {
        "title": "Multimodal big data affective analytics: A comprehensive survey using text, audio, visual and physiological signals",
        "year": "2020",
        "type": "article",
        "venue": "J. Netw. Comput. Appl.",
        "url": "https://doi.org/10.1016/j.jnca.2019.102447",
        "doi": "10.1016/J.JNCA.2019.102447"
    },
    "leong_facial_2023": {
        "title": "Facial expression and body gesture emotion recognition: A systematic review on the use of visual data in affective computing",
        "year": "2023",
        "type": "article",
        "venue": "Comput. Sci. Rev.",
        "url": "https://doi.org/10.1016/j.cosrev.2023.100545",
        "doi": "10.1016/J.COSREV.2023.100545"
    },
    "johnson_towards_2023": {
        "title": "Towards Interpretability in Audio and Visual Affective Machine Learning: A Review",
        "year": "2023",
        "type": "article",
        "venue": "CoRR",
        "url": "https://doi.org/10.48550/arXiv.2306.08933",
        "doi": "10.48550/ARXIV.2306.08933"
    },
    "palacios_affective_2021": {
        "title": "Affective Visualization in Virtual Reality: An Integrative Review",
        "year": "2021",
        "type": "article",
        "venue": "Frontiers Virtual Real.",
        "url": "https://doi.org/10.3389/frvir.2021.630731",
        "doi": "10.3389/FRVIR.2021.630731"
    },
    "zhang_deep_2024": {
        "title": "Deep learning-based multimodal emotion recognition from audio, visual, and text modalities: A systematic review of recent advancements and future prospects",
        "year": "2024",
        "type": "article",
        "venue": "Expert Syst. Appl.",
        "url": "https://doi.org/10.1016/j.eswa.2023.121692",
        "doi": "10.1016/J.ESWA.2023.121692"
    },
    "veeranki_systematic_2021": {
        "title": "A Systematic Review of Sensing and Differentiating Dichotomous Emotional States Using Audio-Visual Stimuli",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Access",
        "url": "https://doi.org/10.1109/ACCESS.2021.3110773",
        "doi": "10.1109/ACCESS.2021.3110773"
    },
    "palacios_emotion_2020": {
        "title": "Emotion visualization in Virtual Reality: An integrative review",
        "year": "2020",
        "type": "article",
        "venue": "CoRR",
        "url": "https://arxiv.org/abs/2012.08849",
        "doi": null
    },
    "haeringen_emotion_2023": {
        "title": "Emotion contagion in agent-based simulations of crowds: a systematic review",
        "year": "2023",
        "type": "article",
        "venue": "Auton. Agents Multi Agent Syst.",
        "url": "https://doi.org/10.1007/s10458-022-09589-z",
        "doi": "10.1007/S10458-022-09589-Z"
    },
    "pan_review_2023": {
        "title": "A review of multimodal emotion recognition from datasets, preprocessing, features, and fusion methods",
        "year": "2023",
        "type": "article",
        "venue": "Neurocomputing",
        "url": "https://doi.org/10.1016/j.neucom.2023.126866",
        "doi": "10.1016/J.NEUCOM.2023.126866"
    },
    "ezzameli_emotion_2023": {
        "title": "Emotion recognition from unimodal to multimodal analysis: A review",
        "year": "2023",
        "type": "article",
        "venue": "Inf. Fusion",
        "url": "https://doi.org/10.1016/j.inffus.2023.101847",
        "doi": "10.1016/J.INFFUS.2023.101847"
    },
    "chau_advances_2023": {
        "title": "Advances in Emotion Recognition for Driving: A Review of Uni-Modal and Multi-Modal Methods",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CoDIT",
        "url": "https://doi.org/10.1109/CoDIT58514.2023.10284410",
        "doi": "10.1109/CODIT58514.2023.10284410"
    },
    "chaturvedi_music_2022": {
        "title": "Music mood and human emotion recognition based on physiological signals: a systematic review",
        "year": "2022",
        "type": "article",
        "venue": "Multim. Syst.",
        "url": "https://doi.org/10.1007/s00530-021-00786-6",
        "doi": "10.1007/S00530-021-00786-6"
    },
    "assuncao_considering_2022": {
        "title": "Considering emotions and contextual factors in music recommendation: a systematic literature review",
        "year": "2022",
        "type": "article",
        "venue": "Multim. Tools Appl.",
        "url": "https://doi.org/10.1007/s11042-022-12110-z",
        "doi": "10.1007/S11042-022-12110-Z"
    },
    "tang_emotion_2022": {
        "title": "Emotion Analysis of Chinese reviews Based on fusion of Multi-layer CNN and LSTM",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ICIGP",
        "url": "https://doi.org/10.1145/3512388.3512439",
        "doi": "10.1145/3512388.3512439"
    },
    "wang_how_2021": {
        "title": "How Can Autonomous Vehicles Convey Emotions to Pedestrians? A Review of Emotionally Expressive Non-Humanoid Robots",
        "year": "2021",
        "type": "article",
        "venue": "Multimodal Technol. Interact.",
        "url": "https://doi.org/10.3390/mti5120084",
        "doi": "10.3390/MTI5120084"
    },
    "pang_emotional_2021": {
        "title": "Emotional Design for Educational Multimedia: A Mini-Review",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HSI",
        "url": "https://doi.org/10.1109/HSI52170.2021.9538667",
        "doi": "10.1109/HSI52170.2021.9538667"
    },
    "noroozi_multimodal_2020": {
        "title": "Multimodal data indicators for capturing cognitive, motivational, and emotional learning processes: A systematic literature review",
        "year": "2020",
        "type": "article",
        "venue": "Educ. Inf. Technol.",
        "url": "https://doi.org/10.1007/s10639-020-10229-w",
        "doi": "10.1007/S10639-020-10229-W"
    },
    "zhang_emotion_2020": {
        "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
        "year": "2020",
        "type": "article",
        "venue": "Inf. Fusion",
        "url": "https://doi.org/10.1016/j.inffus.2020.01.011",
        "doi": "10.1016/J.INFFUS.2020.01.011"
    },
    "wang_review_2020": {
        "title": "A review of emotion sensing: categorization models and algorithms",
        "year": "2020",
        "type": "article",
        "venue": "Multim. Tools Appl.",
        "url": "https://doi.org/10.1007/s11042-019-08328-z",
        "doi": "10.1007/S11042-019-08328-Z"
    },
    "huang_responding_2020": {
        "title": "Responding to Reviews Expressing Different Emotional Arousal Levels: The Effect of Managerial Response on Multi-Dimensional Ratings",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ICIS",
        "url": "https://aisel.aisnet.org/icis2020/digital\\_commerce/digital\\_commerce/8",
        "doi": null
    },
    "stappen_muse_2020": {
        "title": "MuSe 2020 Challenge and Workshop: Multimodal Sentiment Analysis, Emotion-target Engagement and Trustworthiness Detection in Real-life Media: Emotional Car Reviews in-the-wild",
        "year": "2020",
        "type": "inproceedings",
        "venue": "MuSe",
        "url": "https://doi.org/10.1145/3423327.3423673",
        "doi": "10.1145/3423327.3423673"
    },
    "kocon_multilingual_2019": {
        "title": "Multilingual and Language-Agnostic Recognition of Emotions, Valence and Arousal in Large-Scale Multi-domain Text Reviews",
        "year": "2019",
        "type": "inproceedings",
        "venue": "LTC",
        "url": "https://doi.org/10.1007/978-3-031-05328-3\\_14",
        "doi": "10.1007/978-3-031-05328-3_14"
    },
    "yang_review_2018": {
        "title": "Review of data features-based music emotion recognition methods",
        "year": "2018",
        "type": "article",
        "venue": "Multim. Syst.",
        "url": "https://doi.org/10.1007/s00530-017-0559-4",
        "doi": "10.1007/S00530-017-0559-4"
    },
    "cooney_design_2018": {
        "title": "Design for an Art Therapy Robot: An Explorative Review of the Theoretical Foundations for Engaging in Emotional and Creative Painting with a Robot",
        "year": "2018",
        "type": "article",
        "venue": "Multimodal Technol. Interact.",
        "url": "https://doi.org/10.3390/mti2030052",
        "doi": "10.3390/MTI2030052"
    },
    "chakraverty_review_2017": {
        "title": "Review based emotion profiles for cross domain recommendation",
        "year": "2017",
        "type": "article",
        "venue": "Multim. Tools Appl.",
        "url": "https://doi.org/10.1007/s11042-017-4767-x",
        "doi": "10.1007/S11042-017-4767-X"
    },
    "jiang_cognitive_2016": {
        "title": "Cognitive Detection of Multiple Discrete Emotions from Chinese Online Reviews",
        "year": "2016",
        "type": "inproceedings",
        "venue": "DCS",
        "url": "https://doi.org/10.1109/DSC.2016.85",
        "doi": "10.1109/DSC.2016.85"
    },
    "buitinck_multi-emotion_2015": {
        "title": "Multi-emotion Detection in User-Generated Reviews",
        "year": "2015",
        "type": "inproceedings",
        "venue": "EICR",
        "url": "https://doi.org/10.1007/978-3-319-16354-3\\_5",
        "doi": "10.1007/978-3-319-16354-3_5"
    },
    "a_survey_2023": {
        "title": "Survey on multimodal approaches to emotion recognition",
        "year": "2023",
        "type": "article",
        "venue": "Neurocomputing",
        "url": "https://doi.org/10.1016/j.neucom.2023.126693",
        "doi": "10.1016/J.NEUCOM.2023.126693"
    },
    "ahmed_systematic_2023": {
        "title": "A systematic survey on multimodal emotion recognition using learning algorithms",
        "year": "2023",
        "type": "article",
        "venue": "Intell. Syst. Appl.",
        "url": "https://doi.org/10.1016/j.iswa.2022.200171",
        "doi": "10.1016/J.ISWA.2022.200171"
    },
    "kamble_comprehensive_2023": {
        "title": "A comprehensive survey on emotion recognition based on electroencephalograph (EEG) signals",
        "year": "2023",
        "type": "article",
        "venue": "Multim. Tools Appl.",
        "url": "https://doi.org/10.1007/s11042-023-14489-9",
        "doi": "10.1007/S11042-023-14489-9"
    },
    "kaur_trends_2023": {
        "title": "Trends in speech emotion recognition: a comprehensive survey",
        "year": "2023",
        "type": "article",
        "venue": "Multim. Tools Appl.",
        "url": "https://doi.org/10.1007/s11042-023-14656-y",
        "doi": "10.1007/S11042-023-14656-Y"
    },
    "horvat_survey_2023": {
        "title": "A Survey on Usage of Multimedia Databases for Emotion Elicitation: A Quantitative Report on How Content Diversity Can Improve Performance",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ICT",
        "url": "https://doi.org/10.23919/MIPRO57284.2023.10159867",
        "doi": "10.23919/MIPRO57284.2023.10159867"
    },
    "siddiqui_survey_2022": {
        "title": "A Survey on Databases for Multimodal Emotion Recognition and an Introduction to the VIRI (Visible and InfraRed Image) Database",
        "year": "2022",
        "type": "article",
        "venue": "Multimodal Technol. Interact.",
        "url": "https://doi.org/10.3390/mti6060047",
        "doi": "10.3390/MTI6060047"
    },
    "nandi_survey_2020": {
        "title": "A Survey on Multimodal Data Stream Mining for e-Learner's Emotion Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "COINS",
        "url": "https://doi.org/10.1109/COINS49042.2020.9191370",
        "doi": "10.1109/COINS49042.2020.9191370"
    },
    "marechal_survey_2019": {
        "title": "Survey on AI-Based Multimodal Methods for Emotion Detection",
        "year": "2019",
        "type": "incollection",
        "venue": "COST",
        "url": "https://doi.org/10.1007/978-3-030-16272-6\\_11",
        "doi": "10.1007/978-3-030-16272-6_11"
    },
    "braun_survey_2018": {
        "title": "A Survey to Understand Emotional Situations on the Road and What They Mean for Affective Automotive UIs",
        "year": "2018",
        "type": "article",
        "venue": "Multimodal Technol. Interact.",
        "url": "https://doi.org/10.3390/mti2040075",
        "doi": "10.3390/MTI2040075"
    },
    "arya_survey_2021": {
        "title": "A survey of multidisciplinary domains contributing to affective computing",
        "year": "2021",
        "type": "article",
        "venue": "Comput. Sci. Rev.",
        "url": "https://doi.org/10.1016/j.cosrev.2021.100399",
        "doi": "10.1016/J.COSREV.2021.100399"
    },
    "shoumy_multimodal_2020-1": {
        "title": "Multimodal big data affective analytics: A comprehensive survey using text, audio, visual and physiological signals",
        "year": "2020",
        "type": "article",
        "venue": "J. Netw. Comput. Appl.",
        "url": "https://doi.org/10.1016/j.jnca.2019.102447",
        "doi": "10.1016/J.JNCA.2019.102447"
    },
    "zhao_affective_2019": {
        "title": "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey",
        "year": "2019",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1911.05609",
        "doi": null
    },
    "braun_survey_2018-1": {
        "title": "A Survey to Understand Emotional Situations on the Road and What They Mean for Affective Automotive UIs",
        "year": "2018",
        "type": "article",
        "venue": "Multimodal Technol. Interact.",
        "url": "https://doi.org/10.3390/mti2040075",
        "doi": "10.3390/MTI2040075"
    },
    "bravo-marquez_words_2022": {
        "title": "Words, Tweets, and Reviews: Leveraging Affective Knowledge Between Multiple Domains",
        "year": "2022",
        "type": "article",
        "venue": "Cogn. Comput.",
        "url": "https://doi.org/10.1007/s12559-021-09923-9",
        "doi": "10.1007/S12559-021-09923-9"
    },
    "wang_multiple_2019": {
        "title": "Multiple affective attribute classification of online customer product reviews: A heuristic deep learning method for supporting Kansei engineering",
        "year": "2019",
        "type": "article",
        "venue": "Eng. Appl. Artif. Intell.",
        "url": "https://doi.org/10.1016/j.engappai.2019.05.015",
        "doi": "10.1016/J.ENGAPPAI.2019.05.015"
    },
    "wang_classification_2018": {
        "title": "Classification of Multiple Affective Attributes of Customer Reviews: Using Classical Machine Learning and Deep Learning",
        "year": "2018",
        "type": "inproceedings",
        "venue": "CSAE",
        "url": "https://doi.org/10.1145/3207677.3277953",
        "doi": "10.1145/3207677.3277953"
    },
    "poria_review_2017": {
        "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
        "year": "2017",
        "type": "article",
        "venue": "Inf. Fusion",
        "url": "https://doi.org/10.1016/j.inffus.2017.02.003",
        "doi": "10.1016/J.INFFUS.2017.02.003"
    },
    "kim_few-shot_2023": {
        "title": "Few-shot learning for facial expression recognition: a comprehensive survey",
        "year": "2023",
        "type": "article",
        "venue": "J. Real Time Image Process.",
        "url": "https://doi.org/10.1007/s11554-023-01310-x",
        "doi": "10.1007/S11554-023-01310-X"
    },
    "mohan_understanding_2023": {
        "title": "Understanding Deep Learning Techniques for Recognition of Human Emotions Using Facial Expressions: A Comprehensive Survey",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Instrum. Meas.",
        "url": "https://doi.org/10.1109/TIM.2023.3243661",
        "doi": "10.1109/TIM.2023.3243661"
    },
    "rawal_facial_2022": {
        "title": "Facial Emotion Expressions in Human-Robot Interaction: A Survey",
        "year": "2022",
        "type": "article",
        "venue": "Int. J. Soc. Robotics",
        "url": "https://doi.org/10.1007/s12369-022-00867-0",
        "doi": "10.1007/S12369-022-00867-0"
    },
    "esmaeili_comprehensive_2022": {
        "title": "A comprehensive survey on facial micro-expression: approaches and databases",
        "year": "2022",
        "type": "article",
        "venue": "Multim. Tools Appl.",
        "url": "https://doi.org/10.1007/s11042-022-13133-2",
        "doi": "10.1007/S11042-022-13133-2"
    },
    "ben_video-based_2022-1": {
        "title": "Video-Based Facial Micro-Expression Analysis: A Survey of Datasets, Features and Algorithms",
        "year": "2022",
        "type": "article",
        "venue": "TPAMI",
        "url": "https://doi.org/10.1109/TPAMI.2021.3067464",
        "doi": "10.1109/TPAMI.2021.3067464"
    },
    "guerdelli_macro-_2022": {
        "title": "Macro- and Micro-Expressions Facial Datasets: A Survey",
        "year": "2022",
        "type": "article",
        "venue": "Sensors",
        "url": "https://doi.org/10.3390/s22041524",
        "doi": "10.3390/S22041524"
    },
    "jampour_multiview_2022-1": {
        "title": "Multiview Facial Expression Recognition, A Survey",
        "year": "2022",
        "type": "article",
        "venue": "TAC",
        "url": "https://doi.org/10.1109/TAFFC.2022.3184995",
        "doi": "10.1109/TAFFC.2022.3184995"
    },
    "li_deep_2022-1": {
        "title": "Deep Facial Expression Recognition: A Survey",
        "year": "2022",
        "type": "article",
        "venue": "TAC",
        "url": "https://doi.org/10.1109/TAFFC.2020.2981446",
        "doi": "10.1109/TAFFC.2020.2981446"
    },
    "dang_facial_2022": {
        "title": "Facial Expression Recognition: A Survey and its Applications",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ICACT",
        "url": "https://doi.org/10.23919/ICACT53585.2022.9728930",
        "doi": "10.23919/ICACT53585.2022.9728930"
    },
    "dwivedi_challenges_2022": {
        "title": "Challenges of Facial Micro-Expression Detection and Recognition: A Survey",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ICONIP",
        "url": "https://doi.org/10.1007/978-981-99-1648-1\\_40",
        "doi": "10.1007/978-981-99-1648-1_40"
    },
    "zhao_survey_2021": {
        "title": "Survey on Facial Expression Recognition: History, Applications, and Challenges",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Multim.",
        "url": "https://doi.org/10.1109/MMUL.2021.3107862",
        "doi": "10.1109/MMUL.2021.3107862"
    },
    "hassan_automatic_2021-1": {
        "title": "Automatic Detection of Pain from Facial Expressions: A Survey",
        "year": "2021",
        "type": "article",
        "venue": "TPAMI",
        "url": "https://doi.org/10.1109/TPAMI.2019.2958341",
        "doi": "10.1109/TPAMI.2019.2958341"
    },
    "oidekivi_identifying_2021": {
        "title": "Identifying emotions from facial expression displays of robots - results from a survey study",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HSI",
        "url": "https://doi.org/10.1109/HSI52170.2021.9538774",
        "doi": "10.1109/HSI52170.2021.9538774"
    },
    "dang_facial_2021": {
        "title": "Facial Expression Recognition: A Survey and its Applications",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICACT",
        "url": "https://doi.org/10.23919/ICACT51234.2021.9370369",
        "doi": "10.23919/ICACT51234.2021.9370369"
    },
    "rawal_facial_2021": {
        "title": "Facial emotion expressions in human-robot interaction: A survey",
        "year": "2021",
        "type": "article",
        "venue": "CoRR",
        "url": "https://arxiv.org/abs/2103.07169",
        "doi": null
    },
    "khan_survey_2020": {
        "title": "A survey on analysis of human faces and facial expressions datasets",
        "year": "2020",
        "type": "article",
        "venue": "Int. J. Mach. Learn. Cybern.",
        "url": "https://doi.org/10.1007/s13042-019-00995-6",
        "doi": "10.1007/S13042-019-00995-6"
    },
    "dufourq_survey_2020": {
        "title": "A Survey on Factors Affecting Facial Expression Recognition based on Convolutional Neural Networks",
        "year": "2020",
        "type": "inproceedings",
        "venue": "SAICSIT",
        "url": "https://doi.org/10.1145/3410886.3410891",
        "doi": "10.1145/3410886.3410891"
    },
    "rajan_facial_2019": {
        "title": "Facial expression recognition techniques: a comprehensive survey",
        "year": "2019",
        "type": "article",
        "venue": "IET Image Process.",
        "url": "https://doi.org/10.1049/iet-ipr.2018.6647",
        "doi": "10.1049/IET-IPR.2018.6647"
    },
    "jiang_unicode38750unicode27491unicode38754unicode20154unicode33080unicode34920unicode24773unicode35782unicode21035unicode26041unicode27861unicode32508unicode36848_2019": {
        "title": "\\textbackslashunicode38750\\textbackslashunicode27491\\textbackslashunicode38754\\textbackslashunicode20154\\textbackslashunicode33080\\textbackslashunicode34920\\textbackslashunicode24773\\textbackslashunicode35782\\textbackslashunicode21035\\textbackslashunicode26041\\textbackslashunicode27861\\textbackslashunicode32508\\textbackslashunicode36848 (Survey on Non-frontal Facial Expression Recognition Methods)",
        "year": "2019",
        "type": "article",
        "venue": "\u4e2d\u6587",
        "url": "https://doi.org/10.11896/j.issn.1002-137X.2019.03.007",
        "doi": "10.11896/J.ISSN.1002-137X.2019.03.007"
    },
    "saha_survey_2019": {
        "title": "A Survey on Image Acquisition Protocols for Non-posed Facial Expression Recognition Systems",
        "year": "2019",
        "type": "article",
        "venue": "MTA",
        "url": "https://doi.org/10.1007/s11042-019-7596-2",
        "doi": "10.1007/S11042-019-7596-2"
    },
    "huang_facial_2019": {
        "title": "Facial Expression Recognition: A Survey",
        "year": "2019",
        "type": "article",
        "venue": "Symmetry",
        "url": "https://doi.org/10.3390/sym11101189",
        "doi": "10.3390/SYM11101189"
    },
    "luqin_survey_2019": {
        "title": "A Survey of Facial Expression Recognition Based on Convolutional Neural Network",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ICIS",
        "url": "https://doi.org/10.1109/ICIS46139.2019.8940228",
        "doi": "10.1109/ICIS46139.2019.8940228"
    },
    "li_survey_2019": {
        "title": "A Survey on Databases for Facial Micro-Expression Analysis",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IJCV",
        "url": "https://doi.org/10.5220/0007309202410248",
        "doi": "10.5220/0007309202410248"
    },
    "zhang_facial_2018": {
        "title": "Facial Expression Analysis under Partial Occlusion: A Survey",
        "year": "2018",
        "type": "article",
        "venue": "ACM Comput. Surv.",
        "url": "https://doi.org/10.1145/3158369",
        "doi": "10.1145/3158369"
    },
    "azam_feature_2018": {
        "title": "Feature Extraction Trends for Intelligent Facial Expression Recognition: A Survey",
        "year": "2018",
        "type": "article",
        "venue": "Informatica (Slovenia)",
        "url": "https://doi.org/10.31449/inf.v42i4.2037",
        "doi": "10.31449/INF.V42I4.2037"
    },
    "takalkar_survey_2018": {
        "title": "A survey: facial micro-expression recognition",
        "year": "2018",
        "type": "article",
        "venue": "MTA",
        "url": "https://doi.org/10.1007/s11042-017-5317-2",
        "doi": "10.1007/S11042-017-5317-2"
    },
    "weber_survey_2018": {
        "title": "A Survey on Databases of Facial Macro-expression and Micro-expression",
        "year": "2018",
        "type": "inproceedings",
        "venue": "VISIGRAPP",
        "url": "https://doi.org/10.1007/978-3-030-26756-8\\_15",
        "doi": "10.1007/978-3-030-26756-8_15"
    },
    "chengeta_facial_2018": {
        "title": "Facial Expression Recognition: A Survey on Local Binary and Local Directional Patterns",
        "year": "2018",
        "type": "inproceedings",
        "venue": "ICCCI",
        "url": "https://doi.org/10.1007/978-3-319-98443-8\\_47",
        "doi": "10.1007/978-3-319-98443-8_47"
    },
    "weber_survey_2018-1": {
        "title": "A Survey on Databases for Facial Expression Analysis",
        "year": "2018",
        "type": "inproceedings",
        "venue": "VISIGRAPP",
        "url": "https://doi.org/10.5220/0006553900730084",
        "doi": "10.5220/0006553900730084"
    },
    "li_deep_2018": {
        "title": "Deep Facial Expression Recognition: A Survey",
        "year": "2018",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1804.08348",
        "doi": null
    },
    "oh_survey_2018": {
        "title": "A Survey of Automatic Facial Micro-expression Analysis: Databases, Methods and Challenges",
        "year": "2018",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1806.05781",
        "doi": null
    },
    "deshmukh_survey_2016": {
        "title": "Survey on real-time facial expression recognition techniques",
        "year": "2016",
        "type": "article",
        "venue": "IET Biom.",
        "url": "https://doi.org/10.1049/iet-bmt.2014.0104",
        "doi": "10.1049/IET-BMT.2014.0104"
    },
    "corneanu_survey_2016": {
        "title": "Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-related Applications",
        "year": "2016",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1606.03237",
        "doi": null
    },
    "danelakis_survey_2015": {
        "title": "A survey on facial expression recognition in 3D video sequences",
        "year": "2015",
        "type": "article",
        "venue": "Multim. Tools Appl.",
        "url": "https://doi.org/10.1007/s11042-014-1869-6",
        "doi": "10.1007/S11042-014-1869-6"
    },
    "danelakis_erratum_2015": {
        "title": "Erratum to: A survey on facial expression recognition in 3D video sequences",
        "year": "2015",
        "type": "article",
        "venue": "Multim. Tools Appl.",
        "url": "https://doi.org/10.1007/s11042-014-2012-4",
        "doi": "10.1007/S11042-014-2012-4"
    },
    "roychowdhury_survey_2015": {
        "title": "A Survey of the Trends in Facial and Expression Recognition Databases and Methods",
        "year": "2015",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1511.02407",
        "doi": null
    },
    "murtaza_analysis_2013": {
        "title": "Analysis of face recognition under varying facial expression: a survey",
        "year": "2013",
        "type": "article",
        "venue": "Int. Arab J. Inf. Technol.",
        "url": "http://iajit.org/index.php?option=com\\_content\\&task=blogcategory\\&id=87\\&Itemid=355",
        "doi": null
    },
    "agianpuye_3d_2013": {
        "title": "3D Facial expression synthesis: A survey",
        "year": "2013",
        "type": "inproceedings",
        "venue": "8th International Conference on Information Technology in Asia, CITA 2013, Kota Samarahan, Malaysia, July 1-4, 2013",
        "url": "https://doi.org/10.1109/CITA.2013.6637552",
        "doi": "10.1109/CITA.2013.6637552"
    },
    "sandbach_static_2012": {
        "title": "Static and dynamic 3D facial expression recognition: A comprehensive survey",
        "year": "2012",
        "type": "article",
        "venue": "Image Vis. Comput.",
        "url": "https://doi.org/10.1016/j.imavis.2012.06.005",
        "doi": "10.1016/J.IMAVIS.2012.06.005"
    },
    "fu_parametric_2012": {
        "title": "A Parametric Survey for Facial Expression Database",
        "year": "2012",
        "type": "inproceedings",
        "venue": "Advances in Brain Inspired Cognitive Systems - 5th International Conference, BICS 2012, Shenyang, China, July 11-14, 2012. Proceedings",
        "url": "https://doi.org/10.1007/978-3-642-31561-9\\_42",
        "doi": "10.1007/978-3-642-31561-9_42"
    },
    "wu_survey_2012": {
        "title": "Survey of the Facial Expression Recognition Research",
        "year": "2012",
        "type": "inproceedings",
        "venue": "Advances in Brain Inspired Cognitive Systems - 5th International Conference, BICS 2012, Shenyang, China, July 11-14, 2012. Proceedings",
        "url": "https://doi.org/10.1007/978-3-642-31561-9\\_44",
        "doi": "10.1007/978-3-642-31561-9_44"
    },
    "fang_facial_2023": {
        "title": "Facial Expression Recognition in Educational Research From the Perspective of Machine Learning: A Systematic Review",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Access",
        "url": "https://doi.org/10.1109/ACCESS.2023.3322454",
        "doi": "10.1109/ACCESS.2023.3322454"
    },
    "pinto_systematic_2023": {
        "title": "A Systematic Review of Facial Expression Detection Methods",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Access",
        "url": "https://doi.org/10.1109/ACCESS.2023.3287090",
        "doi": "10.1109/ACCESS.2023.3287090"
    },
    "leong_facial_2023-1": {
        "title": "Facial expression and body gesture emotion recognition: A systematic review on the use of visual data in affective computing",
        "year": "2023",
        "type": "article",
        "venue": "Comput. Sci. Rev.",
        "url": "https://doi.org/10.1016/j.cosrev.2023.100545",
        "doi": "10.1016/J.COSREV.2023.100545"
    },
    "adyapady_comprehensive_2023": {
        "title": "A comprehensive review of facial expression recognition techniques",
        "year": "2023",
        "type": "article",
        "venue": "Multim. Syst.",
        "url": "https://doi.org/10.1007/s00530-022-00984-w",
        "doi": "10.1007/S00530-022-00984-W"
    },
    "ekundayo_facial_2021": {
        "title": "Facial Expression Recognition: A Review of Trends and Techniques",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Access",
        "url": "https://doi.org/10.1109/ACCESS.2021.3113464",
        "doi": "10.1109/ACCESS.2021.3113464"
    },
    "sonawane_review_2021": {
        "title": "Review of automated emotion-based quantification of facial expression in Parkinson's patients",
        "year": "2021",
        "type": "article",
        "venue": "Vis. Comput.",
        "url": "https://doi.org/10.1007/s00371-020-01859-9",
        "doi": "10.1007/S00371-020-01859-9"
    },
    "alexandre_systematic_2020-1": {
        "title": "Systematic review of 3D facial expression recognition methods",
        "year": "2020",
        "type": "article",
        "venue": "PR",
        "url": "https://doi.org/10.1016/j.patcog.2019.107108",
        "doi": "10.1016/J.PATCOG.2019.107108"
    },
    "ullah_systematic_2020": {
        "title": "A Systematic Literature Review of Recognition of Compound Facial Expression of Emotions",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ICVIP",
        "url": "https://doi.org/10.1145/3447450.3447469",
        "doi": "10.1145/3447450.3447469"
    },
    "samadiani_review_2019": {
        "title": "A Review on Automatic Facial Expression Recognition Systems Assisted by Multimodal Sensor Data",
        "year": "2019",
        "type": "article",
        "venue": "Sensors",
        "url": "https://doi.org/10.3390/s19081863",
        "doi": "10.3390/S19081863"
    },
    "ray_review_2019": {
        "title": "A Review on Facial Expression Based Behavioral Analysis Using Computational Technique for Autistic Disorder Patients",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ICACDS",
        "url": "https://doi.org/10.1007/978-981-13-9942-8\\_43",
        "doi": "10.1007/978-981-13-9942-8_43"
    },
    "chengeta_review_2019": {
        "title": "A Review of Local, Holistic and Deep Learning Approaches in Facial Expressions Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ICTAS",
        "url": "https://doi.org/10.1109/ICTAS.2019.8703521",
        "doi": "10.1109/ICTAS.2019.8703521"
    },
    "ekundayo_facial_2019": {
        "title": "Facial Expression Recognition: A Review of Methods, Performances and Limitations",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ICTAS",
        "url": "https://doi.org/10.1109/ICTAS.2019.8703619",
        "doi": "10.1109/ICTAS.2019.8703619"
    },
    "chengeta_review_2019-1": {
        "title": "A Review of Local Feature Algorithms and Deep Learning Approaches in Facial Expression Recognition with Tensorflow and Keras",
        "year": "2019",
        "type": "inproceedings",
        "venue": "MCPR",
        "url": "https://doi.org/10.1007/978-3-030-21077-9\\_12",
        "doi": "10.1007/978-3-030-21077-9_12"
    },
    "merghani_review_2018": {
        "title": "A Review on Facial Micro-Expressions Analysis: Datasets, Features and Metrics",
        "year": "2018",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1805.02397",
        "doi": null
    },
    "chen_automated_2018": {
        "title": "Automated Pain Detection from Facial Expressions using FACS: A Review",
        "year": "2018",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1811.07988",
        "doi": null
    },
    "chaugule_product_2016": {
        "title": "Product review based on optimized facial expression detection",
        "year": "2016",
        "type": "inproceedings",
        "venue": "IC3",
        "url": "https://doi.org/10.1109/IC3.2016.7880213",
        "doi": "10.1109/IC3.2016.7880213"
    },
    "greche_performance_2016": {
        "title": "Performance Review of a Multi-Layer Feed-Forward Neural Network and Normalized Cross Correlation for Facial Expression Identification",
        "year": "2016",
        "type": "inproceedings",
        "venue": "SITIS",
        "url": "https://doi.org/10.1109/SITIS.2016.43",
        "doi": "10.1109/SITIS.2016.43"
    },
    "mary_review_2015": {
        "title": "A Review on How Human Aging Influences Facial Expression Recognition (FER)",
        "year": "2015",
        "type": "inproceedings",
        "venue": "Innovations in Bio-Inspired Computing and Applications - Proceedings of the 6th International Conference on Innovations in Bio-Inspired Computing and Applications (IBICA 2015) held in Kochi, India during December 16-18, 2015",
        "url": "https://doi.org/10.1007/978-3-319-28031-8\\_27",
        "doi": "10.1007/978-3-319-28031-8_27"
    },
    "lee_consumer_2013": {
        "title": "Consumer reviews: reviewer avatar facial expression and review valence",
        "year": "2013",
        "type": "article",
        "venue": "Internet Res.",
        "url": "https://doi.org/10.1108/10662241311313277",
        "doi": "10.1108/10662241311313277"
    },
    "farman_facial_2023": {
        "title": "Facial Emotion Recognition in Smart Education Systems: A Review",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ISC2",
        "url": "https://doi.org/10.1109/ISC257844.2023.10293353",
        "doi": "10.1109/ISC257844.2023.10293353"
    },
    "ortmann_facial_2023": {
        "title": "Facial Emotion Recognition in Immersive Virtual Reality: A Systematic Literature Review",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM",
        "url": "https://doi.org/10.1145/3594806.3594861",
        "doi": "10.1145/3594806.3594861"
    },
    "canal_survey_2022": {
        "title": "A survey on facial emotion recognition techniques: A state-of-the-art literature review",
        "year": "2022",
        "type": "article",
        "venue": "Inf. Sci.",
        "url": "https://doi.org/10.1016/j.ins.2021.10.005",
        "doi": "10.1016/J.INS.2021.10.005"
    },
    "mohanta_trends_2022": {
        "title": "Trends and challenges of image analysis in facial emotion recognition: a review",
        "year": "2022",
        "type": "article",
        "venue": "NMAHIB",
        "url": "https://doi.org/10.1007/s13721-022-00376-0",
        "doi": "10.1007/S13721-022-00376-0"
    },
    "mazhar_movie_2022": {
        "title": "Movie Reviews Classification through Facial Image Recognition and Emotion Detection Using Machine Learning Methods",
        "year": "2022",
        "type": "article",
        "venue": "Symmetry",
        "url": "https://doi.org/10.3390/sym14122607",
        "doi": "10.3390/SYM14122607"
    },
    "veriscimo_facial_2021": {
        "title": "Facial Emotion Recognition in UX Evaluation: A Systematic Review",
        "year": "2021",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-030-78462-1\\_40",
        "doi": "10.1007/978-3-030-78462-1_40"
    },
    "mellouk_facial_2020": {
        "title": "Facial emotion recognition using deep learning: review and insights",
        "year": "2020",
        "type": "inproceedings",
        "venue": "MobiSPC",
        "url": "https://doi.org/10.1016/j.procs.2020.07.101",
        "doi": "10.1016/J.PROCS.2020.07.101"
    },
    "jia_detection_2020": {
        "title": "Detection of Genuine and Posed Facial Expressions of Emotion: A Review",
        "year": "2020",
        "type": "article",
        "venue": "CoRR",
        "url": "https://arxiv.org/abs/2008.11353",
        "doi": null
    },
    "ko_brief_2018": {
        "title": "A Brief Review of Facial Emotion Recognition Based on Visual Information",
        "year": "2018",
        "type": "article",
        "venue": "Sensors",
        "url": "https://doi.org/10.3390/s18020401",
        "doi": "10.3390/S18020401"
    },
    "gutierrez-maldonado_associations_2012": {
        "title": "Associations Between Facial Emotion Recognition, Cognition and Alexithymia in Patients with Schizophrenia: Comparison of Photographic and Virtual Reality Presentations",
        "year": "2012",
        "type": "incollection",
        "venue": "Annual Review of Cybertherapy and Telemedicine 2012 - Advanced Technologies in the Behavioral, Social and Neurosciences",
        "url": "https://doi.org/10.3233/978-1-61499-121-2-88",
        "doi": "10.3233/978-1-61499-121-2-88"
    },
    "rathour_decadal_2022": {
        "title": "The decadal perspective of facial emotion processing and Recognition: A survey",
        "year": "2022",
        "type": "article",
        "venue": "Displays",
        "url": "https://doi.org/10.1016/j.displa.2022.102330",
        "doi": "10.1016/J.DISPLA.2022.102330"
    },
    "amimi_survey_2022": {
        "title": "A Survey of Smart Classroom: Concept, Technologies and Facial Emotions Recognition Application",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ISC",
        "url": "https://doi.org/10.1007/978-3-031-16075-2\\_23",
        "doi": "10.1007/978-3-031-16075-2_23"
    },
    "amimi_survey_2022-1": {
        "title": "A survey of smart classroom: Concept, technologies and facial emotions recognition application",
        "year": "2022",
        "type": "article",
        "venue": "CoRR",
        "url": "https://doi.org/10.48550/arXiv.2212.01675",
        "doi": "10.48550/ARXIV.2212.01675"
    },
    "dalvi_survey_2021": {
        "title": "A Survey of AI-Based Facial Emotion Recognition: Features, ML \\& DL Techniques, Age-Wise Datasets and Future Directions",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Access",
        "url": "https://doi.org/10.1109/ACCESS.2021.3131733",
        "doi": "10.1109/ACCESS.2021.3131733"
    },
    "mehta_facial_2018": {
        "title": "Facial Emotion Recognition: A Survey and Real-World User Experiences in Mixed Reality",
        "year": "2018",
        "type": "article",
        "venue": "Sensors",
        "url": "https://doi.org/10.3390/s18020416",
        "doi": "10.3390/S18020416"
    },
    "yang_review_2023": {
        "title": "Review Helpfulness as a Function of discrete negative emotions and image colorfulness",
        "year": "2023",
        "type": "inproceedings",
        "venue": "PACIS",
        "url": "https://aisel.aisnet.org/pacis2023/52",
        "doi": null
    },
    "mohanta_trends_2022-1": {
        "title": "Trends and challenges of image analysis in facial emotion recognition: a review",
        "year": "2022",
        "type": "article",
        "venue": "NMAHIB",
        "url": "https://doi.org/10.1007/s13721-022-00376-0",
        "doi": "10.1007/S13721-022-00376-0"
    },
    "mazhar_movie_2022-1": {
        "title": "Movie Reviews Classification through Facial Image Recognition and Emotion Detection Using Machine Learning Methods",
        "year": "2022",
        "type": "article",
        "venue": "Symmetry",
        "url": "https://doi.org/10.3390/sym14122607",
        "doi": "10.3390/SYM14122607"
    },
    "sun_image_2019": {
        "title": "Image Based Review Text Generation with Emotional Guidance",
        "year": "2019",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1901.04140",
        "doi": null
    },
    "devi_descriptive_2023": {
        "title": "A Descriptive Survey on Face Emotion Recognition Techniques",
        "year": "2023",
        "type": "article",
        "venue": "Int. J. Image Graph.",
        "url": "https://doi.org/10.1142/S0219467823500080",
        "doi": "10.1142/S0219467823500080"
    },
    "siddiqui_survey_2022-1": {
        "title": "A Survey on Databases for Multimodal Emotion Recognition and an Introduction to the VIRI (Visible and InfraRed Image) Database",
        "year": "2022",
        "type": "article",
        "venue": "Multimodal Technol. Interact.",
        "url": "https://doi.org/10.3390/mti6060047",
        "doi": "10.3390/MTI6060047"
    },
    "zhao_survey_2021-1": {
        "title": "A Survey on Image Emotion Recognition",
        "year": "2021",
        "type": "article",
        "venue": "J. Inf. Process. Syst.",
        "url": "https://doi.org/10.3745/JIPS.01.0082",
        "doi": "10.3745/JIPS.01.0082"
    },
    "wang_survey_2008": {
        "title": "A survey on emotional semantic image retrieval",
        "year": "2008",
        "type": "inproceedings",
        "venue": "ICIP",
        "url": "https://doi.org/10.1109/ICIP.2008.4711705",
        "doi": "10.1109/ICIP.2008.4711705"
    },
    "zhao_affective_2018-1": {
        "title": "Affective Image Content Analysis: A Comprehensive Survey",
        "year": "2018",
        "type": "inproceedings",
        "venue": "IJCAI",
        "url": "https://doi.org/10.24963/ijcai.2018/780",
        "doi": "10.24963/IJCAI.2018/780"
    },
    "gnacek_avdos_2022-1": {
        "title": "AVDOS - Affective Video Database Online Study Video database for affective research emotionally validated through an online survey",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACII",
        "url": "https://doi.org/10.1109/ACII55700.2022.9953891",
        "doi": "10.1109/ACII55700.2022.9953891"
    },
    "xue_emotion_2023": {
        "title": "Emotion Recognition by Video: A review",
        "year": "2023",
        "type": "article",
        "venue": "CoRR",
        "url": "https://doi.org/10.48550/arXiv.2310.17212",
        "doi": "10.48550/ARXIV.2310.17212"
    },
    "wei_prior_2023": {
        "title": "Prior Information Based Decomposition and Reconstruction Learning for Micro-Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "IEICE Trans. Inf. Syst.",
        "url": "https://doi.org/10.1587/transinf.2022edl8065",
        "doi": "10.1587/TRANSINF.2022EDL8065"
    },
    "zhao_facial_2023-1": {
        "title": "Facial Micro-Expressions: An Overview",
        "year": "2023",
        "type": "article",
        "venue": "Proc. IEEE",
        "url": "https://doi.org/10.1109/JPROC.2023.3275192",
        "doi": "10.1109/JPROC.2023.3275192"
    },
    "li_4dme_2023": {
        "title": "4DME: A Spontaneous 4D Micro-Expression Dataset With Multimodalities",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3182342",
        "doi": "10.1109/TAFFC.2022.3182342"
    },
    "varanka_learnable_2023": {
        "title": "Learnable Eulerian Dynamics for Micro-Expression Action Unit Detection",
        "year": "2023",
        "type": "inproceedings",
        "venue": "Image Analysis - 22nd Scandinavian Conference, SCIA 2023, Sirkka, Finland, April 18-21, 2023, Proceedings, Part II",
        "url": "https://doi.org/10.1007/978-3-031-31438-4\\_26",
        "doi": "10.1007/978-3-031-31438-4_26"
    },
    "wei_prior_2023-1": {
        "title": "Prior Information based Decomposition and Reconstruction Learning for Micro-Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "CoRR",
        "url": "https://doi.org/10.48550/arXiv.2303.01776",
        "doi": "10.48550/ARXIV.2303.01776"
    },
    "zhou_empirical_2023": {
        "title": "An Empirical Study of Super-resolution on Low-resolution Micro-expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "CoRR",
        "url": "https://doi.org/10.48550/arXiv.2310.10022",
        "doi": "10.48550/ARXIV.2310.10022"
    },
    "li_deep_2022-2": {
        "title": "Deep Learning for Micro-Expression Recognition: A Survey",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3205170",
        "doi": "10.1109/TAFFC.2022.3205170"
    },
    "zhang_short_2022": {
        "title": "Short and Long Range Relation Based Spatio-Temporal Transformer for Micro-Expression Recognition",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3213509",
        "doi": "10.1109/TAFFC.2022.3213509"
    },
    "zhang_cross-database_2022": {
        "title": "Cross-Database Micro-Expression Recognition: A Benchmark",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Knowl. Data Eng.",
        "url": "https://doi.org/10.1109/TKDE.2020.2985365",
        "doi": "10.1109/TKDE.2020.2985365"
    },
    "wei_geometric_2022": {
        "title": "Geometric Graph Representation with Learnable Graph Structure and Adaptive AU Constraint for Micro-Expression Recognition",
        "year": "2022",
        "type": "article",
        "venue": "CoRR",
        "url": "https://doi.org/10.48550/arXiv.2205.00380",
        "doi": "10.48550/ARXIV.2205.00380"
    },
    "varanka_data_2022": {
        "title": "Data Leakage and Evaluation Issues in Micro-Expression Analysis",
        "year": "2022",
        "type": "article",
        "venue": "CoRR",
        "url": "https://doi.org/10.48550/arXiv.2211.11425",
        "doi": "10.48550/ARXIV.2211.11425"
    },
    "li_micro-expression_2021": {
        "title": "Micro-expression action unit detection with spatial and channel attention",
        "year": "2021",
        "type": "article",
        "venue": "Neurocomputing",
        "url": "https://doi.org/10.1016/j.neucom.2021.01.032",
        "doi": "10.1016/J.NEUCOM.2021.01.032"
    },
    "tran_micro-expression_2021": {
        "title": "Micro-expression spotting: A new benchmark",
        "year": "2021",
        "type": "article",
        "venue": "Neurocomputing",
        "url": "https://doi.org/10.1016/j.neucom.2021.02.022",
        "doi": "10.1016/J.NEUCOM.2021.02.022"
    },
    "li_joint_2021": {
        "title": "Joint Local and Global Information Learning With Single Apex Frame Detection for Micro-Expression Recognition",
        "year": "2021",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2020.3035042",
        "doi": "10.1109/TIP.2020.3035042"
    },
    "li_micro-expression_2021-1": {
        "title": "Micro-expression Action Unit Detection with Dual-view Attentive Similarity-Preserving Knowledge Distillation",
        "year": "2021",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG52635.2021.9666975",
        "doi": "10.1109/FG52635.2021.9666975"
    },
    "varanka_micro-expression_2021": {
        "title": "Micro-Expression Recognition with Noisy Labels",
        "year": "2021",
        "type": "inproceedings",
        "venue": "Human Vision and Electronic Imaging 2021, Virtual Event, January 2021",
        "url": "https://doi.org/10.2352/ISSN.2470-1173.2021.11.HVEI-157",
        "doi": "10.2352/ISSN.2470-1173.2021.11.HVEI-157"
    },
    "li_intra-_2021": {
        "title": "Intra- and Inter-Contrastive Learning for Micro-expression Action Unit Detection",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICMI '21: International Conference on Multimodal Interaction, Montr\u00e9al, QC, Canada, October 18-22, 2021",
        "url": "https://doi.org/10.1145/3462244.3479956",
        "doi": "10.1145/3462244.3479956"
    },
    "tran_dyngeonet_2021": {
        "title": "DynGeoNet: Fusion Network for Micro-expression Spotting",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICMI '21: International Conference on Multimodal Interaction, Montr\u00e9al, QC, Canada, October 18-22, 2021",
        "url": "https://doi.org/10.1145/3462244.3479958",
        "doi": "10.1145/3462244.3479958"
    },
    "li_deep_2021": {
        "title": "Deep Learning based Micro-expression Recognition: A Survey",
        "year": "2021",
        "type": "article",
        "venue": "CoRR",
        "url": "https://arxiv.org/abs/2107.02823",
        "doi": null
    },
    "zhang_short_2021": {
        "title": "Short and Long Range Relation Based Spatio-Temporal Transformer for Micro-Expression Recognition",
        "year": "2021",
        "type": "article",
        "venue": "CoRR",
        "url": "https://arxiv.org/abs/2112.05851",
        "doi": null
    },
    "xia_revealing_2020": {
        "title": "Revealing the Invisible With Model and Data Shrinking for Composite-Database Micro-Expression Recognition",
        "year": "2020",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2020.3018222",
        "doi": "10.1109/TIP.2020.3018222"
    },
    "xia_spatiotemporal_2020": {
        "title": "Spatiotemporal Recurrent Convolutional Networks for Recognizing Spontaneous Micro-Expressions",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2019.2931351",
        "doi": "10.1109/TMM.2019.2931351"
    },
    "xia_corrections_2020": {
        "title": "Corrections to \"Spatiotemporal Recurrent Convolutional Networks for Recognizing Spontaneous Micro-Expressions\"",
        "year": "2020",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2020.2980722",
        "doi": "10.1109/TMM.2020.2980722"
    },
    "xia_revealing_2020-1": {
        "title": "Revealing the Invisible with Model and Data Shrinking for Composite-database Micro-expression Recognition",
        "year": "2020",
        "type": "article",
        "venue": "CoRR",
        "url": "https://arxiv.org/abs/2006.09674",
        "doi": null
    },
    "tran_micro-expression_2020": {
        "title": "Micro-expression spotting: A new benchmark",
        "year": "2020",
        "type": "article",
        "venue": "CoRR",
        "url": "https://arxiv.org/abs/2007.12421",
        "doi": null
    },
    "huang_discriminative_2019-1": {
        "title": "Discriminative Spatiotemporal Local Binary Pattern with Revisited Integral Projection for Spontaneous Facial Micro-Expression Recognition",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2017.2713359",
        "doi": "10.1109/TAFFC.2017.2713359"
    },
    "tran_dense_2019": {
        "title": "Dense prediction for micro-expression spotting based on deep sequence model",
        "year": "2019",
        "type": "inproceedings",
        "venue": "Imaging and Multimedia Analytics in a Web and Mobile World 2019, Burlingame, CA, USA, January 13-17, 2019",
        "url": "https://doi.org/10.2352/ISSN.2470-1173.2019.8.IMAWM-401",
        "doi": "10.2352/ISSN.2470-1173.2019.8.IMAWM-401"
    },
    "zong_cross-database_2019": {
        "title": "Cross-Database Micro-Expression Recognition: A Benchmark",
        "year": "2019",
        "type": "inproceedings",
        "venue": "Proceedings of the 2019 on International Conference on Multimedia Retrieval, ICMR 2019, Ottawa, ON, Canada, June 10-13, 2019",
        "url": "https://doi.org/10.1145/3323873.3326590",
        "doi": "10.1145/3323873.3326590"
    },
    "li_micro-expression_2019": {
        "title": "Micro-expression Recognition Under Low-resolution Cases",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IJCV",
        "url": "https://doi.org/10.5220/0007373604270434",
        "doi": "10.5220/0007373604270434"
    },
    "xia_spatiotemporal_2019": {
        "title": "Spatiotemporal Recurrent Convolutional Networks for Recognizing Spontaneous Micro-expressions",
        "year": "2019",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1901.04656",
        "doi": null
    },
    "li_towards_2018": {
        "title": "Towards Reading Hidden Emotions: A Comparative Study of Spontaneous Micro-Expression Spotting and Recognition Methods",
        "year": "2018",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2017.2667642",
        "doi": "10.1109/TAFFC.2017.2667642"
    },
    "zong_domain_2018": {
        "title": "Domain Regeneration for Cross-Database Micro-Expression Recognition",
        "year": "2018",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2018.2797479",
        "doi": "10.1109/TIP.2018.2797479"
    },
    "zong_learning_2018": {
        "title": "Learning From Hierarchical Spatiotemporal Descriptors for Micro-Expression Recognition",
        "year": "2018",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2018.2820321",
        "doi": "10.1109/TMM.2018.2820321"
    },
    "li_can_2018": {
        "title": "Can Micro-Expression be Recognized Based on Single Apex Frame?",
        "year": "2018",
        "type": "inproceedings",
        "venue": "ICIP",
        "url": "https://doi.org/10.1109/ICIP.2018.8451376",
        "doi": "10.1109/ICIP.2018.8451376"
    },
    "xia_spontaneous_2018": {
        "title": "Spontaneous Facial Micro-expression Recognition via Deep Convolutional Network",
        "year": "2018",
        "type": "inproceedings",
        "venue": "Eighth International Conference on Image Processing Theory, Tools and Applications, IPTA 2018, Xi'an, China, November 7-10, 2018",
        "url": "https://doi.org/10.1109/IPTA.2018.8608119",
        "doi": "10.1109/IPTA.2018.8608119"
    },
    "zong_cross-database_2018": {
        "title": "Cross-Database Micro-Expression Recognition: A Benchmark",
        "year": "2018",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1812.07742",
        "doi": null
    },
    "tran_sliding_2017": {
        "title": "Sliding Window Based Micro-expression Spotting: A Benchmark",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Advanced Concepts for Intelligent Vision Systems - 18th International Conference, ACIVS 2017, Antwerp, Belgium, September 18-21, 2017, Proceedings",
        "url": "https://doi.org/10.1007/978-3-319-70353-4\\_46",
        "doi": "10.1007/978-3-319-70353-4_46"
    },
    "zong_learning_2017": {
        "title": "Learning a Target Sample Re-Generator for Cross-Database Micro-Expression Recognition",
        "year": "2017",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3123266.3123367",
        "doi": "10.1145/3123266.3123367"
    },
    "zong_learning_2017-1": {
        "title": "Learning a Target Sample Re-Generator for Cross-Database Micro-Expression Recognition",
        "year": "2017",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1707.08645",
        "doi": null
    },
    "hong_micro-expression_2017": {
        "title": "Micro-Expression Spotting: A Benchmark",
        "year": "2017",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1710.02820",
        "doi": null
    },
    "xia_spontaneous_2016": {
        "title": "Spontaneous micro-expression spotting via geometric deformation modeling",
        "year": "2016",
        "type": "article",
        "venue": "Comput. Vis. Image Underst.",
        "url": "https://doi.org/10.1016/j.cviu.2015.12.006",
        "doi": "10.1016/J.CVIU.2015.12.006"
    },
    "huang_spontaneous_2016": {
        "title": "Spontaneous facial micro-expression analysis using Spatiotemporal Completed Local Quantized Patterns",
        "year": "2016",
        "type": "article",
        "venue": "Neurocomputing",
        "url": "https://doi.org/10.1016/j.neucom.2015.10.096",
        "doi": "10.1016/J.NEUCOM.2015.10.096"
    },
    "wang_sparse_2016": {
        "title": "Sparse tensor canonical correlation analysis for micro-expression recognition",
        "year": "2016",
        "type": "article",
        "venue": "Neurocomputing",
        "url": "https://doi.org/10.1016/j.neucom.2016.05.083",
        "doi": "10.1016/J.NEUCOM.2016.05.083"
    },
    "liu_main_2016": {
        "title": "A Main Directional Mean Optical Flow Feature for Spontaneous Micro-Expression Recognition",
        "year": "2016",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2015.2485205",
        "doi": "10.1109/TAFFC.2015.2485205"
    },
    "patel_selective_2016": {
        "title": "Selective deep features for micro-expression recognition",
        "year": "2016",
        "type": "inproceedings",
        "venue": "ICPR",
        "url": "https://doi.org/10.1109/ICPR.2016.7899972",
        "doi": "10.1109/ICPR.2016.7899972"
    },
    "huang_spontaneous_2016-1": {
        "title": "Spontaneous Facial Micro-Expression Recognition using Discriminative Spatiotemporal Local Binary Pattern with an Improved Integral Projection",
        "year": "2016",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1608.02255",
        "doi": null
    },
    "wang_micro-expression_2015": {
        "title": "Micro-Expression Recognition Using Color Spaces",
        "year": "2015",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2015.2496314",
        "doi": "10.1109/TIP.2015.2496314"
    },
    "patel_spatiotemporal_2015": {
        "title": "Spatiotemporal Integration of Optical Flow Vectors for Micro-expression Detection",
        "year": "2015",
        "type": "inproceedings",
        "venue": "Advanced Concepts for Intelligent Vision Systems - 16th International Conference, ACIVS 2015, Catania, Italy, October 26-29, 2015, Proceedings",
        "url": "https://doi.org/10.1007/978-3-319-25903-1\\_32",
        "doi": "10.1007/978-3-319-25903-1_32"
    },
    "huang_facial_2015": {
        "title": "Facial Micro-Expression Recognition Using Spatiotemporal Local Binary Pattern with Integral Projection",
        "year": "2015",
        "type": "inproceedings",
        "venue": "ICCVW",
        "url": "https://doi.org/10.1109/ICCVW.2015.10",
        "doi": "10.1109/ICCVW.2015.10"
    },
    "li_reading_2015": {
        "title": "Reading Hidden Emotions: Spontaneous Micro-expression Spotting and Recognition",
        "year": "2015",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1511.00423",
        "doi": null
    },
    "wang_micro-expression_2014": {
        "title": "Micro-Expression Recognition Using Robust Principal Component Analysis and Local Spatiotemporal Directional Features",
        "year": "2014",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-319-16178-5\\_23",
        "doi": "10.1007/978-3-319-16178-5_23"
    },
    "yan_quantifying_2014": {
        "title": "Quantifying Micro-expressions with Constraint Local Model and Local Binary Pattern",
        "year": "2014",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-319-16178-5\\_20",
        "doi": "10.1007/978-3-319-16178-5_20"
    },
    "wang_micro-expression_2014-1": {
        "title": "Micro-expression Recognition Using Dynamic Textures on Tensor Independent Color Space",
        "year": "2014",
        "type": "inproceedings",
        "venue": "ICPR",
        "url": "https://doi.org/10.1109/ICPR.2014.800",
        "doi": "10.1109/ICPR.2014.800"
    },
    "li_spontaneous_2013": {
        "title": "A Spontaneous Micro-expression Database: Inducement, collection and baseline",
        "year": "2013",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2013.6553717",
        "doi": "10.1109/FG.2013.6553717"
    },
    "pfister_recognising_2011": {
        "title": "Recognising spontaneous facial micro-expressions",
        "year": "2011",
        "type": "inproceedings",
        "venue": "ICCV",
        "url": "https://doi.org/10.1109/ICCV.2011.6126401",
        "doi": "10.1109/ICCV.2011.6126401"
    },
    "zhou_dual-atme_2023": {
        "title": "Dual-ATME: Dual-Branch Attention Network for Micro-Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "Entropy",
        "url": "https://doi.org/10.3390/e25030460",
        "doi": "10.3390/E25030460"
    },
    "li_casmembox3_2023-1": {
        "title": "CAS(ME)\\textbackslash(\\textasciicircum\\textbackslashmbox3\\textbackslash): A Third Generation Facial Spontaneous Micro-Expression Database With Depth Information and High Ecological Validity",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2022.3174895",
        "doi": "10.1109/TPAMI.2022.3174895"
    },
    "yang_simple_2023": {
        "title": "Simple but Effective In-the-wild Micro-Expression Spotting Based on Head Pose Segmentation",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM FME",
        "url": "https://doi.org/10.1145/3607829.3616445",
        "doi": "10.1145/3607829.3616445"
    },
    "davison_fme_2023-1": {
        "title": "FME '23: 3rd Facial Micro-Expression Workshop",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3610948",
        "doi": "10.1145/3581783.3610948"
    },
    "davison_proceedings_2023-1": {
        "title": "Proceedings of the 3rd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis, FME 2023, Ottawa, ON, Canada, 29 October 2023",
        "year": "2023",
        "type": "book",
        "venue": "ACM",
        "url": "https://doi.org/10.1145/3607829",
        "doi": "10.1145/3607829"
    },
    "ben_video-based_2022-2": {
        "title": "Video-Based Facial Micro-Expression Analysis: A Survey of Datasets, Features and Algorithms",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2021.3067464",
        "doi": "10.1109/TPAMI.2021.3067464"
    },
    "lu_more_2022-1": {
        "title": "A More Objective Quantification of Micro-Expression Intensity through Facial Electromyography",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM FME",
        "url": "https://doi.org/10.1145/3552465.3555038",
        "doi": "10.1145/3552465.3555038"
    },
    "li_fme_2022-1": {
        "title": "FME '22: 2nd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3554777",
        "doi": "10.1145/3503161.3554777"
    },
    "li_megc2022_2022": {
        "title": "MEGC2022: ACM Multimedia 2022 Micro-Expression Grand Challenge",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3551601",
        "doi": "10.1145/3503161.3551601"
    },
    "yap_3d-cnn_2022-1": {
        "title": "3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video Sequences using Temporal Oriented Reference Frame",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3551570",
        "doi": "10.1145/3503161.3551570"
    },
    "li_proceedings_2022-1": {
        "title": "Proceedings of the 2nd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis, FME 2022, Lisboa, Portugal, 14 October 2022",
        "year": "2022",
        "type": "book",
        "venue": "ACM",
        "url": "https://doi.org/10.1145/3552465",
        "doi": "10.1145/3552465"
    },
    "ben_video-based_2022-3": {
        "title": "Video-based Facial Micro-Expression Analysis: A Survey of Datasets, Features and Algorithms",
        "year": "2022",
        "type": "article",
        "venue": "CoRR",
        "url": "https://arxiv.org/abs/2201.12728",
        "doi": null
    },
    "wang_mesnet_2021": {
        "title": "MESNet: A Convolutional Neural Network for Spotting Multi-Scale Micro-Expression Intervals in Long Videos",
        "year": "2021",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2021.3064258",
        "doi": "10.1109/TIP.2021.3064258"
    },
    "dong_brief_2021": {
        "title": "A Brief Guide: Code for Spontaneous Expressions and Micro-Expressions in Videos",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM FME",
        "url": "https://doi.org/10.1145/3476100.3484464",
        "doi": "10.1145/3476100.3484464"
    },
    "li_fme21_2021-1": {
        "title": "FME'21: 1st Workshop on Facial Micro-Expression: Advanced Techniques for Facial Expressions Generation and Spotting",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3474085.3478579",
        "doi": "10.1145/3474085.3478579"
    },
    "he_spotting_2020": {
        "title": "Spotting Macro-and Micro-expression Intervals in Long Video Sequences",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00036",
        "doi": "10.1109/FG47880.2020.00036"
    },
    "li_megc2020_2020-1": {
        "title": "MEGC2020 - The Third Facial Micro-Expression Grand Challenge",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00035",
        "doi": "10.1109/FG47880.2020.00035"
    },
    "zhang_spatio-temporal_2020": {
        "title": "Spatio-temporal fusion for Macro- and Micro-expression Spotting in Long Video Sequences",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00037",
        "doi": "10.1109/FG47880.2020.00037"
    },
    "huang_discriminative_2019-2": {
        "title": "Discriminative Spatiotemporal Local Binary Pattern with Revisited Integral Projection for Spontaneous Facial Micro-Expression Recognition",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2017.2713359",
        "doi": "10.1109/TAFFC.2017.2713359"
    },
    "li_spotting_2019": {
        "title": "Spotting Micro-Expressions on Long Videos Sequences",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756626",
        "doi": "10.1109/FG.2019.8756626"
    },
    "see_megc_2019-1": {
        "title": "MEGC 2019 - The Second Facial Micro-Expressions Grand Challenge",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756611",
        "doi": "10.1109/FG.2019.8756611"
    },
    "he_spotting_2019": {
        "title": "Spotting Macro- and Micro-expression Intervals in Long Video Sequences",
        "year": "2019",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1912.11985",
        "doi": null
    },
    "wang_micro-expression_2018": {
        "title": "Micro-expression recognition with small sample size by transferring long-term convolutional neural network",
        "year": "2018",
        "type": "article",
        "venue": "Neurocomputing",
        "url": "https://doi.org/10.1016/j.neucom.2018.05.107",
        "doi": "10.1016/J.NEUCOM.2018.05.107"
    },
    "qu_casmembox2_2018": {
        "title": "CAS(ME)\\textbackslash(\\textasciicircum\\textbackslashmbox2\\textbackslash): A Database for Spontaneous Macro-Expression and Micro-Expression Spotting and Recognition",
        "year": "2018",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2017.2654440",
        "doi": "10.1109/TAFFC.2017.2654440"
    },
    "yap_facial_2018": {
        "title": "Facial Micro-Expressions Grand Challenge 2018 Summary",
        "year": "2018",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2018.00106",
        "doi": "10.1109/FG.2018.00106"
    },
    "li_spotting_2018": {
        "title": "Spotting Micro-Expressions on Long Videos Sequences",
        "year": "2018",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1812.10306",
        "doi": null
    },
    "wang_sparse_2016-1": {
        "title": "Sparse tensor canonical correlation analysis for micro-expression recognition",
        "year": "2016",
        "type": "article",
        "venue": "Neurocomputing",
        "url": "https://doi.org/10.1016/j.neucom.2016.05.083",
        "doi": "10.1016/J.NEUCOM.2016.05.083"
    },
    "liu_main_2016-1": {
        "title": "A Main Directional Mean Optical Flow Feature for Spontaneous Micro-Expression Recognition",
        "year": "2016",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2015.2485205",
        "doi": "10.1109/TAFFC.2015.2485205"
    },
    "wang_main_2016": {
        "title": "A Main Directional Maximal Difference Analysis for Spotting Micro-expressions",
        "year": "2016",
        "type": "inproceedings",
        "venue": "ACCV",
        "url": "https://doi.org/10.1007/978-3-319-54427-4\\_33",
        "doi": "10.1007/978-3-319-54427-4_33"
    },
    "qu_casme2_2016": {
        "title": "CAS(ME)2: A Database of Spontaneous Macro-expressions and Micro-expressions",
        "year": "2016",
        "type": "inproceedings",
        "venue": "HCI",
        "url": "https://doi.org/10.1007/978-3-319-39513-5\\_5",
        "doi": "10.1007/978-3-319-39513-5_5"
    },
    "huang_spontaneous_2016-2": {
        "title": "Spontaneous Facial Micro-Expression Recognition using Discriminative Spatiotemporal Local Binary Pattern with an Improved Integral Projection",
        "year": "2016",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1608.02255",
        "doi": null
    },
    "wang_micro-expression_2015-1": {
        "title": "Micro-Expression Recognition Using Color Spaces",
        "year": "2015",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2015.2496314",
        "doi": "10.1109/TIP.2015.2496314"
    },
    "huang_facial_2015-1": {
        "title": "Facial Micro-Expression Recognition Using Spatiotemporal Local Binary Pattern with Integral Projection",
        "year": "2015",
        "type": "inproceedings",
        "venue": "ICCVW",
        "url": "https://doi.org/10.1109/ICCVW.2015.10",
        "doi": "10.1109/ICCVW.2015.10"
    },
    "yan_for_2014": {
        "title": "For micro-expression recognition: Database and suggestions",
        "year": "2014",
        "type": "article",
        "venue": "Neurocomputing",
        "url": "https://doi.org/10.1016/j.neucom.2014.01.029",
        "doi": "10.1016/J.NEUCOM.2014.01.029"
    },
    "wang_face_2014": {
        "title": "Face Recognition and Micro-expression Recognition Based on Discriminant Tensor Subspace Analysis Plus Extreme Learning Machine",
        "year": "2014",
        "type": "article",
        "venue": "Neural Process. Lett.",
        "url": "https://doi.org/10.1007/s11063-013-9288-7",
        "doi": "10.1007/S11063-013-9288-7"
    },
    "wang_micro-expression_2014-2": {
        "title": "Micro-Expression Recognition Using Robust Principal Component Analysis and Local Spatiotemporal Directional Features",
        "year": "2014",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-319-16178-5\\_23",
        "doi": "10.1007/978-3-319-16178-5_23"
    },
    "yan_quantifying_2014-1": {
        "title": "Quantifying Micro-expressions with Constraint Local Model and Local Binary Pattern",
        "year": "2014",
        "type": "inproceedings",
        "venue": "ECCV",
        "url": "https://doi.org/10.1007/978-3-319-16178-5\\_20",
        "doi": "10.1007/978-3-319-16178-5_20"
    },
    "wang_micro-expression_2014-3": {
        "title": "Micro-expression Recognition Using Dynamic Textures on Tensor Independent Color Space",
        "year": "2014",
        "type": "inproceedings",
        "venue": "ICPR",
        "url": "https://doi.org/10.1109/ICPR.2014.800",
        "doi": "10.1109/ICPR.2014.800"
    },
    "yan_casme_2013": {
        "title": "CASME database: A dataset of spontaneous micro-expressions collected from neutralized faces",
        "year": "2013",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2013.6553799",
        "doi": "10.1109/FG.2013.6553799"
    },
    "liong_sfamnet_2024": {
        "title": "SFAMNet: A scene flow attention-based micro-expression network",
        "year": "2024",
        "type": "article",
        "venue": "Neurocomputing",
        "url": "https://doi.org/10.1016/j.neucom.2023.126998",
        "doi": "10.1016/J.NEUCOM.2023.126998"
    },
    "liong_spot-then-recognize_2023": {
        "title": "Spot-then-Recognize: A Micro-Expression Analysis Network for Seamless Evaluation of Long Videos",
        "year": "2023",
        "type": "article",
        "venue": "Signal Process. Image Commun.",
        "url": "https://doi.org/10.1016/j.image.2022.116875",
        "doi": "10.1016/J.IMAGE.2022.116875"
    },
    "davison_fme_2023-2": {
        "title": "FME '23: 3rd Facial Micro-Expression Workshop",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3610948",
        "doi": "10.1145/3581783.3610948"
    },
    "davison_proceedings_2023-2": {
        "title": "Proceedings of the 3rd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis, FME 2023, Ottawa, ON, Canada, 29 October 2023",
        "year": "2023",
        "type": "book",
        "venue": "ACM",
        "url": "https://doi.org/10.1145/3607829",
        "doi": "10.1145/3607829"
    },
    "gan_needle_2022": {
        "title": "Needle in a Haystack: Spotting and recognising micro-expressions \"in the wild\"",
        "year": "2022",
        "type": "article",
        "venue": "Neurocomputing",
        "url": "https://doi.org/10.1016/j.neucom.2022.06.101",
        "doi": "10.1016/J.NEUCOM.2022.06.101"
    },
    "liong_mtsn_2022-1": {
        "title": "MTSN: A Multi-Temporal Stream Network for Spotting Facial Macro- and Micro-Expression with Hard and Soft Pseudo-labels",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM FME",
        "url": "https://doi.org/10.1145/3552465.3555040",
        "doi": "10.1145/3552465.3555040"
    },
    "li_fme_2022-2": {
        "title": "FME '22: 2nd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3554777",
        "doi": "10.1145/3503161.3554777"
    },
    "li_megc2022_2022-1": {
        "title": "MEGC2022: ACM Multimedia 2022 Micro-Expression Grand Challenge",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3551601",
        "doi": "10.1145/3503161.3551601"
    },
    "li_proceedings_2022-2": {
        "title": "Proceedings of the 2nd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis, FME 2022, Lisboa, Portugal, 14 October 2022",
        "year": "2022",
        "type": "book",
        "venue": "ACM",
        "url": "https://doi.org/10.1145/3552465",
        "doi": "10.1145/3552465"
    },
    "liong_shallow_2021": {
        "title": "Shallow Optical Flow Three-Stream CNN For Macro- And Micro-Expression Spotting From Long Videos",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ICIP",
        "url": "https://doi.org/10.1109/ICIP42928.2021.9506349",
        "doi": "10.1109/ICIP42928.2021.9506349"
    },
    "li_fme21_2021-2": {
        "title": "FME'21: 1st Workshop on Facial Micro-Expression: Advanced Techniques for Facial Expressions Generation and Spotting",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3474085.3478579",
        "doi": "10.1145/3474085.3478579"
    },
    "liong_shallow_2021-1": {
        "title": "Shallow Optical Flow Three-Stream CNN for Macro- and Micro-Expression Spotting from Long Videos",
        "year": "2021",
        "type": "article",
        "venue": "CoRR",
        "url": "https://arxiv.org/abs/2106.06489",
        "doi": null
    },
    "li_megc2020_2020-2": {
        "title": "MEGC2020 - The Third Facial Micro-Expression Grand Challenge",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00035",
        "doi": "10.1109/FG47880.2020.00035"
    },
    "li_micro-expression_2019-1": {
        "title": "Micro-expression recognition based on 3D flow convolutional neural network",
        "year": "2019",
        "type": "article",
        "venue": "Pattern Anal. Appl.",
        "url": "https://doi.org/10.1007/s10044-018-0757-5",
        "doi": "10.1007/S10044-018-0757-5"
    },
    "liong_shallow_2019": {
        "title": "Shallow Triple Stream Three-dimensional CNN (STSTNet) for Micro-expression Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756567",
        "doi": "10.1109/FG.2019.8756567"
    },
    "see_megc_2019-2": {
        "title": "MEGC 2019 - The Second Facial Micro-Expressions Grand Challenge",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756611",
        "doi": "10.1109/FG.2019.8756611"
    },
    "khor_dual-stream_2019": {
        "title": "Dual-stream Shallow Networks for Facial Micro-expression Recognition",
        "year": "2019",
        "type": "inproceedings",
        "venue": "ICIP",
        "url": "https://doi.org/10.1109/ICIP.2019.8802965",
        "doi": "10.1109/ICIP.2019.8802965"
    },
    "liong_shallow_2019-1": {
        "title": "A Shallow Triple Stream Three-dimensional CNN (STSTNet) for Micro-expression Recognition System",
        "year": "2019",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1902.03634",
        "doi": null
    },
    "liong_less_2018": {
        "title": "Less is more: Micro-expression recognition from video using apex frame",
        "year": "2018",
        "type": "article",
        "venue": "Signal Process. Image Commun.",
        "url": "https://doi.org/10.1016/j.image.2017.11.006",
        "doi": "10.1016/J.IMAGE.2017.11.006"
    },
    "liong_hybrid_2018": {
        "title": "Hybrid Facial Regions Extraction for Micro-expression Recognition System",
        "year": "2018",
        "type": "article",
        "venue": "J. Signal Process. Syst.",
        "url": "https://doi.org/10.1007/s11265-017-1276-0",
        "doi": "10.1007/S11265-017-1276-0"
    },
    "khor_enriched_2018": {
        "title": "Enriched Long-Term Recurrent Convolutional Network for Facial Micro-Expression Recognition",
        "year": "2018",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2018.00105",
        "doi": "10.1109/FG.2018.00105"
    },
    "ngo_micro-expression_2018": {
        "title": "Micro-Expression Motion Magnification: Global Lagrangian vs. Local Eulerian Approaches",
        "year": "2018",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2018.00102",
        "doi": "10.1109/FG.2018.00102"
    },
    "yap_facial_2018-1": {
        "title": "Facial Micro-Expressions Grand Challenge 2018 Summary",
        "year": "2018",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2018.00106",
        "doi": "10.1109/FG.2018.00106"
    },
    "khor_enriched_2018-1": {
        "title": "Enriched Long-term Recurrent Convolutional Network for Facial Micro-Expression Recognition",
        "year": "2018",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1805.08417",
        "doi": null
    },
    "oh_survey_2018-1": {
        "title": "A Survey of Automatic Facial Micro-expression Analysis: Databases, Methods and Challenges",
        "year": "2018",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1806.05781",
        "doi": null
    },
    "wang_effective_2017": {
        "title": "Effective recognition of facial micro-expressions with video motion magnification",
        "year": "2017",
        "type": "article",
        "venue": "Multim. Tools Appl.",
        "url": "https://doi.org/10.1007/s11042-016-4079-6",
        "doi": "10.1007/S11042-016-4079-6"
    },
    "liong_automatic_2016": {
        "title": "Automatic Micro-expression Recognition from Long Video Using a Single Spotted Apex",
        "year": "2016",
        "type": "inproceedings",
        "venue": "ACCV",
        "url": "https://doi.org/10.1007/978-3-319-54427-4\\_26",
        "doi": "10.1007/978-3-319-54427-4_26"
    },
    "oh_intrinsic_2016": {
        "title": "Intrinsic two-dimensional local structures for micro-expression recognition",
        "year": "2016",
        "type": "inproceedings",
        "venue": "ICASSP",
        "url": "https://doi.org/10.1109/ICASSP.2016.7471997",
        "doi": "10.1109/ICASSP.2016.7471997"
    },
    "liong_less_2016": {
        "title": "Less is More: Micro-expression Recognition from Video using Apex Frame",
        "year": "2016",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1606.01721",
        "doi": null
    },
    "liong_automatic_2015": {
        "title": "Automatic apex frame spotting in micro-expression database",
        "year": "2015",
        "type": "inproceedings",
        "venue": "3rd IAPR Asian Conference on Pattern Recognition, ACPR 2015, Kuala Lumpur, Malaysia, November 3-6, 2015",
        "url": "https://doi.org/10.1109/ACPR.2015.7486586",
        "doi": "10.1109/ACPR.2015.7486586"
    },
    "oh_monogenic_2015": {
        "title": "Monogenic Riesz wavelet representation for micro-expression recognition",
        "year": "2015",
        "type": "inproceedings",
        "venue": "2015 IEEE International Conference on Digital Signal Processing, DSP 2015, Singapore, July 21-24, 2015",
        "url": "https://doi.org/10.1109/ICDSP.2015.7252078",
        "doi": "10.1109/ICDSP.2015.7252078"
    },
    "wang_lbp_2014": {
        "title": "LBP with Six Intersection Points: Reducing Redundant Information in LBP-TOP for Micro-expression Recognition",
        "year": "2014",
        "type": "inproceedings",
        "venue": "ACCV",
        "url": "https://doi.org/10.1007/978-3-319-16865-4\\_34",
        "doi": "10.1007/978-3-319-16865-4_34"
    },
    "zhou_dual-atme_2023-1": {
        "title": "Dual-ATME: Dual-Branch Attention Network for Micro-Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "Entropy",
        "url": "https://doi.org/10.3390/e25030460",
        "doi": "10.3390/E25030460"
    },
    "li_casmembox3_2023-2": {
        "title": "CAS(ME)\\textbackslash(\\textasciicircum\\textbackslashmbox3\\textbackslash): A Third Generation Facial Spontaneous Micro-Expression Database With Depth Information and High Ecological Validity",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2022.3174895",
        "doi": "10.1109/TPAMI.2022.3174895"
    },
    "li_local_2023": {
        "title": "Local Temporal Pattern and Data Augmentation for Spotting Micro-Expressions",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2020.3023821",
        "doi": "10.1109/TAFFC.2020.3023821"
    },
    "yang_simple_2023-1": {
        "title": "Simple but Effective In-the-wild Micro-Expression Spotting Based on Head Pose Segmentation",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM FME",
        "url": "https://doi.org/10.1145/3607829.3616445",
        "doi": "10.1145/3607829.3616445"
    },
    "davison_fme_2023-3": {
        "title": "FME '23: 3rd Facial Micro-Expression Workshop",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3610948",
        "doi": "10.1145/3581783.3610948"
    },
    "davison_proceedings_2023-3": {
        "title": "Proceedings of the 3rd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis, FME 2023, Ottawa, ON, Canada, 29 October 2023",
        "year": "2023",
        "type": "book",
        "venue": "ACM",
        "url": "https://doi.org/10.1145/3607829",
        "doi": "10.1145/3607829"
    },
    "lu_more_2022-2": {
        "title": "A More Objective Quantification of Micro-Expression Intensity through Facial Electromyography",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM FME",
        "url": "https://doi.org/10.1145/3552465.3555038",
        "doi": "10.1145/3552465.3555038"
    },
    "li_fme_2022-3": {
        "title": "FME '22: 2nd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3554777",
        "doi": "10.1145/3503161.3554777"
    },
    "li_megc2022_2022-2": {
        "title": "MEGC2022: ACM Multimedia 2022 Micro-Expression Grand Challenge",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3551601",
        "doi": "10.1145/3503161.3551601"
    },
    "yap_3d-cnn_2022-2": {
        "title": "3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video Sequences using Temporal Oriented Reference Frame",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3503161.3551570",
        "doi": "10.1145/3503161.3551570"
    },
    "li_proceedings_2022-3": {
        "title": "Proceedings of the 2nd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis, FME 2022, Lisboa, Portugal, 14 October 2022",
        "year": "2022",
        "type": "book",
        "venue": "ACM",
        "url": "https://doi.org/10.1145/3552465",
        "doi": "10.1145/3552465"
    },
    "wang_mesnet_2021-1": {
        "title": "MESNet: A Convolutional Neural Network for Spotting Multi-Scale Micro-Expression Intervals in Long Videos",
        "year": "2021",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2021.3064258",
        "doi": "10.1109/TIP.2021.3064258"
    },
    "li_fme21_2021-3": {
        "title": "FME'21: 1st Workshop on Facial Micro-Expression: Advanced Techniques for Facial Expressions Generation and Spotting",
        "year": "2021",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3474085.3478579",
        "doi": "10.1145/3474085.3478579"
    },
    "cheng_fme21_2021-1": {
        "title": "FME'21: Proceedings of the 1st Workshop on Facial Micro-Expression: Advanced Techniques for Facial Expressions Generation and Spotting, Virtual Event, China, 24 October 2021",
        "year": "2021",
        "type": "book",
        "venue": "ACM",
        "url": "https://doi.org/10.1145/3476100",
        "doi": "10.1145/3476100"
    },
    "he_spotting_2020-1": {
        "title": "Spotting Macro-and Micro-expression Intervals in Long Video Sequences",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00036",
        "doi": "10.1109/FG47880.2020.00036"
    },
    "li_megc2020_2020-3": {
        "title": "MEGC2020 - The Third Facial Micro-Expression Grand Challenge",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00035",
        "doi": "10.1109/FG47880.2020.00035"
    },
    "zhang_spatio-temporal_2020-1": {
        "title": "Spatio-temporal fusion for Macro- and Micro-expression Spotting in Long Video Sequences",
        "year": "2020",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG47880.2020.00037",
        "doi": "10.1109/FG47880.2020.00037"
    },
    "li_spotting_2019-1": {
        "title": "Spotting Micro-Expressions on Long Videos Sequences",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756626",
        "doi": "10.1109/FG.2019.8756626"
    },
    "see_megc_2019-3": {
        "title": "MEGC 2019 - The Second Facial Micro-Expressions Grand Challenge",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756611",
        "doi": "10.1109/FG.2019.8756611"
    },
    "li_survey_2019-1": {
        "title": "A Survey on Databases for Facial Micro-Expression Analysis",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IJCV",
        "url": "https://doi.org/10.5220/0007309202410248",
        "doi": "10.5220/0007309202410248"
    },
    "he_spotting_2019-1": {
        "title": "Spotting Macro- and Micro-expression Intervals in Long Video Sequences",
        "year": "2019",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1912.11985",
        "doi": null
    },
    "li_ltp-ml_2018": {
        "title": "LTP-ML: Micro-Expression Detection by Recognition of Local Temporal Pattern of Facial Movements",
        "year": "2018",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2018.00100",
        "doi": "10.1109/FG.2018.00100"
    },
    "weber_survey_2018-2": {
        "title": "A Survey on Databases of Facial Macro-expression and Micro-expression",
        "year": "2018",
        "type": "inproceedings",
        "venue": "Computer Vision, Imaging and Computer Graphics Theory and Applications - 13th International Joint Conference, VISIGRAPP 2018, Funchal, Madeira, Portugal, January 27-29, 2018, Revised Selected Papers",
        "url": "https://doi.org/10.1007/978-3-030-26756-8\\_15",
        "doi": "10.1007/978-3-030-26756-8_15"
    },
    "li_spotting_2018-1": {
        "title": "Spotting Micro-Expressions on Long Videos Sequences",
        "year": "2018",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1812.10306",
        "doi": null
    },
    "wei_cmnet_2023": {
        "title": "CMNet: Contrastive Magnification Network for Micro-Expression Recognition",
        "year": "2023",
        "type": "inproceedings",
        "venue": "AAAI",
        "url": "https://doi.org/10.1609/aaai.v37i1.25083",
        "doi": "10.1609/AAAI.V37I1.25083"
    },
    "zhu_learning_2023": {
        "title": "Learning to Rank Onset-Occurring-Offset Representations for Micro-Expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "CoRR",
        "url": "https://doi.org/10.48550/arXiv.2310.04664",
        "doi": "10.48550/ARXIV.2310.04664"
    },
    "zhou_empirical_2023-1": {
        "title": "An Empirical Study of Super-resolution on Low-resolution Micro-expression Recognition",
        "year": "2023",
        "type": "article",
        "venue": "CoRR",
        "url": "https://doi.org/10.48550/arXiv.2310.10022",
        "doi": "10.48550/ARXIV.2310.10022"
    },
    "liu_cross-database_2022": {
        "title": "Cross-database micro-expression recognition based on transfer double sparse learning",
        "year": "2022",
        "type": "article",
        "venue": "Multim. Tools Appl.",
        "url": "https://doi.org/10.1007/s11042-022-12878-0",
        "doi": "10.1007/S11042-022-12878-0"
    },
    "mao_objective_2022": {
        "title": "Objective Class-Based Micro-Expression Recognition Under Partial Occlusion Via Region-Inspired Relation Reasoning Network",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3197785",
        "doi": "10.1109/TAFFC.2022.3197785"
    },
    "zhang_cross-database_2022-1": {
        "title": "Cross-Database Micro-Expression Recognition: A Benchmark",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Knowl. Data Eng.",
        "url": "https://doi.org/10.1109/TKDE.2020.2985365",
        "doi": "10.1109/TKDE.2020.2985365"
    },
    "wei_novel_2022": {
        "title": "A Novel Micro-Expression Recognition Approach Using Attention-Based Magnification-Adaptive Networks",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ICASSP",
        "url": "https://doi.org/10.1109/ICASSP43922.2022.9747232",
        "doi": "10.1109/ICASSP43922.2022.9747232"
    },
    "jiang_seeking_2022": {
        "title": "Seeking Salient Facial Regions for Cross-Database Micro-Expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ICPR",
        "url": "https://doi.org/10.1109/ICPR56361.2022.9956540",
        "doi": "10.1109/ICPR56361.2022.9956540"
    },
    "wei_novel_2022-1": {
        "title": "A Novel Magnification-Robust Network with Sparse Self-Attention for Micro-expression Recognition",
        "year": "2022",
        "type": "inproceedings",
        "venue": "ICPR",
        "url": "https://doi.org/10.1109/ICPR56361.2022.9956629",
        "doi": "10.1109/ICPR56361.2022.9956629"
    },
    "mao_region_2021": {
        "title": "Region attention and graph embedding network for occlusion objective class-based micro-expression recognition",
        "year": "2021",
        "type": "article",
        "venue": "CoRR",
        "url": "https://arxiv.org/abs/2107.05904",
        "doi": null
    },
    "jiang_seeking_2021": {
        "title": "Seeking Salient Facial Regions for Cross-Database Micro-Expression Recognition",
        "year": "2021",
        "type": "article",
        "venue": "CoRR",
        "url": "https://arxiv.org/abs/2111.15361",
        "doi": null
    },
    "xia_motion_2020": {
        "title": "Motion Attention Deep Transfer Network for Cross-database Micro-expression Recognition",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ICPR",
        "url": "https://doi.org/10.1007/978-3-030-68796-0\\_49",
        "doi": "10.1007/978-3-030-68796-0_49"
    },
    "song_recognizing_2019": {
        "title": "Recognizing Spontaneous Micro-Expression Using a Three-Stream Convolutional Neural Network",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Access",
        "url": "https://doi.org/10.1109/ACCESS.2019.2960629",
        "doi": "10.1109/ACCESS.2019.2960629"
    },
    "li_three-stream_2019": {
        "title": "Three-Stream Convolutional Neural Network for Micro-Expression Recognition",
        "year": "2019",
        "type": "article",
        "venue": "Aust. J. Intell. Inf. Process. Syst.",
        "url": "http://ajiips.com.au/papers/V15.3/v15n3\\_53-62.pdf",
        "doi": null
    },
    "li_unsupervised_2019": {
        "title": "Unsupervised Cross-Database Micro-Expression Recognition Using Target-Adapted Least-Squares Regression",
        "year": "2019",
        "type": "article",
        "venue": "IEICE Trans. Inf. Syst.",
        "url": "https://doi.org/10.1587/transinf.2018EDL8174",
        "doi": "10.1587/TRANSINF.2018EDL8174"
    },
    "tang_micro-expression_2019": {
        "title": "Micro-Expression Recognition by Leveraging Color Space Information",
        "year": "2019",
        "type": "article",
        "venue": "IEICE Trans. Inf. Syst.",
        "url": "https://doi.org/10.1587/transinf.2018EDL8220",
        "doi": "10.1587/TRANSINF.2018EDL8220"
    },
    "zong_cross-database_2019-1": {
        "title": "Cross-Database Micro-Expression Recognition: A Benchmark",
        "year": "2019",
        "type": "inproceedings",
        "venue": "Proceedings of the 2019 on International Conference on Multimedia Retrieval, ICMR 2019, Ottawa, ON, Canada, June 10-13, 2019",
        "url": "https://doi.org/10.1145/3323873.3326590",
        "doi": "10.1145/3323873.3326590"
    },
    "zong_domain_2018-1": {
        "title": "Domain Regeneration for Cross-Database Micro-Expression Recognition",
        "year": "2018",
        "type": "article",
        "venue": "TIP",
        "url": "https://doi.org/10.1109/TIP.2018.2797479",
        "doi": "10.1109/TIP.2018.2797479"
    },
    "zong_learning_2018-1": {
        "title": "Learning From Hierarchical Spatiotemporal Descriptors for Micro-Expression Recognition",
        "year": "2018",
        "type": "article",
        "venue": "IEEE Trans. Multim.",
        "url": "https://doi.org/10.1109/TMM.2018.2820321",
        "doi": "10.1109/TMM.2018.2820321"
    },
    "zong_cross-database_2018-1": {
        "title": "Cross-Database Micro-Expression Recognition: A Benchmark",
        "year": "2018",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1812.07742",
        "doi": null
    },
    "zong_learning_2017-2": {
        "title": "Learning a Target Sample Re-Generator for Cross-Database Micro-Expression Recognition",
        "year": "2017",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3123266.3123367",
        "doi": "10.1145/3123266.3123367"
    },
    "zong_learning_2017-3": {
        "title": "Learning a Target Sample Re-Generator for Cross-Database Micro-Expression Recognition",
        "year": "2017",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1707.08645",
        "doi": null
    },
    "lu_micro-expression_2016": {
        "title": "Micro-Expression Recognition by Regression Model and Group Sparse Spatio-Temporal Feature Learning",
        "year": "2016",
        "type": "article",
        "venue": "IEICE Trans. Inf. Syst.",
        "url": "https://doi.org/10.1587/transinf.2015EDL8221",
        "doi": "10.1587/TRANSINF.2015EDL8221"
    },
    "huang_spontaneous_2016-3": {
        "title": "Spontaneous facial micro-expression analysis using Spatiotemporal Completed Local Quantized Patterns",
        "year": "2016",
        "type": "article",
        "venue": "Neurocomputing",
        "url": "https://doi.org/10.1016/j.neucom.2015.10.096",
        "doi": "10.1016/J.NEUCOM.2015.10.096"
    },
    "almazrua_comprehensive_2022": {
        "title": "A Comprehensive Survey of Recent Hybrid Feature Selection Methods in Cancer Microarray Gene Expression Data",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Access",
        "url": "https://doi.org/10.1109/ACCESS.2022.3185226",
        "doi": "10.1109/ACCESS.2022.3185226"
    },
    "esmaeili_comprehensive_2022-1": {
        "title": "A comprehensive survey on facial micro-expression: approaches and databases",
        "year": "2022",
        "type": "article",
        "venue": "Multim. Tools Appl.",
        "url": "https://doi.org/10.1007/s11042-022-13133-2",
        "doi": "10.1007/S11042-022-13133-2"
    },
    "ben_video-based_2022-4": {
        "title": "Video-Based Facial Micro-Expression Analysis: A Survey of Datasets, Features and Algorithms",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2021.3067464",
        "doi": "10.1109/TPAMI.2021.3067464"
    },
    "li_deep_2022-3": {
        "title": "Deep Learning for Micro-Expression Recognition: A Survey",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3205170",
        "doi": "10.1109/TAFFC.2022.3205170"
    },
    "dwivedi_challenges_2022-1": {
        "title": "Challenges of Facial Micro-Expression Detection and Recognition: A Survey",
        "year": "2022",
        "type": "inproceedings",
        "venue": "Neural Information Processing - 29th International Conference, ICONIP 2022, Virtual Event, November 22-26, 2022, Proceedings, Part VII",
        "url": "https://doi.org/10.1007/978-981-99-1648-1\\_40",
        "doi": "10.1007/978-981-99-1648-1_40"
    },
    "zhou_survey_2021": {
        "title": "A survey of micro-expression recognition",
        "year": "2021",
        "type": "article",
        "venue": "Image Vis. Comput.",
        "url": "https://doi.org/10.1016/j.imavis.2020.104043",
        "doi": "10.1016/J.IMAVIS.2020.104043"
    },
    "abd-elnaby_classification_2021": {
        "title": "Classification of breast cancer using microarray gene expression data: A survey",
        "year": "2021",
        "type": "article",
        "venue": "J. Biomed. Informatics",
        "url": "https://doi.org/10.1016/j.jbi.2021.103764",
        "doi": "10.1016/J.JBI.2021.103764"
    },
    "li_deep_2021-1": {
        "title": "Deep Learning based Micro-expression Recognition: A Survey",
        "year": "2021",
        "type": "article",
        "venue": "CoRR",
        "url": "https://arxiv.org/abs/2107.02823",
        "doi": null
    },
    "almugren_survey_2019": {
        "title": "A Survey on Hybrid Feature Selection Methods in Microarray Gene Expression Data for Cancer Classification",
        "year": "2019",
        "type": "article",
        "venue": "IEEE Access",
        "url": "https://doi.org/10.1109/ACCESS.2019.2922987",
        "doi": "10.1109/ACCESS.2019.2922987"
    },
    "li_survey_2019-2": {
        "title": "A Survey on Databases for Facial Micro-Expression Analysis",
        "year": "2019",
        "type": "inproceedings",
        "venue": "IJCV",
        "url": "https://doi.org/10.5220/0007309202410248",
        "doi": "10.5220/0007309202410248"
    },
    "takalkar_survey_2018-1": {
        "title": "A survey: facial micro-expression recognition",
        "year": "2018",
        "type": "article",
        "venue": "Multim. Tools Appl.",
        "url": "https://doi.org/10.1007/s11042-017-5317-2",
        "doi": "10.1007/S11042-017-5317-2"
    },
    "weber_survey_2018-3": {
        "title": "A Survey on Databases of Facial Macro-expression and Micro-expression",
        "year": "2018",
        "type": "inproceedings",
        "venue": "Computer Vision, Imaging and Computer Graphics Theory and Applications - 13th International Joint Conference, VISIGRAPP 2018, Funchal, Madeira, Portugal, January 27-29, 2018, Revised Selected Papers",
        "url": "https://doi.org/10.1007/978-3-030-26756-8\\_15",
        "doi": "10.1007/978-3-030-26756-8_15"
    },
    "oh_survey_2018-2": {
        "title": "A Survey of Automatic Facial Micro-expression Analysis: Databases, Methods and Challenges",
        "year": "2018",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1806.05781",
        "doi": null
    },
    "osama_gene_2023": {
        "title": "Gene reduction and machine learning algorithms for cancer classification based on microarray gene expression data: A comprehensive review",
        "year": "2023",
        "type": "article",
        "venue": "Expert Syst. Appl.",
        "url": "https://doi.org/10.1016/j.eswa.2022.118946",
        "doi": "10.1016/J.ESWA.2022.118946"
    },
    "zhang_review_2023": {
        "title": "A review of micro-expression spotting: methods and challenges",
        "year": "2023",
        "type": "article",
        "venue": "Multim. Syst.",
        "url": "https://doi.org/10.1007/s00530-023-01076-z",
        "doi": "10.1007/S00530-023-01076-Z"
    },
    "alhenawi_feature_2022": {
        "title": "Feature selection methods on gene expression microarray data for cancer classification: A systematic review",
        "year": "2022",
        "type": "article",
        "venue": "Comput. Biol. Medicine",
        "url": "https://doi.org/10.1016/j.compbiomed.2021.105051",
        "doi": "10.1016/J.COMPBIOMED.2021.105051"
    },
    "ali_hybrid_2022": {
        "title": "Hybrid Feature Selection of Breast Cancer Gene Expression Microarray Data Based on Metaheuristic Methods: A Comprehensive Review",
        "year": "2022",
        "type": "article",
        "venue": "Symmetry",
        "url": "https://doi.org/10.3390/sym14101955",
        "doi": "10.3390/SYM14101955"
    },
    "zhang_review_2022": {
        "title": "A Review of Micro-expression Recognition based on Deep Learning",
        "year": "2022",
        "type": "inproceedings",
        "venue": "IJCNN",
        "url": "https://doi.org/10.1109/IJCNN55064.2022.9892307",
        "doi": "10.1109/IJCNN55064.2022.9892307"
    },
    "pan_review_2021": {
        "title": "Review of micro-expression spotting and recognition in video sequences",
        "year": "2021",
        "type": "article",
        "venue": "Virtual Real. Intell. Hardw.",
        "url": "https://doi.org/10.1016/j.vrih.2020.10.003",
        "doi": "10.1016/J.VRIH.2020.10.003"
    },
    "goh_micro-expression_2020": {
        "title": "Micro-expression recognition: an updated review of current trends, challenges and solutions",
        "year": "2020",
        "type": "article",
        "venue": "Vis. Comput.",
        "url": "https://doi.org/10.1007/s00371-018-1607-6",
        "doi": "10.1007/S00371-018-1607-6"
    },
    "biswal_review_2018": {
        "title": "A review on biclustering of gene expression microarray data: algorithms, effective measures and validations",
        "year": "2018",
        "type": "article",
        "venue": "Int. J. Data Min. Bioinform.",
        "url": "https://doi.org/10.1504/IJDMB.2018.097683",
        "doi": "10.1504/IJDMB.2018.097683"
    },
    "merghani_review_2018-1": {
        "title": "A Review on Facial Micro-Expressions Analysis: Datasets, Features and Metrics",
        "year": "2018",
        "type": "article",
        "venue": "CoRR",
        "url": "http://arxiv.org/abs/1805.02397",
        "doi": null
    },
    "zhao_facial_2023-2": {
        "title": "Facial Micro-Expressions: An Overview",
        "year": "2023",
        "type": "article",
        "venue": "Proc. IEEE",
        "url": "https://doi.org/10.1109/JPROC.2023.3275192",
        "doi": "10.1109/JPROC.2023.3275192"
    },
    "xie_overview_2023-1": {
        "title": "An Overview of Facial Micro-Expression Analysis: Data, Methodology and Challenge",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Affect. Comput.",
        "url": "https://doi.org/10.1109/TAFFC.2022.3143100",
        "doi": "10.1109/TAFFC.2022.3143100"
    },
    "xie_overview_2020": {
        "title": "An Overview of Facial Micro-Expression Analysis: Data, Methodology and Challenge",
        "year": "2020",
        "type": "article",
        "venue": "CoRR",
        "url": "https://arxiv.org/abs/2012.11307",
        "doi": null
    },
    "salle_gene_2017": {
        "title": "Gene co-expression analyses: an overview from microarray collections in Arabidopsis thaliana",
        "year": "2017",
        "type": "article",
        "venue": "Briefings Bioinform.",
        "url": "https://doi.org/10.1093/bib/bbw002",
        "doi": "10.1093/BIB/BBW002"
    },
    "chen_smg_2023-1": {
        "title": "SMG: A Micro-gesture Dataset Towards Spontaneous Body Gestures for Emotional Stress State Analysis",
        "year": "2023",
        "type": "article",
        "venue": "IJCV",
        "url": "https://doi.org/10.1007/s11263-023-01761-6",
        "doi": "10.1007/S11263-023-01761-6"
    },
    "zhao_proceedings_2023": {
        "title": "Proceedings of IJCAI-2023 Workshop\\&Challenge on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA 2023) co-located with 32nd International Joint Conference on Artificial Intelligence (IJCAI 2023), Macau, China, August 21-22, 2023",
        "year": "2023",
        "type": "book",
        "venue": "CEUR-WS.org",
        "url": "https://ceur-ws.org/Vol-3522",
        "doi": null
    },
    "liu_imigue_2021-1": {
        "title": "iMiGUE: An Identity-Free Video Dataset for Micro-Gesture Understanding and Emotion Analysis",
        "year": "2021",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://openaccess.thecvf.com/content/CVPR2021/html/Liu\\_iMiGUE\\_An\\_Identity-Free\\_Video\\_Dataset\\_for\\_Micro-Gesture\\_Understanding\\_and\\_Emotion\\_CVPR\\_2021\\_paper.html",
        "doi": "10.1109/CVPR46437.2021.01049"
    },
    "liu_imigue_2021-2": {
        "title": "iMiGUE: An Identity-free Video Dataset for Micro-Gesture Understanding and Emotion Analysis",
        "year": "2021",
        "type": "article",
        "venue": "CoRR",
        "url": "https://arxiv.org/abs/2107.00285",
        "doi": null
    },
    "chen_analyze_2019": {
        "title": "Analyze Spontaneous Gestures for Emotional Stress State Recognition: A Micro-gesture Dataset and Analysis with Deep Learning",
        "year": "2019",
        "type": "inproceedings",
        "venue": "FG",
        "url": "https://doi.org/10.1109/FG.2019.8756513",
        "doi": "10.1109/FG.2019.8756513"
    },
    "domova_model_2023": {
        "title": "A Model for Types and Levels of Automation in Visual Analytics: A Survey, a Taxonomy, and Examples",
        "year": "2023",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2022.3163765",
        "doi": "10.1109/TVCG.2022.3163765"
    },
    "mcnutt_no_2023": {
        "title": "No Grammar to Rule Them All: A Survey of JSON-style DSLs for Visualization",
        "year": "2023",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2022.3209460",
        "doi": "10.1109/TVCG.2022.3209460"
    },
    "shen_towards_2023": {
        "title": "Towards Natural Language Interfaces for Data Visualization: A Survey",
        "year": "2023",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2022.3148007",
        "doi": "10.1109/TVCG.2022.3148007"
    },
    "tu_sdrquerier_2023": {
        "title": "SDRQuerier: A Visual Querying Framework for Cross-National Survey Data Recycling",
        "year": "2023",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2023.3261944",
        "doi": "10.1109/TVCG.2023.3261944"
    },
    "wang_dl4scivis_2023": {
        "title": "DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization",
        "year": "2023",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2022.3167896",
        "doi": "10.1109/TVCG.2022.3167896"
    },
    "bressa_whats_2022": {
        "title": "What's the Situation with Situated Visualization? A Survey and Perspectives on Situatedness",
        "year": "2022",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2021.3114835",
        "doi": "10.1109/TVCG.2021.3114835"
    },
    "guo_survey_2022": {
        "title": "Survey on Visual Analysis of Event Sequence Data",
        "year": "2022",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2021.3100413",
        "doi": "10.1109/TVCG.2021.3100413"
    },
    "quadri_survey_2022": {
        "title": "A Survey of Perception-Based Visualization Studies by Task",
        "year": "2022",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2021.3098240",
        "doi": "10.1109/TVCG.2021.3098240"
    },
    "wang_survey_2022": {
        "title": "A Survey on ML4VIS: Applying Machine Learning Advances to Data Visualization",
        "year": "2022",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2021.3106142",
        "doi": "10.1109/TVCG.2021.3106142"
    },
    "wu_ai4vis_2022": {
        "title": "AI4VIS: Survey on Artificial Intelligence Approaches for Data Visualization",
        "year": "2022",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2021.3099002",
        "doi": "10.1109/TVCG.2021.3099002"
    },
    "yousef_survey_2021": {
        "title": "A Survey of Text Alignment Visualization",
        "year": "2021",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2020.3028975",
        "doi": "10.1109/TVCG.2020.3028975"
    },
    "hohman_visual_2019": {
        "title": "Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers",
        "year": "2019",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2018.2843369",
        "doi": "10.1109/TVCG.2018.2843369"
    },
    "hullman_pursuit_2019": {
        "title": "In Pursuit of Error: A Survey of Uncertainty Visualization Evaluation",
        "year": "2019",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2018.2864889",
        "doi": "10.1109/TVCG.2018.2864889"
    },
    "liu_bridging_2019": {
        "title": "Bridging Text Visualization and Mining: A Task-Driven Survey",
        "year": "2019",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2018.2834341",
        "doi": "10.1109/TVCG.2018.2834341"
    },
    "wang_visualization_2019": {
        "title": "Visualization and Visual Analysis of Ensemble Data: A Survey",
        "year": "2019",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2018.2853721",
        "doi": "10.1109/TVCG.2018.2853721"
    },
    "rautenhaus_visualization_2018": {
        "title": "Visualization in Meteorology - A Survey of Techniques and Tools for Data Analysis Tasks",
        "year": "2018",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2017.2779501",
        "doi": "10.1109/TVCG.2017.2779501"
    },
    "federico_survey_2017": {
        "title": "A Survey on Visual Approaches for Analyzing Scientific Literature and Patents",
        "year": "2017",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2016.2610422",
        "doi": "10.1109/TVCG.2016.2610422"
    },
    "zhou_survey_2016": {
        "title": "A Survey of Colormaps in Visualization",
        "year": "2016",
        "type": "article",
        "venue": "TVCG",
        "url": "https://doi.org/10.1109/TVCG.2015.2489649",
        "doi": "10.1109/TVCG.2015.2489649"
    },
    "javed_visual_2023": {
        "title": "Visual Object Tracking With Discriminative Filters and Siamese Networks: A Survey and Outlook",
        "year": "2023",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2022.3212594",
        "doi": "10.1109/TPAMI.2022.3212594"
    },
    "robinson_survey_2022": {
        "title": "Survey on the Analysis and Modeling of Visual Kinship: A Decade in the Making",
        "year": "2022",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2021.3063078",
        "doi": "10.1109/TPAMI.2021.3063078"
    },
    "jing_self-supervised_2021": {
        "title": "Self-Supervised Visual Feature Learning With Deep Neural Networks: A Survey",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2020.2992393",
        "doi": "10.1109/TPAMI.2020.2992393"
    },
    "liu_visual_2021": {
        "title": "Visual Semantic Information Pursuit: A Survey",
        "year": "2021",
        "type": "article",
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "url": "https://doi.org/10.1109/TPAMI.2019.2950025",
        "doi": "10.1109/TPAMI.2019.2950025"
    },
    "kollias_abaw_2023": {
        "title": "ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection \\& Emotional Reaction Intensity Estimation Challenges",
        "year": "2023",
        "type": "inproceedings",
        "venue": "CVPR",
        "url": "https://doi.org/10.1109/CVPRW59228.2023.00626",
        "doi": "10.1109/CVPRW59228.2023.00626"
    },
    "dhall_emotiw_2023": {
        "title": "EmotiW 2023: Emotion Recognition in the Wild Challenge",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ICMI",
        "url": "https://doi.org/10.1145/3577190.3616545",
        "doi": "10.1145/3577190.3616545"
    },
    "schuller_acm_2023": {
        "title": "The ACM Multimedia 2023 Computational Paralinguistics Challenge: Emotion Share \\& Requests",
        "year": "2023",
        "type": "inproceedings",
        "venue": "ACM MM",
        "url": "https://doi.org/10.1145/3581783.3612835",
        "doi": "10.1145/3581783.3612835"
    },
    "christ_muse_2023": {
        "title": "The MuSe 2023 Multimodal Sentiment Analysis Challenge: Mimicked Emotions, Cross-Cultural Humour, and Personalisation",
        "year": "2023",
        "type": "inproceedings",
        "venue": "MuSe",
        "url": "https://doi.org/10.1145/3606039.3613114",
        "doi": "10.1145/3606039.3613114"
    },
    "christ_muse_2022": {
        "title": "The MuSe 2022 Multimodal Sentiment Analysis Challenge: Humor, Emotional Reactions, and Stress",
        "year": "2022",
        "type": "inproceedings",
        "venue": "MuSe",
        "url": "https://doi.org/10.1145/3551876.3554817",
        "doi": "10.1145/3551876.3554817"
    },
    "alshaer_emotional_2021": {
        "title": "Emotional Mario: A Games Analytics Challenge: MediaEval 2021",
        "year": "2021",
        "type": "inproceedings",
        "venue": "Working Notes Proceedings of the MediaEval 2021 Workshop, Online, 13-15 December 2021",
        "url": "https://ceur-ws.org/Vol-3181/paper20.pdf",
        "doi": null
    },
    "stappen_muse_2021": {
        "title": "The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment, Emotion, Physiological-Emotion, and Stress",
        "year": "2021",
        "type": "inproceedings",
        "venue": "MuSe",
        "url": "https://doi.org/10.1145/3475957.3484450",
        "doi": "10.1145/3475957.3484450"
    },
    "dhall_emotiw_2020": {
        "title": "EmotiW 2020: Driver Gaze, Group Emotion, Student Engagement and Physiological Signal based Challenges",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ICMI",
        "url": "https://doi.org/10.1145/3382507.3417973",
        "doi": "10.1145/3382507.3417973"
    },
    "schuller_interspeech_2020": {
        "title": "The INTERSPEECH 2020 Computational Paralinguistics Challenge: Elderly Emotion, Breathing \\& Masks",
        "year": "2020",
        "type": "inproceedings",
        "venue": "INTERSPEECH",
        "url": "https://doi.org/10.21437/Interspeech.2020-32",
        "doi": "10.21437/INTERSPEECH.2020-32"
    },
    "stappen_muse_2020-1": {
        "title": "MuSe 2020 Challenge and Workshop: Multimodal Sentiment Analysis, Emotion-target Engagement and Trustworthiness Detection in Real-life Media: Emotional Car Reviews in-the-wild",
        "year": "2020",
        "type": "inproceedings",
        "venue": "MuSe",
        "url": "https://doi.org/10.1145/3423327.3423673",
        "doi": "10.1145/3423327.3423673"
    },
    "seo_kerc_2020": {
        "title": "KERC 2019: The 1st Korean Emotion Recognition Challenge",
        "year": "2020",
        "type": "inproceedings",
        "venue": "ACM SMA",
        "url": "https://doi.org/10.1145/3426020.3426067",
        "doi": "10.1145/3426020.3426067"
    },
    "ringeval_avec19_2019-1": {
        "title": "AVEC'19: Audio/Visual Emotion Challenge and Workshop",
        "year": "2019",
        "type": "inproceedings",
        "venue": "AVEC",
        "url": "https://doi.org/10.1145/3343031.3350550",
        "doi": "10.1145/3343031.3350550"
    },
    "smith_emotion_2017": {
        "title": "Emotion challenge: building a new photoreal facial performance pipeline for games",
        "year": "2017",
        "type": "inproceedings",
        "venue": "Proceedings of the ACM SIGGRAPH Digital Production Symposium, DigiPro '17, Los Angeles, California, USA, July 29, 2017",
        "url": "https://doi.org/10.1145/3105692.3105695",
        "doi": "10.1145/3105692.3105695"
    },
    "huang_overview_2017": {
        "title": "Overview of the NLPCC 2017 Shared Task: Emotion Generation Challenge",
        "year": "2017",
        "type": "inproceedings",
        "venue": "NLPCC",
        "url": "https://doi.org/10.1007/978-3-319-73618-1\\_82",
        "doi": "10.1007/978-3-319-73618-1_82"
    },
    "li_mec_2016": {
        "title": "MEC 2016: The Multimodal Emotion Recognition Challenge of CCPR 2016",
        "year": "2016",
        "type": "inproceedings",
        "venue": "CCPR",
        "url": "https://doi.org/10.1007/978-981-10-3005-5\\_55",
        "doi": "10.1007/978-981-10-3005-5_55"
    },
    "dhall_emotiw_2016-1": {
        "title": "EmotiW 2016: video and group-level emotion recognition challenges",
        "year": "2016",
        "type": "inproceedings",
        "venue": "ICMI",
        "url": "https://doi.org/10.1145/2993148.2997638",
        "doi": "10.1145/2993148.2997638"
    },
    "valstar_avec_2016": {
        "title": "AVEC 2016: Depression, Mood, and Emotion Recognition Workshop and Challenge",
        "year": "2016",
        "type": "inproceedings",
        "venue": "AVEC",
        "url": "https://doi.org/10.1145/2988257.2988258",
        "doi": "10.1145/2988257.2988258"
    },
    "dhall_video_2015-3": {
        "title": "Video and Image based Emotion Recognition Challenges in the Wild: EmotiW 2015",
        "year": "2015",
        "type": "inproceedings",
        "venue": "MuSe",
        "url": "https://doi.org/10.1145/2818346.2829994",
        "doi": "10.1145/2818346.2829994"
    },
    "ringeval_avec_2015": {
        "title": "AVEC 2015: The 5th International Audio/Visual Emotion Challenge and Workshop",
        "year": "2015",
        "type": "inproceedings",
        "venue": "AVEC",
        "url": "https://doi.org/10.1145/2733373.2806408",
        "doi": "10.1145/2733373.2806408"
    },
    "dhall_emotion_2014": {
        "title": "Emotion Recognition In The Wild Challenge 2014: Baseline, Data and Protocol",
        "year": "2014",
        "type": "inproceedings",
        "venue": "ICMI",
        "url": "https://doi.org/10.1145/2663204.2666275",
        "doi": "10.1145/2663204.2666275"
    },
    "valstar_avec_2014": {
        "title": "AVEC 2014: the 4th international audio/visual emotion challenge and workshop",
        "year": "2014",
        "type": "inproceedings",
        "venue": "AVEC",
        "url": "https://doi.org/10.1145/2647868.2647869",
        "doi": "10.1145/2647868.2647869"
    },
    "dhall_emotion_2013": {
        "title": "Emotion recognition in the wild challenge 2013",
        "year": "2013",
        "type": "inproceedings",
        "venue": "ICMI",
        "url": "https://doi.org/10.1145/2522848.2531739",
        "doi": "10.1145/2522848.2531739"
    },
    "schuller_interspeech_2013": {
        "title": "The INTERSPEECH 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism",
        "year": "2013",
        "type": "inproceedings",
        "venue": "INTERSPEECH",
        "url": "https://doi.org/10.21437/Interspeech.2013-56",
        "doi": "10.21437/INTERSPEECH.2013-56"
    },
    "valstar_avec_2013": {
        "title": "AVEC 2013: the continuous audio/visual emotion and depression recognition challenge",
        "year": "2013",
        "type": "inproceedings",
        "venue": "AVEC",
        "url": "https://doi.org/10.1145/2512530.2512533",
        "doi": "10.1145/2512530.2512533"
    },
    "schuller_avec_2012": {
        "title": "AVEC 2012: the continuous audio/visual emotion challenge - an introduction",
        "year": "2012",
        "type": "inproceedings",
        "venue": "AVEC ICMI",
        "url": "https://doi.org/10.1145/2388676.2388758",
        "doi": "10.1145/2388676.2388758"
    },
    "schuller_avec_2012-1": {
        "title": "AVEC 2012: the continuous audio/visual emotion challenge",
        "year": "2012",
        "type": "inproceedings",
        "venue": "AVEC ICMI",
        "url": "https://doi.org/10.1145/2388676.2388776",
        "doi": "10.1145/2388676.2388776"
    },
    "schuller_first_2011": {
        "title": "The First Audio/Visual Emotion Challenge and Workshop - An Introduction",
        "year": "2011",
        "type": "inproceedings",
        "venue": "AVEC ACII",
        "url": "https://doi.org/10.1007/978-3-642-24571-8\\_42",
        "doi": "10.1007/978-3-642-24571-8_42"
    },
    "schuller_avec_2011": {
        "title": "AVEC 2011-The First International Audio/Visual Emotion Challenge",
        "year": "2011",
        "type": "inproceedings",
        "venue": "AVEC ACII",
        "url": "https://doi.org/10.1007/978-3-642-24571-8\\_53",
        "doi": "10.1007/978-3-642-24571-8_53"
    },
    "schuller_interspeech_2009": {
        "title": "The INTERSPEECH 2009 emotion challenge",
        "year": "2009",
        "type": "inproceedings",
        "venue": "INTERSPEECH",
        "url": "https://doi.org/10.21437/Interspeech.2009-103",
        "doi": "10.21437/INTERSPEECH.2009-103"
    }
}